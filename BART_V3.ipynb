{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3KK2hS75Cxu"
      },
      "source": [
        "**Some references**\n",
        "\n",
        "https://www.kaggle.com/code/minhsienweng/train-infer-pii-detection-deberta-v3\n",
        "\n",
        "(no training) https://www.kaggle.com/code/manavtrivedi/0-967-nlp-sakura/notebook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-ignite\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUjXSW_z63tT",
        "outputId": "b54a7e4e-54c1-44ef-a560-baa6f7c12ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.10/dist-packages (0.5.0.post2)\n",
            "Requirement already satisfied: torch<3,>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-ignite) (2.2.1+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytorch-ignite) (24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3,>=1.3->pytorch-ignite) (1.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZW_wByR5Cxw"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import datasets\n",
        "import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments\n",
        "from scipy.special import softmax\n",
        "from sklearn.model_selection import train_test_split\n",
        "from spacy.lang.en import English\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from ignite.metrics import Fbeta\n",
        "from functools import partial\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.metrics import recall_score, precision_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from torch.nn.functional import softmax\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Kaggle API\n",
        "#!pip install kaggle\n",
        "# Upload Kaggle API key\n",
        "#from google.colab import files\n",
        "#files.upload()\n",
        "# Set up Kaggle API credentials\n",
        "#!mkdir -p ~/.kaggle\n",
        "#!cp kaggle.json ~/.kaggle/\n",
        "#!chmod 600 ~/.kaggle/kaggle.json\n",
        "#!kaggle competitions download -c pii-detection-removal-from-educational-data\n",
        "!#unzip pii-detection-removal-from-educational-data.zip"
      ],
      "metadata": {
        "id": "TQo8N-1V5MWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CAzDOtI5Cxz"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VULldbuC5Cxz",
        "outputId": "5750834f-d4b8-4242-c377-5c3f33ac3a4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of labels: 13\n",
            "Label I-PHONE_NUM has index 0\n",
            "Label B-ID_NUM has index 1\n",
            "Label I-NAME_STUDENT has index 2\n",
            "Label I-ID_NUM has index 3\n",
            "Label B-STREET_ADDRESS has index 4\n"
          ]
        }
      ],
      "source": [
        "#Finding out the number of labels\n",
        "data = json.load(open('train.json'))\n",
        "\n",
        "# Finding unique labels\n",
        "all_labels = set()\n",
        "for d in data:\n",
        "    all_labels.update(d['labels'])\n",
        "\n",
        "# Clear memory\n",
        "del data\n",
        "\n",
        "# Creating label-index mappings\n",
        "label2id = {label: index for index, label in enumerate(all_labels)}\n",
        "id2label = {index: label for index, label in enumerate(all_labels)}\n",
        "\n",
        "# Inspecting mappings\n",
        "print(f\"Number of labels: {len(label2id)}\")\n",
        "for label, idx in list(label2id.items())[:5]:  # print first 5\n",
        "    print(f\"Label {label} has index {idx}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def oh_encoder(labels, unique_labels=['O', 'B-NAME_STUDENT','I-NAME_STUDENT','B-URL_PERSONAL',\n",
        "                                      'B-ID_NUM','I-ID_NUM','B-EMAIL','I-STREET_ADDRESS',\n",
        "                                      'I-PHONE_NUM', 'B-USERNAME', 'B-PHONE_NUM','B-STREET_ADDRESS', 'I-URL_PERSONAL']):\n",
        "    labels_oh = []\n",
        "    for label in labels:  # label: str\n",
        "        label_oh = [0.0] * len(unique_labels)\n",
        "        if label in unique_labels:\n",
        "            label_oh[unique_labels.index(label)] = 1.0\n",
        "        labels_oh.append(label_oh)\n",
        "\n",
        "    return torch.tensor(labels_oh, dtype=torch.float32)  # list of one-hot labels as tensors\n",
        "\n",
        "    # Example usage\n",
        "unique_labels = list(all_labels)\n",
        "encoded_labels = oh_encoder([\"B-NAME_STUDENT\", \"O\"], unique_labels)\n",
        "print(encoded_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDbMYg_cjXLD",
        "outputId": "d6e344f3-01de-477e-8cd6-f6ff11ce7d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example, tokenizer, label2id):\n",
        "    tokens = []\n",
        "    labels = []\n",
        "    for token, label, t_ws in zip(example[\"tokens\"], example[\"labels\"], example[\"trailing_whitespace\"]):\n",
        "        tokens.append(token)\n",
        "        labels.extend([label] * len(token))\n",
        "        if t_ws:\n",
        "            tokens.append(\" \")\n",
        "            labels.append(\"O\")\n",
        "\n",
        "    text = \"\".join(tokens)\n",
        "    tokenized = tokenizer(text, return_offsets_mapping=True, truncation=False)\n",
        "    token_labels = []\n",
        "    for start_idx, end_idx in tokenized.offset_mapping:\n",
        "        if start_idx == 0 and end_idx == 0:\n",
        "            token_labels.append(label2id[\"O\"])\n",
        "        else:\n",
        "            if text[start_idx].isspace():\n",
        "                start_idx += 1\n",
        "            label_id = label2id[labels[start_idx]]\n",
        "            token_labels.append(label_id)\n",
        "\n",
        "    return {**tokenized, \"labels\": token_labels}"
      ],
      "metadata": {
        "id": "f1hcosFJXCwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = json.load(open('train.json'))\n",
        "\n",
        "from transformers import BartTokenizerFast\n",
        "\n",
        "tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-large\", add_prefix_space=True)\n",
        "\n",
        "# Assume we are processing the first document for demonstration\n",
        "d = data[0]\n",
        "tokenized_output = tokenize(d, tokenizer, label2id)\n",
        "\n",
        "# Convert tokenized input IDs to tensor for use with PyTorch models\n",
        "input_ids_tensor = torch.tensor(tokenized_output['input_ids'])\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids_tensor)\n",
        "\n",
        "# Debugging output to verify tokens\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhDW1-2oYDnV",
        "outputId": "9744c914-0b31-4fd2-fc8c-12e841ab2978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'ĠDesign', 'ĠThinking', 'Ġfor', 'Ġinnovation', 'Ġreflex', 'ion', '-', 'Av', 'ril', 'Ġ2021', '-', 'N', 'ath', 'al', 'ie', 'ĠSy', 'lla', 'Ċ', 'Ċ', 'Chall', 'enge', 'Ġ&', 'Ġselection', 'Ċ', 'Ċ', 'The', 'Ġtool', 'ĠI', 'Ġuse', 'Ġto', 'Ġhelp', 'Ġall', 'Ġstakeholders', 'Ġfinding', 'Ġtheir', 'Ġway', 'Ġthrough', 'Ġthe', 'Ġcomplexity', 'Ġof', 'Ġa', 'Ġproject', 'Ġis', 'Ġthe', 'Ġ', 'Ġmind', 'Ġmap', '.', 'Ċ', 'Ċ', 'What', 'Ġexactly', 'Ġis', 'Ġa', 'Ġmind', 'Ġmap', '?', 'ĠAccording', 'Ġto', 'Ġthe', 'Ġdefinition', 'Ġof', 'ĠBu', 'zan', 'ĠT', '.', 'Ġand', 'ĠBu', 'zan', 'ĠB', '.', 'Ġ(', '1999', ',', 'ĠD', 'ess', 'ine', '-', 'mo', 'i', 'Ġ', 'Ġl', \"'\", 'intelligence', '.', 'ĠParis', ':', 'ĠLes', 'ĠÃī', 'd', 'itions', 'Ġd', \"'\", 'Organ', 'isation', '.),', 'Ġthe', 'Ġmind', 'Ġmap', 'Ġ(', 'or', 'Ġhe', 'uristic', 'Ġdiagram', ')', 'Ġis', 'Ġa', 'Ġgraphic', 'Ġ', 'Ġrepresentation', 'Ġtechnique', 'Ġthat', 'Ġfollows', 'Ġthe', 'Ġnatural', 'Ġfunctioning', 'Ġof', 'Ġthe', 'Ġmind', 'Ġand', 'Ġallows', 'Ġthe', 'Ġbrain', \"'s\", 'Ġ', 'Ġpotential', 'Ġto', 'Ġbe', 'Ġreleased', '.', 'ĠCf', 'ĠAnnex', '1', 'Ċ', 'Ċ', 'This', 'Ġtool', 'Ġhas', 'Ġmany', 'Ġadvantages', ':', 'Ċ', 'Ċ', 'âĢ¢', 'Ġ', 'ĠIt', 'Ġis', 'Ġaccessible', 'Ġto', 'Ġall', 'Ġand', 'Ġdoes', 'Ġnot', 'Ġrequire', 'Ġsignificant', 'Ġmaterial', 'Ġinvestment', 'Ġand', 'Ġcan', 'Ġbe', 'Ġdone', 'Ġ', 'Ġquickly', 'Ċ', 'Ċ', 'âĢ¢', 'Ġ', 'ĠIt', 'Ġis', 'Ġscalable', 'Ċ', 'Ċ', 'âĢ¢', 'Ġ', 'ĠIt', 'Ġallows', 'Ġcategor', 'ization', 'Ġand', 'Ġlinking', 'Ġof', 'Ġinformation', 'Ċ', 'Ċ', 'âĢ¢', 'Ġ', 'ĠIt', 'Ġcan', 'Ġbe', 'Ġapplied', 'Ġto', 'Ġany', 'Ġtype', 'Ġof', 'Ġsituation', ':', 'Ġnot', 'et', 'aking', ',', 'Ġproblem', 'Ġsolving', ',', 'Ġanalysis', ',', 'Ġcreation', 'Ġof', 'Ġ', 'Ġnew', 'Ġideas', 'Ċ', 'Ċ', 'âĢ¢', 'Ġ', 'ĠIt', 'Ġis', 'Ġsuitable', 'Ġfor', 'Ġall', 'Ġpeople', 'Ġand', 'Ġis', 'Ġeasy', 'Ġto', 'Ġlearn', 'Ċ', 'Ċ', 'âĢ¢', 'Ġ', 'ĠIt', 'Ġis', 'Ġfun', 'Ġand', 'Ġencourages', 'Ġexchanges', 'Ċ', 'Ċ', 'âĢ¢', 'Ġ', 'ĠIt', 'Ġmakes', 'Ġvisible', 'Ġthe', 'Ġdimension', 'Ġof', 'Ġprojects', ',', 'Ġopportunities', ',', 'Ġinter', 'connect', 'ions', 'Ċ', 'Ċ', 'âĢ¢', 'Ġ', 'ĠIt', 'Ġsynthes', 'izes', 'Ċ', 'Ċ', 'âĢ¢', 'Ġ', 'ĠIt', 'Ġmakes', 'Ġthe', 'Ġproject', 'Ġunderstandable', 'Ċ', 'Ċ', 'âĢ¢', 'Ġ', 'ĠIt', 'Ġallows', 'Ġyou', 'Ġto', 'Ġexplore', 'Ġideas', 'Ċ', 'Ċ', 'The', 'Ġcreation', 'Ġof', 'Ġa', 'Ġmind', 'Ġmap', 'Ġstarts', 'Ġwith', 'Ġan', 'Ġidea', '/', 'problem', 'Ġlocated', 'Ġat', 'Ġits', 'Ġcenter', '.', 'ĠThis', 'Ġstarting', 'Ġpoint', 'Ġ', 'Ġgenerates', 'Ġideas', '/', 'work', 'Ġareas', ',', 'Ġincre', 'mented', 'Ġaround', 'Ġthis', 'Ġcenter', 'Ġin', 'Ġa', 'Ġradial', 'Ġstructure', ',', 'Ġwhich', 'Ġin', 'Ġturn', 'Ġis', 'Ġ', 'Ġcompleted', 'Ġwith', 'Ġas', 'Ġmany', 'Ġbranches', 'Ġas', 'Ġnew', 'Ġideas', '.', 'Ċ', 'Ċ', 'This', 'Ġtool', 'Ġenables', 'Ġcreativity', 'Ġand', 'Ġlogic', 'Ġto', 'Ġbe', 'Ġmobilized', ',', 'Ġit', 'Ġis', 'Ġa', 'Ġmap', 'Ġof', 'Ġthe', 'Ġthoughts', '.', 'Ċ', 'Ċ', 'Creat', 'ivity', 'Ġis', 'Ġenhanced', 'Ġbecause', 'Ġparticipants', 'Ġfeel', 'Ġcomfortable', 'Ġwith', 'Ġthe', 'Ġmethod', '.', 'Ċ', 'Ċ', 'Application', 'Ġ&', 'ĠInsight', 'Ċ', 'Ċ', 'I', 'Ġstart', 'Ġthe', 'Ġprocess', 'Ġof', 'Ġthe', 'Ġmind', 'Ġmap', 'Ġcreation', 'Ġwith', 'Ġthe', 'Ġstakeholders', 'Ġstanding', 'Ġaround', 'Ġa', 'Ġlarge', 'Ġboard', 'Ġ', 'Ġ(', 'white', 'Ġor', 'Ġpaper', 'Ġboard', ').', 'ĠIn', 'Ġthe', 'Ġcenter', 'Ġof', 'Ġthe', 'Ġboard', ',', 'ĠI', 'Ġwrite', 'Ġand', 'Ġhighlight', 'Ġthe', 'Ġtopic', 'Ġto', 'Ġdesign', '.', 'Ċ', 'Ċ', 'Through', 'Ġa', 'Ġseries', 'Ġof', 'Ġquestions', ',', 'ĠI', 'Ġguide', 'Ġthe', 'Ġstakeholders', 'Ġin', 'Ġmodelling', 'Ġthe', 'Ġmind', 'Ġmap', '.', 'ĠI', 'Ġadapt', 'Ġthe', 'Ġseries', 'Ġ', 'Ġof', 'Ġquestions', 'Ġaccording', 'Ġto', 'Ġthe', 'Ġtopic', 'Ġto', 'Ġbe', 'Ġaddressed', '.', 'ĠIn', 'Ġthe', 'Ġtype', 'Ġof', 'Ġquestions', ',', 'Ġwe', 'Ġcan', 'Ġuse', ':', 'Ġwho', ',', 'Ġwhat', ',', 'Ġ', 'Ġwhen', ',', 'Ġwhere', ',', 'Ġwhy', ',', 'Ġhow', ',', 'Ġhow', 'Ġmuch', '.', 'Ċ', 'Ċ', 'The', 'Ġuse', 'Ġof', 'Ġthe', 'ĠâĢ', 'ľ', 'why', 'âĢ', 'Ŀ', 'Ġis', 'Ġvery', 'Ġinteresting', 'Ġto', 'Ġunderstand', 'Ġthe', 'Ġorigin', '.', 'ĠBy', 'Ġthis', 'Ġway', ',', 'Ġthe', 'Ġinterviewed', 'Ġperson', 'Ġ', 'Ġfre', 'es', 'Ġitself', 'Ġfrom', 'Ġparad', 'ig', 'ms', 'Ġand', 'Ġthus', 'Ġd', 'ares', 'Ġto', 'Ġpropose', 'Ġnew', 'Ġideas', 'Ġ/', 'Ġways', 'Ġof', 'Ġfunctioning', '.', 'ĠI', 'Ġplan', 'Ġtwo', 'Ġ', 'Ġhours', 'Ġfor', 'Ġa', 'Ġworkshop', '.', 'Ċ', 'Ċ', 'Design', 'ĠThinking', 'Ġfor', 'Ġinnovation', 'Ġreflex', 'ion', '-', 'Av', 'ril', 'Ġ2021', '-', 'N', 'ath', 'al', 'ie', 'ĠSy', 'lla', 'Ċ', 'Ċ', 'After', 'Ġmodelling', 'Ġthe', 'Ġmind', 'Ġmap', 'Ġon', 'Ġpaper', ',', 'ĠI', 'Ġpropose', 'Ġto', 'Ġthe', 'Ġparticipants', 'Ġa', 'Ġdigital', 'Ġvisualization', 'Ġof', 'Ġtheir', 'Ġ', 'Ġwork', 'Ġwith', 'Ġthe', 'Ġaddition', 'Ġof', 'Ġcolor', 'Ġcodes', ',', 'Ġimages', 'Ġand', 'Ġinter', 'connect', 'ions', '.', 'ĠThis', 'Ġsecond', 'Ġworkshop', 'Ġalso', 'Ġlasts', 'Ġ', 'Ġtwo', 'Ġhours', 'Ġand', 'Ġallows', 'Ġthe', 'Ġmind', 'Ġmap', 'Ġto', 'Ġevolve', '.', 'ĠOnce', 'Ġfamiliar', 'ized', 'Ġwith', 'Ġit', ',', 'Ġthe', 'Ġstakeholders', 'Ġdiscover', 'Ġ', 'Ġthe', 'Ġpower', 'Ġof', 'Ġthe', 'Ġtool', '.', 'ĠThen', ',', 'Ġthe', 'Ġsecond', 'Ġworkshop', 'Ġbrings', 'Ġout', 'Ġeven', 'Ġmore', 'Ġideas', 'Ġand', 'Ġconstructive', 'Ġ', 'Ġexchanges', 'Ġbetween', 'Ġthe', 'Ġstakeholders', '.', 'ĠAround', 'Ġthis', 'Ġnew', 'Ġmind', 'Ġmap', ',', 'Ġthey', 'Ġhave', 'Ġlearned', 'Ġto', 'Ġwork', 'Ġ', 'Ġtogether', 'Ġand', 'Ġwant', 'Ġto', 'Ġmake', 'Ġvisible', 'Ġthe', 'Ġuntold', 'Ġideas', '.', 'Ċ', 'Ċ', 'I', 'Ġnow', 'Ġpresent', 'Ġall', 'Ġthe', 'Ġprojects', 'ĠI', 'Ġmanage', 'Ġin', 'Ġthis', 'Ġtype', 'Ġof', 'Ġformat', 'Ġin', 'Ġorder', 'Ġto', 'Ġease', 'Ġrapid', 'Ġunderstanding', 'Ġfor', 'Ġ', 'Ġdecision', '-', 'makers', '.', 'ĠThese', 'Ġpresentations', 'Ġare', 'Ġthe', 'Ġcore', 'Ġof', 'Ġmy', 'Ġbusiness', 'Ġmodels', '.', 'ĠThe', 'Ġdecision', '-', 'makers', 'Ġare', 'Ġ', 'Ġthus', 'Ġable', 'Ġto', 'Ġidentify', 'Ġthe', 'Ġopportunities', 'Ġof', 'Ġthe', 'Ġprojects', 'Ġand', 'Ġcan', 'Ġtake', 'Ġquick', 'Ġdecisions', 'Ġto', 'Ġvalidate', 'Ġthem', '.', 'Ġ', 'ĠThey', 'Ġfind', 'Ġanswers', 'Ġto', 'Ġtheir', 'Ġquestions', 'Ġthank', 'Ġto', 'Ġa', 'Ġschematic', 'Ġrepresentation', '.', 'Ċ', 'Ċ', 'App', 'roach', 'Ċ', 'Ċ', 'What', 'ĠI', 'Ġfind', 'Ġamazing', 'Ġwith', 'Ġthe', 'Ġfac', 'ilitation', 'Ġof', 'Ġthis', 'Ġtype', 'Ġof', 'Ġworkshop', 'Ġis', 'Ġthe', 'Ġparticipants', 'Ġcommitment', 'Ġfor', 'Ġ', 'Ġthe', 'Ġproject', '.', 'ĠThis', 'Ġtool', 'Ġhelps', 'Ġto', 'Ġgive', 'Ġmeaning', '.', 'ĠThe', 'Ġparticipants', 'Ġappropriate', 'Ġthe', 'Ġstory', 'Ġand', 'Ġwant', 'Ġto', 'Ġkeep', 'Ġ', 'Ġwriting', 'Ġit', '.', 'ĠThen', ',', 'Ġthey', 'Ġeasily', 'Ġbecome', 'Ġactors', 'Ġor', 'Ġsponsors', 'Ġof', 'Ġthe', 'Ġproject', '.', 'ĠA', 'Ġtrust', 'Ġrelationship', 'Ġis', 'Ġbuilt', ',', 'Ġ', 'Ġthus', 'Ġfacilitating', 'Ġthe', 'Ġimplementation', 'Ġof', 'Ġrelated', 'Ġactions', '.', 'Ċ', 'Ċ', 'Design', 'ĠThinking', 'Ġfor', 'Ġinnovation', 'Ġreflex', 'ion', '-', 'Av', 'ril', 'Ġ2021', '-', 'N', 'ath', 'al', 'ie', 'ĠSy', 'lla', 'Ċ', 'Ċ', 'An', 'nex', 'Ġ1', ':', 'ĠMind', 'ĠMap', 'ĠShared', 'Ġfacilities', 'Ġproject', 'ĊĊ', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xvwwElZ5Cx2"
      },
      "source": [
        "# Model: BART\n",
        "\n",
        "Using a pretrained BART, we will build a classifier head on top of it to predict the class at token level."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartModel\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class BartForTokenClassification(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super().__init__()\n",
        "        self.bart = BartModel.from_pretrained(\"facebook/bart-large\")\n",
        "\n",
        "        # Freeze the BART model parameters\n",
        "        for param in self.bart.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.num_labels = num_labels\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.bart.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        outputs = self.bart(input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs[0]\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)  # Assuming -100 is used for padding\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, num_labels)  # Corrected to num_labels\n",
        "                active_labels = torch.where(\n",
        "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "                )\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))  # Corrected to num_labels\n",
        "            return loss, logits\n",
        "        return logits"
      ],
      "metadata": {
        "id": "cq-8vFG6ZeUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model with the number of labels for your classification task.\n",
        "num_labels = len(label2id)\n",
        "model = BartForTokenClassification(num_labels=num_labels)"
      ],
      "metadata": {
        "id": "z_3wcQ5jG3uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "# Calculate the number of trainable parameters (parameters where gradients will be computed)\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total Parameters: {total_params}\")\n",
        "print(f\"Trainable Parameters: {trainable_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9rPJ4J1If0d",
        "outputId": "260e6f27-d7fa-4130-d1df-6108c8568ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 406304781\n",
            "Trainable Parameters: 13325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUICK CHECK: Test model initialization"
      ],
      "metadata": {
        "id": "i3QYAlbNaB4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Dummy inputs\n",
        "input_ids = torch.randint(0, 50265, (1, 10))  # Example input ID for BART, sequence length 10\n",
        "attention_mask = torch.ones(1, 10)\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():  # No need to compute gradients for a simple forward test\n",
        "    logits = model(input_ids, attention_mask)\n",
        "    print(f\"Logits Shape: {logits.shape}\")  # Should be (1, 10, num_labels)\n",
        "\n",
        "# Check if output shapes are as expected\n",
        "assert logits.shape == (1, 10, num_labels), \"Output shape is incorrect!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZtNfoM5aH4_",
        "outputId": "476a4820-ca76-4d28-c36f-bc70171d5225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits Shape: torch.Size([1, 10, 13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd8sbxXb5Cx4"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(device)\n",
        "epochs = 5\n",
        "tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-large\", add_prefix_space=True)\n",
        "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(outputs, labels, label_map):\n",
        "    # Assuming outputs are logits and labels are indices\n",
        "    softmax = nn.Softmax(dim=2)\n",
        "    predictions = torch.argmax(softmax(outputs), dim=2)\n",
        "\n",
        "    true_labels = []\n",
        "    true_preds = []\n",
        "\n",
        "    # Flatten outputs and labels for evaluation ignoring padding index\n",
        "    for i in range(len(labels)):\n",
        "        label_mask = labels[i] != -100  # Assume -100 is used to mark padding labels\n",
        "        true_labels.extend(labels[i][label_mask].cpu().numpy())\n",
        "        true_preds.extend(predictions[i][label_mask].cpu().numpy())\n",
        "\n",
        "    # Convert indices to names using label_map\n",
        "    true_labels = [label_map[label] for label in true_labels]\n",
        "    true_preds = [label_map[pred] for pred in true_preds]\n",
        "\n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(list(label_map.values()))  # Fit all possible labels\n",
        "\n",
        "    true_labels_bin = lb.transform(true_labels)\n",
        "    true_preds_bin = lb.transform(true_preds)\n",
        "\n",
        "    precision, recall, _ = precision_recall_fscore_support(true_labels_bin, true_preds_bin, average='weighted', zero_division=1)[:3]\n",
        "\n",
        "    # Compute the F5 score\n",
        "    beta = 5  # Focus more on recall\n",
        "    f5_score = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
        "\n",
        "    return {\n",
        "        'f5_score': f5_score,\n",
        "        'recall': recall,\n",
        "        'precision': precision\n",
        "    }"
      ],
      "metadata": {
        "id": "ZmnQa_APb40l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "047af7238e4d49229153ca3d5930adbc",
            "d65d8026b69f4c528e61fa223d0f1341",
            "261b4af453d34b4b92c8b414ab520900",
            "0f2e15b8074042f682af1e56aeb12bdc",
            "1d70dd3091ad40fbb743a2120cc67650",
            "17a2f4f13d334774a3bf55fa7c0994d1",
            "4f2b38729666480cb286d13ea9cf7ea5",
            "ebef494a22074eb09bd9e94619a7dc80",
            "3fc3dc95ff0540d5ad07e636b4ff86dd",
            "30a14f42120d4526ae2b275d369080b1",
            "e64bbb2ac48a4a40a4b5786fdbd74b5b",
            "f6e3825d776f468e97b0912e4096efa8",
            "655c1999b018422ca9706068fc41dd7e",
            "23c7835f47fa42fc9ad2892a64dda8d5",
            "82280a5f18244842bc4ff723ec088504",
            "991e2873f40c4b81959db78c73735118",
            "a61df096756d42eb8df9226cbfd8be28",
            "13397cd2819f4dcabd96fd6baad47e50",
            "7e2c5b829c314b409b620f210a9ba442",
            "5a7df00216ce46bba5fd5c3536487e80",
            "b27d0fbeccdd4a40a12dd5fdc5122400",
            "d89b70066b6c437c890e74a53678d2c2"
          ]
        },
        "id": "8dq0bnGp5Cx4",
        "outputId": "397e4cf3-2633-478f-88d9-bae8dd56c39d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=3):   0%|          | 0/10 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "047af7238e4d49229153ca3d5930adbc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1692 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=3):   0%|          | 0/10 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6e3825d776f468e97b0912e4096efa8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1028 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1238 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "#Preparing the datasets for token classification\n",
        "data = json.load(open('train.json'))\n",
        "\n",
        "train_data, val_data = train_test_split(data, test_size=0.15, random_state=42)\n",
        "\n",
        "# Slice the first 10 elements for debugging\n",
        "train_data = train_data[:10]\n",
        "val_data = val_data[:10]\n",
        "\n",
        "trainset = datasets.Dataset.from_dict({\n",
        "    'full_text': [x['full_text'] for x in train_data],\n",
        "    'document': [x['document'] for x in train_data],\n",
        "    'tokens': [x['tokens'] for x in train_data],\n",
        "    'trailing_whitespace': [x['trailing_whitespace'] for x in train_data],\n",
        "    'labels' :[x['labels'] for x in train_data]\n",
        "})\n",
        "\n",
        "trainset = trainset.map(\n",
        "    tokenize,\n",
        "    fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id},\n",
        "    num_proc=3\n",
        ")\n",
        "\n",
        "valset = datasets.Dataset.from_dict({\n",
        "    'full_text': [x['full_text'] for x in val_data],\n",
        "    'document': [x['document'] for x in val_data],\n",
        "    'tokens': [x['tokens'] for x in val_data],\n",
        "    'trailing_whitespace': [x['trailing_whitespace'] for x in val_data],\n",
        "    'labels' :[x['labels'] for x in val_data]\n",
        "})\n",
        "\n",
        "\n",
        "valset = valset.map(\n",
        "    tokenize,\n",
        "    fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id},\n",
        "    num_proc=3\n",
        ")\n",
        "\n",
        "del data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple checks to ensure dataset integrity\n",
        "assert 'input_ids' in trainset.features, \"Input IDs should be part of the dataset.\"\n",
        "assert 'labels' in trainset.features, \"Labels should be part of the dataset.\"\n",
        "\n",
        "print(\"Training Set Sample:\", trainset[0])\n",
        "print(\"Validation Set Sample:\", valset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ezy1dEHdSGb",
        "outputId": "a7c23701-79c9-4c62-80f0-140228b377a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Sample: {'full_text': 'Challenge    Broadly speaking, the challenge is raising money for non-profit organizations. More specifically,  I recently joined the boards of two non-profit organizations for which I have or will have a fund  raising role. The first board I joined supports entrepreneurs and start-ups “from founding  through funding,” and I am a mentor to start-up companies as part of this work. The second  board I joined supports a school; starting in September, I will be leading an effort to raise  money for student travel fellowships.     As I joined the board, the entrepreneur support organization had just begun a $1 million fund  raising campaign that was essential to its sustainability. Then, the COVID-19 pandemic struck. I  was asked to solicit my 30 peers in the mentoring group. I would ask them to join me in  donating $5,000 for each of the next three years.     Selection    The tool I chose was storytelling, and this pre-dated my having begun this design thinking class.  I chose storytelling because I would be asking people for money in a frightening economic  environment, and I wanted my peers to understand the “human” story of my commitment to  the organization. In other words, I wanted the ask to be real and relatable.     Application    I wrote each of my peer mentors and told the story of my having been introduced to the  organization through its “pitch day” and then becoming a mentor, donor, and board member.  Following my message, a founder/fellow board member “piled on” with a follow-up email. The  messages also included specifics about all the great work we’ve been doing.    We received some responses right away, and while these typically included an enthusiastic  message about the organization, nearly none included a commitment of money. We followed  up with reminder emails. Responses that came in the next week or two were similar. In two or  three cases, there was no response, even as (a few weeks later) we successfully wound up the  campaign of which my “mentor ask” was a part.    We raised a small amount – not the money we hoped! We suspected that some mentors were  in no position to commit, but we did not leave anyone out. We worried that the message was  off, but we got no feedback to this effect. I spoke to one of the younger mentors whom I knew  was between jobs; she thought my story was effective and the overall messaging strong.    Insight    While the storytelling tool was used effectively, this was inserted into an existing process that  had not been conceived using design thinking. Notwithstanding, our efforts were probably an\\n\\nexample of rapid prototyping. The fund-raising campaign would have benefitted from a design  thinking process that started at the beginning.    Approach    Both this fund-raising campaign and the effort I will lead in September (for the school’s  traveling fellowships) would benefit by starting at the beginning of a design thinking process  with ethnography/journey mapping. Simply, we need to understand our prospective donors  (customers) better. As well, the “What If” part of the process could prove highly valuable for  the school’s campaign. For both organizations I serve, storytelling is essential and potentially  persuasive to donors. In one case, there is rich material in the entrepreneurs’ stories, and in the  other, the student travelers’ stories can potentially make the campaign more relatable.\\n\\n', 'document': 19404, 'tokens': ['Challenge', '   ', 'Broadly', 'speaking', ',', 'the', 'challenge', 'is', 'raising', 'money', 'for', 'non', '-', 'profit', 'organizations', '.', 'More', 'specifically', ',', ' ', 'I', 'recently', 'joined', 'the', 'boards', 'of', 'two', 'non', '-', 'profit', 'organizations', 'for', 'which', 'I', 'have', 'or', 'will', 'have', 'a', 'fund', ' ', 'raising', 'role', '.', 'The', 'first', 'board', 'I', 'joined', 'supports', 'entrepreneurs', 'and', 'start', '-', 'ups', '“', 'from', 'founding', ' ', 'through', 'funding', ',', '”', 'and', 'I', 'am', 'a', 'mentor', 'to', 'start', '-', 'up', 'companies', 'as', 'part', 'of', 'this', 'work', '.', 'The', 'second', ' ', 'board', 'I', 'joined', 'supports', 'a', 'school', ';', 'starting', 'in', 'September', ',', 'I', 'will', 'be', 'leading', 'an', 'effort', 'to', 'raise', ' ', 'money', 'for', 'student', 'travel', 'fellowships', '.', '    ', 'As', 'I', 'joined', 'the', 'board', ',', 'the', 'entrepreneur', 'support', 'organization', 'had', 'just', 'begun', 'a', '$', '1', 'million', 'fund', ' ', 'raising', 'campaign', 'that', 'was', 'essential', 'to', 'its', 'sustainability', '.', 'Then', ',', 'the', 'COVID-19', 'pandemic', 'struck', '.', 'I', ' ', 'was', 'asked', 'to', 'solicit', 'my', '30', 'peers', 'in', 'the', 'mentoring', 'group', '.', 'I', 'would', 'ask', 'them', 'to', 'join', 'me', 'in', ' ', 'donating', '$', '5,000', 'for', 'each', 'of', 'the', 'next', 'three', 'years', '.', '    ', 'Selection', '   ', 'The', 'tool', 'I', 'chose', 'was', 'storytelling', ',', 'and', 'this', 'pre', '-', 'dated', 'my', 'having', 'begun', 'this', 'design', 'thinking', 'class', '.', ' ', 'I', 'chose', 'storytelling', 'because', 'I', 'would', 'be', 'asking', 'people', 'for', 'money', 'in', 'a', 'frightening', 'economic', ' ', 'environment', ',', 'and', 'I', 'wanted', 'my', 'peers', 'to', 'understand', 'the', '“', 'human', '”', 'story', 'of', 'my', 'commitment', 'to', ' ', 'the', 'organization', '.', 'In', 'other', 'words', ',', 'I', 'wanted', 'the', 'ask', 'to', 'be', 'real', 'and', 'relatable', '.', '    ', 'Application', '   ', 'I', 'wrote', 'each', 'of', 'my', 'peer', 'mentors', 'and', 'told', 'the', 'story', 'of', 'my', 'having', 'been', 'introduced', 'to', 'the', ' ', 'organization', 'through', 'its', '“', 'pitch', 'day', '”', 'and', 'then', 'becoming', 'a', 'mentor', ',', 'donor', ',', 'and', 'board', 'member', '.', ' ', 'Following', 'my', 'message', ',', 'a', 'founder', '/', 'fellow', 'board', 'member', '“', 'piled', 'on', '”', 'with', 'a', 'follow', '-', 'up', 'email', '.', 'The', ' ', 'messages', 'also', 'included', 'specifics', 'about', 'all', 'the', 'great', 'work', 'we', '’ve', 'been', 'doing', '.', '   ', 'We', 'received', 'some', 'responses', 'right', 'away', ',', 'and', 'while', 'these', 'typically', 'included', 'an', 'enthusiastic', ' ', 'message', 'about', 'the', 'organization', ',', 'nearly', 'none', 'included', 'a', 'commitment', 'of', 'money', '.', 'We', 'followed', ' ', 'up', 'with', 'reminder', 'emails', '.', 'Responses', 'that', 'came', 'in', 'the', 'next', 'week', 'or', 'two', 'were', 'similar', '.', 'In', 'two', 'or', ' ', 'three', 'cases', ',', 'there', 'was', 'no', 'response', ',', 'even', 'as', '(', 'a', 'few', 'weeks', 'later', ')', 'we', 'successfully', 'wound', 'up', 'the', ' ', 'campaign', 'of', 'which', 'my', '“', 'mentor', 'ask', '”', 'was', 'a', 'part', '.', '   ', 'We', 'raised', 'a', 'small', 'amount', '–', 'not', 'the', 'money', 'we', 'hoped', '!', 'We', 'suspected', 'that', 'some', 'mentors', 'were', ' ', 'in', 'no', 'position', 'to', 'commit', ',', 'but', 'we', 'did', 'not', 'leave', 'anyone', 'out', '.', 'We', 'worried', 'that', 'the', 'message', 'was', ' ', 'off', ',', 'but', 'we', 'got', 'no', 'feedback', 'to', 'this', 'effect', '.', 'I', 'spoke', 'to', 'one', 'of', 'the', 'younger', 'mentors', 'whom', 'I', 'knew', ' ', 'was', 'between', 'jobs', ';', 'she', 'thought', 'my', 'story', 'was', 'effective', 'and', 'the', 'overall', 'messaging', 'strong', '.', '   ', 'Insight', '   ', 'While', 'the', 'storytelling', 'tool', 'was', 'used', 'effectively', ',', 'this', 'was', 'inserted', 'into', 'an', 'existing', 'process', 'that', ' ', 'had', 'not', 'been', 'conceived', 'using', 'design', 'thinking', '.', 'Notwithstanding', ',', 'our', 'efforts', 'were', 'probably', 'an', '\\n\\n', 'example', 'of', 'rapid', 'prototyping', '.', 'The', 'fund', '-', 'raising', 'campaign', 'would', 'have', 'benefitted', 'from', 'a', 'design', ' ', 'thinking', 'process', 'that', 'started', 'at', 'the', 'beginning', '.', '   ', 'Approach', '   ', 'Both', 'this', 'fund', '-', 'raising', 'campaign', 'and', 'the', 'effort', 'I', 'will', 'lead', 'in', 'September', '(', 'for', 'the', 'school', '’s', ' ', 'traveling', 'fellowships', ')', 'would', 'benefit', 'by', 'starting', 'at', 'the', 'beginning', 'of', 'a', 'design', 'thinking', 'process', ' ', 'with', 'ethnography', '/', 'journey', 'mapping', '.', 'Simply', ',', 'we', 'need', 'to', 'understand', 'our', 'prospective', 'donors', ' ', '(', 'customers', ')', 'better', '.', 'As', 'well', ',', 'the', '“', 'What', 'If', '”', 'part', 'of', 'the', 'process', 'could', 'prove', 'highly', 'valuable', 'for', ' ', 'the', 'school', '’s', 'campaign', '.', 'For', 'both', 'organizations', 'I', 'serve', ',', 'storytelling', 'is', 'essential', 'and', 'potentially', ' ', 'persuasive', 'to', 'donors', '.', 'In', 'one', 'case', ',', 'there', 'is', 'rich', 'material', 'in', 'the', 'entrepreneurs', '’', 'stories', ',', 'and', 'in', 'the', ' ', 'other', ',', 'the', 'student', 'travelers', '’', 'stories', 'can', 'potentially', 'make', 'the', 'campaign', 'more', 'relatable', '.', '\\n\\n'], 'trailing_whitespace': [True, False, True, False, True, True, True, True, True, True, True, False, False, True, False, True, True, False, True, False, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, False, False, True, False, True, True, False, True, False, False, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, False, True, False, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, False, True, False, True, False, True, True, True, True, True, False, True, True, True, False, False, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, False, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, False, True, True, True, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, True, False, False, True, True, True, False, True, False, True, True, True, False, False, True, False, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, False, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, False, True, True, True, True, False, True, True, True, False, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, False, True, False, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, False, True, True, True, True, True, False, False, True, True, True, False, True, True, False, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, False, True, False, True, True, False, False, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, False, True, False, True, False, True, True, True, True, True, True, True, True, False, False, False, True, False, True, True, False, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, False, True, False, True, False, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, False, True, True, True, False, True, True, True, True, True, True, True, False, True, False, True, True, True, True, False, False, True, True, True, False, True, True, True, True, True, True, True, True, False, False, False], 'labels': [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], 'input_ids': [0, 10045, 1437, 1437, 1437, 8397, 352, 2686, 6, 5, 1539, 16, 3282, 418, 13, 786, 12, 7699, 2665, 4, 901, 4010, 6, 1437, 38, 682, 1770, 5, 6904, 9, 80, 786, 12, 7699, 2665, 13, 61, 38, 33, 50, 40, 33, 10, 1391, 1437, 3282, 774, 4, 20, 78, 792, 38, 1770, 4548, 8666, 8, 386, 12, 4489, 44, 48, 7761, 10150, 1437, 149, 1435, 6, 17, 46, 8, 38, 524, 10, 11989, 7, 386, 12, 658, 451, 25, 233, 9, 42, 173, 4, 20, 200, 1437, 792, 38, 1770, 4548, 10, 334, 131, 1158, 11, 772, 6, 38, 40, 28, 981, 41, 1351, 7, 1693, 1437, 418, 13, 1294, 1504, 36304, 7903, 4, 1437, 1437, 1437, 1437, 287, 38, 1770, 5, 792, 6, 5, 11777, 323, 1651, 56, 95, 5812, 10, 68, 134, 153, 1391, 1437, 3282, 637, 14, 21, 4499, 7, 63, 11128, 4, 1892, 6, 5, 6247, 43814, 12, 1646, 23387, 14414, 2322, 4, 38, 1437, 21, 553, 7, 22706, 127, 389, 6763, 11, 5, 12906, 5137, 333, 4, 38, 74, 1394, 106, 7, 1962, 162, 11, 1437, 16831, 68, 245, 6, 151, 13, 349, 9, 5, 220, 130, 107, 4, 1437, 1437, 1437, 1437, 30418, 1437, 1437, 1437, 20, 3944, 38, 4689, 21, 17662, 6, 8, 42, 1198, 12, 26064, 127, 519, 5812, 42, 1521, 2053, 1380, 4, 1437, 38, 4689, 17662, 142, 38, 74, 28, 1996, 82, 13, 418, 11, 10, 21111, 776, 1437, 1737, 6, 8, 38, 770, 127, 6763, 7, 1346, 5, 44, 48, 19003, 17, 46, 527, 9, 127, 2720, 7, 1437, 5, 1651, 4, 96, 97, 1617, 6, 38, 770, 5, 1394, 7, 28, 588, 8, 6258, 18564, 4, 1437, 1437, 1437, 1437, 11199, 1437, 1437, 1437, 38, 875, 349, 9, 127, 10995, 23854, 8, 174, 5, 527, 9, 127, 519, 57, 2942, 7, 5, 1437, 1651, 149, 63, 44, 48, 642, 3239, 183, 17, 46, 8, 172, 1959, 10, 11989, 6, 12125, 6, 8, 792, 919, 4, 1437, 3515, 127, 1579, 6, 10, 3787, 73, 506, 33796, 792, 919, 44, 48, 642, 6691, 15, 17, 46, 19, 10, 1407, 12, 658, 1047, 4, 20, 1437, 3731, 67, 1165, 16047, 59, 70, 5, 372, 173, 52, 17, 27, 548, 57, 608, 4, 1437, 1437, 1437, 166, 829, 103, 8823, 235, 409, 6, 8, 150, 209, 3700, 1165, 41, 15947, 1437, 1579, 59, 5, 1651, 6, 823, 4146, 1165, 10, 2720, 9, 418, 4, 166, 1432, 1437, 62, 19, 8306, 5575, 4, 25714, 293, 14, 376, 11, 5, 220, 186, 50, 80, 58, 1122, 4, 96, 80, 50, 1437, 130, 1200, 6, 89, 21, 117, 1263, 6, 190, 25, 36, 102, 367, 688, 423, 43, 52, 5116, 7725, 62, 5, 1437, 637, 9, 61, 127, 44, 48, 1757, 368, 1394, 17, 46, 21, 10, 233, 4, 1437, 1437, 1437, 166, 1179, 10, 650, 1280, 126, 45, 5, 418, 52, 5207, 328, 166, 3986, 14, 103, 23854, 58, 1437, 11, 117, 737, 7, 6225, 6, 53, 52, 222, 45, 989, 1268, 66, 4, 166, 3915, 14, 5, 1579, 21, 1437, 160, 6, 53, 52, 300, 117, 6456, 7, 42, 1683, 4, 38, 1834, 7, 65, 9, 5, 3240, 23854, 2661, 38, 1467, 1437, 21, 227, 1315, 131, 79, 802, 127, 527, 21, 2375, 8, 5, 1374, 11203, 670, 4, 1437, 1437, 1437, 28209, 1437, 1437, 1437, 616, 5, 17662, 3944, 21, 341, 4296, 6, 42, 21, 24557, 88, 41, 2210, 609, 14, 1437, 56, 45, 57, 23426, 634, 1521, 2053, 4, 42632, 6, 84, 1170, 58, 1153, 41, 50118, 50118, 46781, 9, 6379, 40004, 154, 4, 20, 1391, 12, 21973, 637, 74, 33, 19018, 16430, 31, 10, 1521, 1437, 2053, 609, 14, 554, 23, 5, 1786, 4, 1437, 1437, 1437, 38370, 1437, 1437, 1437, 1868, 42, 1391, 12, 21973, 637, 8, 5, 1351, 38, 40, 483, 11, 772, 36, 1990, 5, 334, 17, 27, 29, 1437, 5796, 36304, 7903, 43, 74, 1796, 30, 1158, 23, 5, 1786, 9, 10, 1521, 2053, 609, 1437, 19, 35526, 10486, 73, 267, 37786, 20410, 4, 17449, 6, 52, 240, 7, 1346, 84, 12429, 9398, 1437, 36, 31458, 268, 43, 357, 4, 287, 157, 6, 5, 44, 48, 2264, 318, 17, 46, 233, 9, 5, 609, 115, 3364, 2200, 5130, 13, 1437, 5, 334, 17, 27, 29, 637, 4, 286, 258, 2665, 38, 1807, 6, 17662, 16, 4499, 8, 2905, 1437, 38245, 7, 9398, 4, 96, 65, 403, 6, 89, 16, 4066, 1468, 11, 5, 8666, 17, 27, 1652, 6, 8, 11, 5, 1437, 97, 6, 5, 1294, 11774, 17, 27, 1652, 64, 2905, 146, 5, 637, 55, 6258, 18564, 4, 50140, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [[0, 0], [0, 9], [10, 10], [11, 11], [12, 12], [13, 18], [18, 20], [21, 29], [29, 30], [31, 34], [35, 44], [45, 47], [48, 55], [56, 61], [62, 65], [66, 69], [69, 70], [70, 76], [77, 90], [90, 91], [92, 96], [97, 109], [109, 110], [111, 111], [112, 113], [114, 122], [123, 129], [130, 133], [134, 140], [141, 143], [144, 147], [148, 151], [151, 152], [152, 158], [159, 172], [173, 176], [177, 182], [183, 184], [185, 189], [190, 192], [193, 197], [198, 202], [203, 204], [205, 209], [210, 210], [211, 218], [219, 223], [223, 224], [225, 228], [229, 234], [235, 240], [241, 242], [243, 249], [250, 258], [259, 272], [273, 276], [277, 282], [282, 283], [283, 286], [287, 288], [287, 288], [288, 292], [293, 301], [302, 302], [303, 310], [311, 318], [318, 319], [319, 320], [319, 320], [321, 324], [325, 326], [327, 329], [330, 331], [332, 338], [339, 341], [342, 347], [347, 348], [348, 350], [351, 360], [361, 363], [364, 368], [369, 371], [372, 376], [377, 381], [381, 382], [383, 386], [387, 393], [394, 394], [395, 400], [401, 402], [403, 409], [410, 418], [419, 420], [421, 427], [427, 428], [429, 437], [438, 440], [441, 450], [450, 451], [452, 453], [454, 458], [459, 461], [462, 469], [470, 472], [473, 479], [480, 482], [483, 488], [489, 489], [490, 495], [496, 499], [500, 507], [508, 514], [515, 522], [522, 526], [526, 527], [528, 528], [529, 529], [530, 530], [531, 531], [532, 534], [535, 536], [537, 543], [544, 547], [548, 553], [553, 554], [555, 558], [559, 571], [572, 579], [580, 592], [593, 596], [597, 601], [602, 607], [608, 609], [610, 611], [611, 612], [613, 620], [621, 625], [626, 626], [627, 634], [635, 643], [644, 648], [649, 652], [653, 662], [663, 665], [666, 669], [670, 684], [684, 685], [686, 690], [690, 691], [692, 695], [696, 698], [698, 701], [701, 702], [702, 704], [705, 709], [709, 713], [714, 720], [720, 721], [722, 723], [724, 724], [725, 728], [729, 734], [735, 737], [738, 745], [746, 748], [749, 751], [752, 757], [758, 760], [761, 764], [765, 769], [769, 774], [775, 780], [780, 781], [782, 783], [784, 789], [790, 793], [794, 798], [799, 801], [802, 806], [807, 809], [810, 812], [813, 813], [814, 822], [823, 824], [824, 825], [825, 826], [826, 829], [830, 833], [834, 838], [839, 841], [842, 845], [846, 850], [851, 856], [857, 862], [862, 863], [864, 864], [865, 865], [866, 866], [867, 867], [868, 877], [878, 878], [879, 879], [880, 880], [881, 884], [885, 889], [890, 891], [892, 897], [898, 901], [902, 914], [914, 915], [916, 919], [920, 924], [925, 928], [928, 929], [929, 934], [935, 937], [938, 944], [945, 950], [951, 955], [956, 962], [963, 971], [972, 977], [977, 978], [979, 979], [980, 981], [982, 987], [988, 1000], [1001, 1008], [1009, 1010], [1011, 1016], [1017, 1019], [1020, 1026], [1027, 1033], [1034, 1037], [1038, 1043], [1044, 1046], [1047, 1048], [1049, 1060], [1061, 1069], [1070, 1070], [1071, 1082], [1082, 1083], [1084, 1087], [1088, 1089], [1090, 1096], [1097, 1099], [1100, 1105], [1106, 1108], [1109, 1119], [1120, 1123], [1124, 1125], [1124, 1125], [1125, 1130], [1130, 1131], [1130, 1131], [1132, 1137], [1138, 1140], [1141, 1143], [1144, 1154], [1155, 1157], [1158, 1158], [1159, 1162], [1163, 1175], [1175, 1176], [1177, 1179], [1180, 1185], [1186, 1191], [1191, 1192], [1193, 1194], [1195, 1201], [1202, 1205], [1206, 1209], [1210, 1212], [1213, 1215], [1216, 1220], [1221, 1224], [1225, 1228], [1228, 1234], [1234, 1235], [1236, 1236], [1237, 1237], [1238, 1238], [1239, 1239], [1240, 1251], [1252, 1252], [1253, 1253], [1254, 1254], [1255, 1256], [1257, 1262], [1263, 1267], [1268, 1270], [1271, 1273], [1274, 1278], [1279, 1286], [1287, 1290], [1291, 1295], [1296, 1299], [1300, 1305], [1306, 1308], [1309, 1311], [1312, 1318], [1319, 1323], [1324, 1334], [1335, 1337], [1338, 1341], [1342, 1342], [1343, 1355], [1356, 1363], [1364, 1367], [1368, 1369], [1368, 1369], [1369, 1370], [1370, 1374], [1375, 1378], [1378, 1379], [1378, 1379], [1380, 1383], [1384, 1388], [1389, 1397], [1398, 1399], [1400, 1406], [1406, 1407], [1408, 1413], [1413, 1414], [1415, 1418], [1419, 1424], [1425, 1431], [1431, 1432], [1433, 1433], [1434, 1443], [1444, 1446], [1447, 1454], [1454, 1455], [1456, 1457], [1458, 1465], [1465, 1466], [1466, 1467], [1467, 1472], [1473, 1478], [1479, 1485], [1486, 1487], [1486, 1487], [1487, 1488], [1488, 1492], [1493, 1495], [1495, 1496], [1495, 1496], [1497, 1501], [1502, 1503], [1504, 1510], [1510, 1511], [1511, 1513], [1514, 1519], [1519, 1520], [1521, 1524], [1525, 1525], [1526, 1534], [1535, 1539], [1540, 1548], [1549, 1558], [1559, 1564], [1565, 1568], [1569, 1572], [1573, 1578], [1579, 1583], [1584, 1586], [1586, 1587], [1586, 1587], [1587, 1589], [1590, 1594], [1595, 1600], [1600, 1601], [1602, 1602], [1603, 1603], [1604, 1604], [1605, 1607], [1608, 1616], [1617, 1621], [1622, 1631], [1632, 1637], [1638, 1642], [1642, 1643], [1644, 1647], [1648, 1653], [1654, 1659], [1660, 1669], [1670, 1678], [1679, 1681], [1682, 1694], [1695, 1695], [1696, 1703], [1704, 1709], [1710, 1713], [1714, 1726], [1726, 1727], [1728, 1734], [1735, 1739], [1740, 1748], [1749, 1750], [1751, 1761], [1762, 1764], [1765, 1770], [1770, 1771], [1772, 1774], [1775, 1783], [1784, 1784], [1785, 1787], [1788, 1792], [1793, 1801], [1802, 1808], [1808, 1809], [1810, 1817], [1817, 1819], [1820, 1824], [1825, 1829], [1830, 1832], [1833, 1836], [1837, 1841], [1842, 1846], [1847, 1849], [1850, 1853], [1854, 1858], [1859, 1866], [1866, 1867], [1868, 1870], [1871, 1874], [1875, 1877], [1878, 1878], [1879, 1884], [1885, 1890], [1890, 1891], [1892, 1897], [1898, 1901], [1902, 1904], [1905, 1913], [1913, 1914], [1915, 1919], [1920, 1922], [1923, 1924], [1924, 1925], [1926, 1929], [1930, 1935], [1936, 1941], [1941, 1942], [1943, 1945], [1946, 1958], [1959, 1964], [1965, 1967], [1968, 1971], [1972, 1972], [1973, 1981], [1982, 1984], [1985, 1990], [1991, 1993], [1994, 1995], [1994, 1995], [1995, 1999], [1999, 2001], [2002, 2005], [2005, 2006], [2005, 2006], [2007, 2010], [2011, 2012], [2013, 2017], [2017, 2018], [2019, 2019], [2020, 2020], [2021, 2021], [2022, 2024], [2025, 2031], [2032, 2033], [2034, 2039], [2040, 2046], [2047, 2048], [2049, 2052], [2053, 2056], [2057, 2062], [2063, 2065], [2066, 2071], [2071, 2072], [2073, 2075], [2076, 2085], [2086, 2090], [2091, 2095], [2096, 2103], [2104, 2108], [2109, 2109], [2110, 2112], [2113, 2115], [2116, 2124], [2125, 2127], [2128, 2134], [2134, 2135], [2136, 2139], [2140, 2142], [2143, 2146], [2147, 2150], [2151, 2156], [2157, 2163], [2164, 2167], [2167, 2168], [2169, 2171], [2172, 2179], [2180, 2184], [2185, 2188], [2189, 2196], [2197, 2200], [2201, 2201], [2202, 2205], [2205, 2206], [2207, 2210], [2211, 2213], [2214, 2217], [2218, 2220], [2221, 2229], [2230, 2232], [2233, 2237], [2238, 2244], [2244, 2245], [2246, 2247], [2248, 2253], [2254, 2256], [2257, 2260], [2261, 2263], [2264, 2267], [2268, 2275], [2276, 2283], [2284, 2288], [2289, 2290], [2291, 2295], [2296, 2296], [2297, 2300], [2301, 2308], [2309, 2313], [2313, 2314], [2315, 2318], [2319, 2326], [2327, 2329], [2330, 2335], [2336, 2339], [2340, 2349], [2350, 2353], [2354, 2357], [2358, 2365], [2366, 2375], [2376, 2382], [2382, 2383], [2384, 2384], [2385, 2385], [2386, 2386], [2387, 2394], [2395, 2395], [2396, 2396], [2397, 2397], [2398, 2403], [2404, 2407], [2408, 2420], [2421, 2425], [2426, 2429], [2430, 2434], [2435, 2446], [2446, 2447], [2448, 2452], [2453, 2456], [2457, 2465], [2466, 2470], [2471, 2473], [2474, 2482], [2483, 2490], [2491, 2495], [2496, 2496], [2497, 2500], [2501, 2504], [2505, 2509], [2510, 2519], [2520, 2525], [2526, 2532], [2533, 2541], [2541, 2542], [2543, 2558], [2558, 2559], [2560, 2563], [2564, 2571], [2572, 2576], [2577, 2585], [2586, 2588], [2588, 2589], [2589, 2590], [2590, 2597], [2598, 2600], [2601, 2606], [2607, 2615], [2615, 2618], [2618, 2619], [2620, 2623], [2624, 2628], [2628, 2629], [2629, 2636], [2637, 2645], [2646, 2651], [2652, 2656], [2657, 2662], [2662, 2667], [2668, 2672], [2673, 2674], [2675, 2681], [2682, 2682], [2683, 2691], [2692, 2699], [2700, 2704], [2705, 2712], [2713, 2715], [2716, 2719], [2720, 2729], [2729, 2730], [2731, 2731], [2732, 2732], [2733, 2733], [2734, 2742], [2743, 2743], [2744, 2744], [2745, 2745], [2746, 2750], [2751, 2755], [2756, 2760], [2760, 2761], [2761, 2768], [2769, 2777], [2778, 2781], [2782, 2785], [2786, 2792], [2793, 2794], [2795, 2799], [2800, 2804], [2805, 2807], [2808, 2817], [2818, 2819], [2819, 2822], [2823, 2826], [2827, 2833], [2833, 2834], [2833, 2834], [2834, 2835], [2836, 2836], [2837, 2846], [2847, 2854], [2854, 2858], [2858, 2859], [2860, 2865], [2866, 2873], [2874, 2876], [2877, 2885], [2886, 2888], [2889, 2892], [2893, 2902], [2903, 2905], [2906, 2907], [2908, 2914], [2915, 2923], [2924, 2931], [2932, 2932], [2933, 2937], [2938, 2942], [2942, 2949], [2949, 2950], [2950, 2951], [2951, 2957], [2958, 2965], [2965, 2966], [2967, 2973], [2973, 2974], [2975, 2977], [2978, 2982], [2983, 2985], [2986, 2996], [2997, 3000], [3001, 3012], [3013, 3019], [3020, 3020], [3021, 3022], [3022, 3028], [3028, 3031], [3031, 3032], [3033, 3039], [3039, 3040], [3041, 3043], [3044, 3048], [3048, 3049], [3050, 3053], [3054, 3055], [3054, 3055], [3055, 3059], [3060, 3062], [3062, 3063], [3062, 3063], [3064, 3068], [3069, 3071], [3072, 3075], [3076, 3083], [3084, 3089], [3090, 3095], [3096, 3102], [3103, 3111], [3112, 3115], [3116, 3116], [3117, 3120], [3121, 3127], [3127, 3128], [3127, 3128], [3128, 3129], [3130, 3138], [3138, 3139], [3140, 3143], [3144, 3148], [3149, 3162], [3163, 3164], [3165, 3170], [3170, 3171], [3172, 3184], [3185, 3187], [3188, 3197], [3198, 3201], [3202, 3213], [3214, 3214], [3215, 3225], [3226, 3228], [3229, 3235], [3235, 3236], [3237, 3239], [3240, 3243], [3244, 3248], [3248, 3249], [3250, 3255], [3256, 3258], [3259, 3263], [3264, 3272], [3273, 3275], [3276, 3279], [3280, 3293], [3293, 3294], [3293, 3294], [3295, 3302], [3302, 3303], [3304, 3307], [3308, 3310], [3311, 3314], [3315, 3315], [3316, 3321], [3321, 3322], [3323, 3326], [3327, 3334], [3335, 3344], [3344, 3345], [3344, 3345], [3346, 3353], [3354, 3357], [3358, 3369], [3370, 3374], [3375, 3378], [3379, 3387], [3388, 3392], [3393, 3396], [3396, 3402], [3402, 3403], [3403, 3405], [0, 0]]}\n",
            "Validation Set Sample: {'full_text': 'Assignment: Mindmapping\\n\\nChallenge\\n\\nThe challenge was to develop a three-view mechanism for my class this semester, it was\\n\\ntogether with my team to find a solution to this problem, which was intended to transmit\\n\\nthree messages in an advertisement.\\n\\nSelection\\n\\nI chose the Mindmapping tool because that tool was taught to me in my class this semester\\n\\nand it was very useful to apply it to this problem since Mindmapping gives you the\\n\\nopportunity to be more organized in terms of your ideas and fundamentals that are\\n\\nconnected between yes. Also, it is important to say that this tool is developed throughout\\n\\nthe project, in the beginning, in the development and when we had everything clearer and\\n\\nat the end to know the organization that should be carried out and the order.\\n\\nApplication\\n\\nAs I mentioned before, I applied this tool in my project due to the order that it takes, at the\\n\\ntime of making the prototype of the project, a mindmap was made about what materials\\n\\nwas going to be used, how it was going to work, all that brainstorming was useful organize\\n\\nit in a mindmap to carry out the project. Also at the time of developing the poster, a\\n\\nmindmap was made of how each section should be organized and what information each\\n\\none should carry, in this way the mindmap tool was applied and it was possible to have a\\n\\nvery striking poster.\\n\\nInsight\\n\\nThe insight that I gained was very great, since in my other projects I had not used the\\n\\nmindmapping tool because I did not consider it useful, but at the time of applying the design\\n\\nthinking with the mindmapping tool and the knowledge acquired it was possible to carry\\n\\nout the project and obtain a perfect grade, appropriate to what the problem presented\\n\\nApproach\\n\\nNow that I have discovered the benefit of using the mindmapping tool, I think that what I\\n\\nwould do differently would be to make the most striking mindmaps because some\\n\\ncontained too much information that it was difficult to synthesize what the problem of the\\n\\nproject asked me to do.\\n\\n', 'document': 17809, 'tokens': ['Assignment', ':', 'Mindmapping', '\\n\\n', 'Challenge', '\\n\\n', 'The', 'challenge', 'was', 'to', 'develop', 'a', 'three', '-', 'view', 'mechanism', 'for', 'my', 'class', 'this', 'semester', ',', 'it', 'was', '\\n\\n', 'together', 'with', 'my', 'team', 'to', 'find', 'a', 'solution', 'to', 'this', 'problem', ',', 'which', 'was', 'intended', 'to', 'transmit', '\\n\\n', 'three', 'messages', 'in', 'an', 'advertisement', '.', '\\n\\n', 'Selection', '\\n\\n', 'I', 'chose', 'the', 'Mindmapping', 'tool', 'because', 'that', 'tool', 'was', 'taught', 'to', 'me', 'in', 'my', 'class', 'this', 'semester', '\\n\\n', 'and', 'it', 'was', 'very', 'useful', 'to', 'apply', 'it', 'to', 'this', 'problem', 'since', 'Mindmapping', 'gives', 'you', 'the', '\\n\\n', 'opportunity', 'to', 'be', 'more', 'organized', 'in', 'terms', 'of', 'your', 'ideas', 'and', 'fundamentals', 'that', 'are', '\\n\\n', 'connected', 'between', 'yes', '.', 'Also', ',', 'it', 'is', 'important', 'to', 'say', 'that', 'this', 'tool', 'is', 'developed', 'throughout', '\\n\\n', 'the', 'project', ',', 'in', 'the', 'beginning', ',', 'in', 'the', 'development', 'and', 'when', 'we', 'had', 'everything', 'clearer', 'and', '\\n\\n', 'at', 'the', 'end', 'to', 'know', 'the', 'organization', 'that', 'should', 'be', 'carried', 'out', 'and', 'the', 'order', '.', '\\n\\n', 'Application', '\\n\\n', 'As', 'I', 'mentioned', 'before', ',', 'I', 'applied', 'this', 'tool', 'in', 'my', 'project', 'due', 'to', 'the', 'order', 'that', 'it', 'takes', ',', 'at', 'the', '\\n\\n', 'time', 'of', 'making', 'the', 'prototype', 'of', 'the', 'project', ',', 'a', 'mindmap', 'was', 'made', 'about', 'what', 'materials', '\\n\\n', 'was', 'going', 'to', 'be', 'used', ',', 'how', 'it', 'was', 'going', 'to', 'work', ',', 'all', 'that', 'brainstorming', 'was', 'useful', 'organize', '\\n\\n', 'it', 'in', 'a', 'mindmap', 'to', 'carry', 'out', 'the', 'project', '.', 'Also', 'at', 'the', 'time', 'of', 'developing', 'the', 'poster', ',', 'a', '\\n\\n', 'mindmap', 'was', 'made', 'of', 'how', 'each', 'section', 'should', 'be', 'organized', 'and', 'what', 'information', 'each', '\\n\\n', 'one', 'should', 'carry', ',', 'in', 'this', 'way', 'the', 'mindmap', 'tool', 'was', 'applied', 'and', 'it', 'was', 'possible', 'to', 'have', 'a', '\\n\\n', 'very', 'striking', 'poster', '.', '\\n\\n', 'Insight', '\\n\\n', 'The', 'insight', 'that', 'I', 'gained', 'was', 'very', 'great', ',', 'since', 'in', 'my', 'other', 'projects', 'I', 'had', 'not', 'used', 'the', '\\n\\n', 'mindmapping', 'tool', 'because', 'I', 'did', 'not', 'consider', 'it', 'useful', ',', 'but', 'at', 'the', 'time', 'of', 'applying', 'the', 'design', '\\n\\n', 'thinking', 'with', 'the', 'mindmapping', 'tool', 'and', 'the', 'knowledge', 'acquired', 'it', 'was', 'possible', 'to', 'carry', '\\n\\n', 'out', 'the', 'project', 'and', 'obtain', 'a', 'perfect', 'grade', ',', 'appropriate', 'to', 'what', 'the', 'problem', 'presented', '\\n\\n', 'Approach', '\\n\\n', 'Now', 'that', 'I', 'have', 'discovered', 'the', 'benefit', 'of', 'using', 'the', 'mindmapping', 'tool', ',', 'I', 'think', 'that', 'what', 'I', '\\n\\n', 'would', 'do', 'differently', 'would', 'be', 'to', 'make', 'the', 'most', 'striking', 'mindmaps', 'because', 'some', '\\n\\n', 'contained', 'too', 'much', 'information', 'that', 'it', 'was', 'difficult', 'to', 'synthesize', 'what', 'the', 'problem', 'of', 'the', '\\n\\n', 'project', 'asked', 'me', 'to', 'do', '.', '\\n\\n'], 'trailing_whitespace': [False, True, False, False, False, False, True, True, True, True, True, True, False, False, True, True, True, True, True, True, False, True, True, False, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, True, True, True, True, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, False, False, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, False, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, False, False, False, False, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, False, False, False], 'labels': [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], 'input_ids': [0, 46091, 35, 9201, 119, 12040, 50118, 50118, 41584, 20526, 50118, 50118, 133, 1539, 21, 7, 2179, 10, 130, 12, 5877, 9562, 13, 127, 1380, 42, 14412, 6, 24, 21, 50118, 50118, 31586, 19, 127, 165, 7, 465, 10, 2472, 7, 42, 936, 6, 61, 21, 3833, 7, 25556, 50118, 50118, 9983, 3731, 11, 41, 6859, 4, 50118, 50118, 14696, 20576, 50118, 50118, 100, 4689, 5, 9201, 119, 12040, 3944, 142, 14, 3944, 21, 5850, 7, 162, 11, 127, 1380, 42, 14412, 50118, 50118, 463, 24, 21, 182, 5616, 7, 3253, 24, 7, 42, 936, 187, 9201, 119, 12040, 2029, 47, 5, 50118, 50118, 10223, 21108, 1571, 7, 28, 55, 5798, 11, 1110, 9, 110, 2956, 8, 14584, 14, 32, 50118, 50118, 21618, 227, 4420, 4, 1578, 6, 24, 16, 505, 7, 224, 14, 42, 3944, 16, 2226, 1328, 50118, 50118, 627, 695, 6, 11, 5, 1786, 6, 11, 5, 709, 8, 77, 52, 56, 960, 18618, 8, 50118, 50118, 415, 5, 253, 7, 216, 5, 1651, 14, 197, 28, 2584, 66, 8, 5, 645, 4, 50118, 50118, 46345, 50118, 50118, 1620, 38, 2801, 137, 6, 38, 5049, 42, 3944, 11, 127, 695, 528, 7, 5, 645, 14, 24, 1239, 6, 23, 5, 50118, 50118, 958, 9, 442, 5, 17715, 9, 5, 695, 6, 10, 1508, 32557, 21, 156, 59, 99, 3183, 50118, 50118, 7325, 164, 7, 28, 341, 6, 141, 24, 21, 164, 7, 173, 6, 70, 14, 33692, 154, 21, 5616, 14192, 50118, 50118, 405, 11, 10, 1508, 32557, 7, 2324, 66, 5, 695, 4, 1578, 23, 5, 86, 9, 2623, 5, 11566, 6, 10, 50118, 50118, 28583, 32557, 21, 156, 9, 141, 349, 2810, 197, 28, 5798, 8, 99, 335, 349, 50118, 50118, 1264, 197, 2324, 6, 11, 42, 169, 5, 1508, 32557, 3944, 21, 5049, 8, 24, 21, 678, 7, 33, 10, 50118, 50118, 5525, 5690, 11566, 4, 50118, 50118, 1121, 32764, 50118, 50118, 133, 8339, 14, 38, 3491, 21, 182, 372, 6, 187, 11, 127, 97, 1377, 38, 56, 45, 341, 5, 50118, 50118, 28583, 119, 12040, 3944, 142, 38, 222, 45, 1701, 24, 5616, 6, 53, 23, 5, 86, 9, 9889, 5, 1521, 50118, 50118, 22710, 19, 5, 1508, 119, 12040, 3944, 8, 5, 2655, 3566, 24, 21, 678, 7, 2324, 50118, 50118, 995, 5, 695, 8, 6925, 10, 1969, 4978, 6, 3901, 7, 99, 5, 936, 2633, 50118, 50118, 19186, 32388, 50118, 50118, 5975, 14, 38, 33, 2967, 5, 1796, 9, 634, 5, 1508, 119, 12040, 3944, 6, 38, 206, 14, 99, 38, 50118, 50118, 14656, 109, 8225, 74, 28, 7, 146, 5, 144, 5690, 1508, 44754, 142, 103, 50118, 50118, 40157, 350, 203, 335, 14, 24, 21, 1202, 7, 33689, 2072, 99, 5, 936, 9, 5, 50118, 50118, 28258, 553, 162, 7, 109, 4, 50140, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [[0, 0], [0, 10], [10, 11], [12, 16], [16, 17], [17, 23], [23, 24], [24, 25], [25, 30], [30, 34], [34, 35], [35, 36], [36, 39], [40, 49], [50, 53], [54, 56], [57, 64], [65, 66], [67, 72], [72, 73], [73, 77], [78, 87], [88, 91], [92, 94], [95, 100], [101, 105], [106, 114], [114, 115], [116, 118], [119, 122], [122, 123], [123, 124], [124, 132], [133, 137], [138, 140], [141, 145], [146, 148], [149, 153], [154, 155], [156, 164], [165, 167], [168, 172], [173, 180], [180, 181], [182, 187], [188, 191], [192, 200], [201, 203], [204, 212], [212, 213], [213, 214], [214, 219], [220, 228], [229, 231], [232, 234], [235, 248], [248, 249], [249, 250], [250, 251], [251, 253], [253, 260], [260, 261], [261, 262], [262, 263], [264, 269], [270, 273], [274, 278], [278, 279], [279, 285], [286, 290], [291, 298], [299, 303], [304, 308], [309, 312], [313, 319], [320, 322], [323, 325], [326, 328], [329, 331], [332, 337], [338, 342], [343, 351], [351, 352], [352, 353], [353, 356], [357, 359], [360, 363], [364, 368], [369, 375], [376, 378], [379, 384], [385, 387], [388, 390], [391, 395], [396, 403], [404, 409], [410, 414], [414, 415], [415, 421], [422, 427], [428, 431], [432, 435], [435, 436], [436, 437], [437, 440], [440, 445], [445, 448], [449, 451], [452, 454], [455, 459], [460, 469], [470, 472], [473, 478], [479, 481], [482, 486], [487, 492], [493, 496], [497, 509], [510, 514], [515, 518], [518, 519], [519, 520], [520, 529], [530, 537], [538, 541], [541, 542], [543, 547], [547, 548], [549, 551], [552, 554], [555, 564], [565, 567], [568, 571], [572, 576], [577, 581], [582, 586], [587, 589], [590, 599], [600, 610], [610, 611], [611, 612], [612, 615], [616, 623], [623, 624], [625, 627], [628, 631], [632, 641], [641, 642], [643, 645], [646, 649], [650, 661], [662, 665], [666, 670], [671, 673], [674, 677], [678, 688], [689, 696], [697, 700], [700, 701], [701, 702], [702, 704], [705, 708], [709, 712], [713, 715], [716, 720], [721, 724], [725, 737], [738, 742], [743, 749], [750, 752], [753, 760], [761, 764], [765, 768], [769, 772], [773, 778], [778, 779], [779, 780], [780, 781], [781, 792], [792, 793], [793, 794], [794, 796], [797, 798], [799, 808], [809, 815], [815, 816], [817, 818], [819, 826], [827, 831], [832, 836], [837, 839], [840, 842], [843, 850], [851, 854], [855, 857], [858, 861], [862, 867], [868, 872], [873, 875], [876, 881], [881, 882], [883, 885], [886, 889], [889, 890], [890, 891], [891, 895], [896, 898], [899, 905], [906, 909], [910, 919], [920, 922], [923, 926], [927, 934], [934, 935], [936, 937], [938, 942], [942, 945], [946, 949], [950, 954], [955, 960], [961, 965], [966, 975], [975, 976], [976, 977], [977, 980], [981, 986], [987, 989], [990, 992], [993, 997], [997, 998], [999, 1002], [1003, 1005], [1006, 1009], [1010, 1015], [1016, 1018], [1019, 1023], [1023, 1024], [1025, 1028], [1029, 1033], [1034, 1044], [1044, 1047], [1048, 1051], [1052, 1058], [1059, 1067], [1067, 1068], [1068, 1069], [1069, 1071], [1072, 1074], [1075, 1076], [1077, 1081], [1081, 1084], [1085, 1087], [1088, 1093], [1094, 1097], [1098, 1101], [1102, 1109], [1109, 1110], [1111, 1115], [1116, 1118], [1119, 1122], [1123, 1127], [1128, 1130], [1131, 1141], [1142, 1145], [1146, 1152], [1152, 1153], [1154, 1155], [1155, 1156], [1156, 1157], [1157, 1161], [1161, 1164], [1165, 1168], [1169, 1173], [1174, 1176], [1177, 1180], [1181, 1185], [1186, 1193], [1194, 1200], [1201, 1203], [1204, 1213], [1214, 1217], [1218, 1222], [1223, 1234], [1235, 1239], [1239, 1240], [1240, 1241], [1241, 1244], [1245, 1251], [1252, 1257], [1257, 1258], [1259, 1261], [1262, 1266], [1267, 1270], [1271, 1274], [1275, 1279], [1279, 1282], [1283, 1287], [1288, 1291], [1292, 1299], [1300, 1303], [1304, 1306], [1307, 1310], [1311, 1319], [1320, 1322], [1323, 1327], [1328, 1329], [1329, 1330], [1330, 1331], [1331, 1335], [1336, 1344], [1345, 1351], [1351, 1352], [1352, 1353], [1353, 1354], [1354, 1356], [1356, 1361], [1361, 1362], [1362, 1363], [1363, 1366], [1367, 1374], [1375, 1379], [1380, 1381], [1382, 1388], [1389, 1392], [1393, 1397], [1398, 1403], [1403, 1404], [1405, 1410], [1411, 1413], [1414, 1416], [1417, 1422], [1423, 1431], [1432, 1433], [1434, 1437], [1438, 1441], [1442, 1446], [1447, 1450], [1450, 1451], [1451, 1452], [1452, 1456], [1456, 1457], [1457, 1463], [1464, 1468], [1469, 1476], [1477, 1478], [1479, 1482], [1483, 1486], [1487, 1495], [1496, 1498], [1499, 1505], [1505, 1506], [1507, 1510], [1511, 1513], [1514, 1517], [1518, 1522], [1523, 1525], [1526, 1534], [1535, 1538], [1539, 1545], [1545, 1546], [1546, 1547], [1547, 1555], [1556, 1560], [1561, 1564], [1565, 1569], [1569, 1570], [1570, 1576], [1577, 1581], [1582, 1585], [1586, 1589], [1590, 1599], [1600, 1608], [1609, 1611], [1612, 1615], [1616, 1624], [1625, 1627], [1628, 1633], [1633, 1634], [1634, 1635], [1635, 1638], [1639, 1642], [1643, 1650], [1651, 1654], [1655, 1661], [1662, 1663], [1664, 1671], [1672, 1677], [1677, 1678], [1679, 1690], [1691, 1693], [1694, 1698], [1699, 1702], [1703, 1710], [1711, 1720], [1720, 1721], [1721, 1722], [1722, 1725], [1725, 1730], [1730, 1731], [1731, 1732], [1732, 1735], [1736, 1740], [1741, 1742], [1743, 1747], [1748, 1758], [1759, 1762], [1763, 1770], [1771, 1773], [1774, 1779], [1780, 1783], [1784, 1788], [1788, 1789], [1789, 1795], [1796, 1800], [1800, 1801], [1802, 1803], [1804, 1809], [1810, 1814], [1815, 1819], [1820, 1821], [1821, 1822], [1822, 1823], [1823, 1828], [1829, 1831], [1832, 1843], [1844, 1849], [1850, 1852], [1853, 1855], [1856, 1860], [1861, 1864], [1865, 1869], [1870, 1878], [1879, 1883], [1883, 1887], [1888, 1895], [1896, 1900], [1900, 1901], [1901, 1902], [1902, 1911], [1912, 1915], [1916, 1920], [1921, 1932], [1933, 1937], [1938, 1940], [1941, 1944], [1945, 1954], [1955, 1957], [1958, 1965], [1965, 1968], [1969, 1973], [1974, 1977], [1978, 1985], [1986, 1988], [1989, 1992], [1992, 1993], [1993, 1994], [1994, 2001], [2002, 2007], [2008, 2010], [2011, 2013], [2014, 2016], [2016, 2017], [2017, 2019], [0, 0]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCdiHOKf5Cx4",
        "outputId": "85f5b3c2-02f9-447d-b23a-a9a0029f247a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training data: 10 || Number of validation data: 10\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of training data: {len(trainset)} || Number of validation data: {len(valset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class Custom_data(Dataset):\n",
        "    def __init__(self, data_dict):\n",
        "        self.data = data_dict['input_ids']\n",
        "        self.attention_mask = data_dict['attention_mask']\n",
        "        self.labels = data_dict['labels']\n",
        "        self.document = data_dict['document']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Directly create tensors without re-wrapping them\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.data[idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
        "            'labels': oh_encoder(self.labels[idx]),  # Using the modified oh_encoder\n",
        "            'document': torch.tensor(self.document[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "custom_train = Custom_data(trainset)\n",
        "custom_val = Custom_data(valset)"
      ],
      "metadata": {
        "id": "eW0WHZFCjidy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class Custom_data(Dataset):\n",
        "    def __init__(self, data_dict):\n",
        "        self.data = data_dict['input_ids']\n",
        "        self.attention_mask = data_dict['attention_mask']\n",
        "        self.labels = data_dict['labels']\n",
        "        self.document = data_dict['document']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Ensure all elements are converted to tensors before passing to the DataLoader\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.data[idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
        "            'labels': oh_encoder(self.labels[idx]),  # Assuming oh_encoder correctly returns a tensor\n",
        "            'document': torch.tensor(self.document[idx], dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "i8D-K7Odk96H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Custom_data(Dataset):\n",
        "    def __init__(self, data_dict):\n",
        "        self.data = data_dict['input_ids']\n",
        "        self.attention_mask = data_dict['attention_mask']\n",
        "        self.labels = data_dict['labels']\n",
        "        self.document = data_dict['document']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx]), torch.tensor(self.attention_mask[idx]), oh_encoder(self.labels[idx]), torch.tensor(self.document[idx])\n",
        "\n",
        "custom_train = Custom_data(trainset)\n",
        "custom_val = Custom_data(valset)\n"
      ],
      "metadata": {
        "id": "5dy_yH00lu8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the following chunk for custom_collate is edited"
      ],
      "metadata": {
        "id": "GCJOaCiyB_Nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "def custom_collate(batch, device):\n",
        "    input_ids, attention_mask, labels, doc = zip(*batch)\n",
        "\n",
        "    # Ensure the tensors are directly moved to device without re-wrapping them\n",
        "    padded_input_ids = pad_sequence([ids.to(device) for ids in input_ids], batch_first=True, padding_value=0)\n",
        "    padded_attention_mask = pad_sequence([mask.to(device) for mask in attention_mask], batch_first=True, padding_value=0)\n",
        "    padded_labels = pad_sequence([l.to(device) for l in labels], batch_first=True, padding_value=-100)\n",
        "\n",
        "    return {\n",
        "        'input_ids': padded_input_ids,\n",
        "        'attention_mask': padded_attention_mask,\n",
        "        'labels': padded_labels,\n",
        "        'doc': doc\n",
        "    }\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "# Now, modify the DataLoader to include a lambda function that passes the device to the custom collate function.\n",
        "train_dataloader = DataLoader(\n",
        "    custom_train,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=lambda x: custom_collate(x, device),\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    custom_val,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=lambda x: custom_collate(x, device),\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "63D-Oa8aBT_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val(model, custom_val, batch_size, custom_collate, criterion, device):\n",
        "    model.eval()\n",
        "    avg_val_loss = 0\n",
        "    avg_val_score = 0\n",
        "    val_loss = 0\n",
        "    val_score = 0\n",
        "    val_dataloader = DataLoader(custom_val, batch_size=batch_size, collate_fn=custom_collate, shuffle=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, data in enumerate(val_dataloader):\n",
        "            input_ids = data['input_ids'].to(device)\n",
        "            attention_mask = data['attention_mask'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits if isinstance(outputs, tuple) else outputs  # Adjust this line based on model output\n",
        "            loss = criterion(logits.view(-1, model.num_labels), labels.view(-1))\n",
        "            results = compute_metrics(logits, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_score += results['f5']\n",
        "\n",
        "            if batch % 200 == 0 or batch + 1 == len(val_dataloader):\n",
        "                print(\"********** For Validation Set **********\")\n",
        "                print(f\"Completed {batch+1}/{len(val_dataloader)}, with current val_loss: {loss.item()},\\n current results: {results}\")\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_dataloader)\n",
        "        avg_val_score = val_score / len(val_dataloader)\n",
        "\n",
        "    print(f\"Average val_loss: {avg_val_loss}, average val_score = {avg_val_score}\")\n",
        "\n",
        "    return avg_val_loss, avg_val_score"
      ],
      "metadata": {
        "id": "qfmsJojdiJyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train function is edited"
      ],
      "metadata": {
        "id": "wYXIWljlNzJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, val_dataloader, optimizer, criterion, scheduler, epochs, device):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch, data in enumerate(train_dataloader):\n",
        "            input_ids = data['input_ids'].to(device)\n",
        "            attention_mask = data['attention_mask'].to(device)\n",
        "            labels = data['labels'].to(device)  # This should be of shape [batch_size, sequence_length]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits  # Ensure logits are [batch_size, sequence_length, num_classes]\n",
        "\n",
        "            # Print shapes for debugging\n",
        "            print(f\"Logits shape: {logits.shape}, Labels shape: {labels.shape}\")\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))  # Flatten both logits and labels\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Optionally add scheduler.step() here if using learning rate scheduler\n",
        "\n",
        "        # Step the scheduler if it's part of the training configuration\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch, data in enumerate(val_dataloader):\n",
        "                input_ids = data['input_ids'].to(device)\n",
        "                attention_mask = data['attention_mask'].to(device)\n",
        "                labels = data['labels'].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs.logits\n",
        "\n",
        "                # Calculate loss and metrics\n",
        "                val_loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "                # calculate other metrics...\n",
        "\n",
        "        # Optionally add learning rate scheduler step here, if needed\n",
        "\n",
        "    return model  # Return the trained model\n"
      ],
      "metadata": {
        "id": "v32Po4OJnU4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_dataloader, val_dataloader, optimizer, criterion, scheduler, epochs, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "YlYKG2Y4nXf8",
        "outputId": "04f82c6c-7579-45b9-aa0c-ca0eab4aa281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Tensor' object has no attribute 'logits'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-141-aa789f5b1d70>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-140-77fb940c9ea0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, criterion, scheduler, epochs, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m  \u001b[0;31m# Ensure logits are [batch_size, sequence_length, num_classes]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# Print shapes for debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'logits'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "export CUDA_LAUNCH_BLOCKING=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "98P2FvTmp6Sa",
        "outputId": "2c0378e6-4f79-4a82-f883-ea362b87a55e"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-144-56efdc3ca8f4>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-144-56efdc3ca8f4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    export CUDA_LAUNCH_BLOCKING=1\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, val_dataloader, optimizer, criterion, scheduler, epochs, device):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        train_loss = 0\n",
        "        train_scores = []\n",
        "\n",
        "        for batch, data in enumerate(train_dataloader):\n",
        "            input_ids = data['input_ids'].to(device)\n",
        "            attention_mask = data['attention_mask'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "\n",
        "            # Convert one-hot to class indices if necessary\n",
        "            if labels.dim() == 3 and labels.size(-1) > 1:  # Assuming the last dimension is one-hot\n",
        "                labels = torch.argmax(labels, dim=-1)  # Convert to class indices\n",
        "\n",
        "            # Clear previously calculated gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits if isinstance(outputs, tuple) else outputs  # Adjust this line based on model output\n",
        "\n",
        "            # Print shapes for debugging\n",
        "            print(f\"Logits shape: {logits.shape}, Labels shape: {labels.shape}\")\n",
        "\n",
        "            # Calculate the batch loss\n",
        "            loss = criterion(logits.view(-1, model.num_labels), labels.view(-1))\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute metrics and store them\n",
        "            results = compute_metrics(logits, labels)\n",
        "            train_scores.append(results['f5'])\n",
        "\n",
        "            if batch % 200 == 0 or batch + 1 == len(train_dataloader):\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch+1}/{len(train_dataloader)}, Loss: {loss.item()}\")\n",
        "\n",
        "        # Step the scheduler if it's part of the training configuration\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Validation step after completing all batches in an epoch\n",
        "        avg_val_loss, avg_val_score = val(model, val_dataloader, batch_size, custom_collate, criterion, device)\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        avg_train_score = sum(train_scores) / len(train_scores)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} completed: Avg Train Loss: {avg_train_loss}, Avg Train F5 Score: {avg_train_score}\")\n",
        "        print(f\"Validation: Avg Val Loss: {avg_val_loss}, Avg Val F5 Score: {avg_val_score}\")\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "# Call the function to start training\n",
        "train_model(model, train_dataloader, val_dataloader, optimizer, criterion, scheduler, epochs, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "NiJ6HwZIi6eo",
        "outputId": "46fe4738-649f-4619-c5a9-84089cfbd150"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-143-9e8bc9d04ecb>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Call the function to start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-143-9e8bc9d04ecb>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, criterion, scheduler, epochs, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Forward pass: compute predicted outputs by passing inputs to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moutputs\u001b[0m  \u001b[0;31m# Adjust this line based on model output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-128-d8628ee9913b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1600\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                 \u001b[0;31m# the manual implementation that requires a 4D causal mask in all cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;31m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                 \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_4d_attention_mask_for_sdpa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                 \u001b[0;31m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py\u001b[0m in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mis_tracing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train(model, custom_train, custom_val, batch_size, custom_collate, epochs, optimizer, criterion, device):\n",
        "    model.train()\n",
        "\n",
        "    # Modify DataLoaders to use lambda for passing device to collate function\n",
        "    train_dataloader = DataLoader(custom_train, batch_size=batch_size,\n",
        "                                  collate_fn=lambda x: custom_collate(x, device), shuffle=True)\n",
        "    val_dataloader = DataLoader(custom_val, batch_size=batch_size,\n",
        "                                collate_fn=lambda x: custom_collate(x, device), shuffle=False)\n",
        "\n",
        "    print('Starting training...')\n",
        "    for epoch in range(epochs):\n",
        "        avg_train_loss = 0\n",
        "        avg_train_score = 0\n",
        "        train_loss = 0\n",
        "        train_score = 0\n",
        "\n",
        "        for batch, data in enumerate(train_dataloader):\n",
        "            input_ids = data['input_ids']\n",
        "            attention_mask = data['attention_mask']\n",
        "            labels = data['labels']\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            #outputs = model(input_ids, attention_mask=attention_mask).logits\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            #loss = criterion(outputs, labels.view(-1))\n",
        "            #loss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
        "\n",
        "            print(f\"Outputs shape: {outputs.shape}\")  # Should be [batch_size, sequence_length, num_classes]\n",
        "            print(f\"Labels shape: {labels.shape}\")    # Should be [batch_size, sequence_length]\n",
        "\n",
        "\n",
        "            outputs = outputs.view(-1, outputs.size(-1))  # Flatten output for cross-entropy which expects 2D logits\n",
        "            labels = labels.view(-1)  # Flatten labels to match output\n",
        "\n",
        "\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            results = compute_metrics(outputs, labels)\n",
        "            train_loss += loss.item()\n",
        "            train_score += results['f5']\n",
        "\n",
        "            if batch % 200 == 0 or batch + 1 == len(train_dataloader):\n",
        "                print(f\"Completed {batch+1}/{len(train_dataloader)}, with current train_loss: {loss},\\n current results:{results}\")\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        avg_train_score = train_score / len(train_dataloader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}: Average train_loss = {avg_train_loss}, average train_score = {avg_train_score}\")\n",
        "        print(\"Starting to validate\")\n",
        "        val_loss, val_score = val(model, custom_val, batch_size, custom_collate, criterion, device)\n",
        "\n",
        "    return avg_train_loss, avg_train_score\n"
      ],
      "metadata": {
        "id": "BY2wjAV3NTNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()  # Clear unused memory"
      ],
      "metadata": {
        "id": "ylPO17xb79Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "fXn2cBmn5Cx5",
        "outputId": "27c674ab-3818-420f-b6ee-223566b2affd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "oh_encoder() missing 1 required positional argument: 'unique_labels'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-a59e659e8637>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_collate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-95-20b38878f363>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, custom_train, custom_val, batch_size, custom_collate, epochs, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-92-2c35c113cefc>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcustom_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustom_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: oh_encoder() missing 1 required positional argument: 'unique_labels'"
          ]
        }
      ],
      "source": [
        "batch_size = 1\n",
        "epochs = 2\n",
        "\n",
        "train(model, custom_train, custom_val, batch_size, custom_collate, epochs, optimizer, criterion, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3M9Htes5Cx5"
      },
      "source": [
        "# Converting from predictions to NER labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d_mEq3v5Cx5"
      },
      "outputs": [],
      "source": [
        "#Used to label each token at NER stage\n",
        "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
        "\n",
        "    idx = 0\n",
        "    spans = []\n",
        "    span = []\n",
        "\n",
        "    for i, token in enumerate(document):\n",
        "        if token != target[idx]:\n",
        "            idx = 0\n",
        "            span = []\n",
        "            continue\n",
        "        span.append(i)\n",
        "        idx += 1\n",
        "        if idx == len(target):\n",
        "            spans.append(span)\n",
        "            span = []\n",
        "            idx = 0\n",
        "            continue\n",
        "\n",
        "    return spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdFCz1GD5Cx5"
      },
      "outputs": [],
      "source": [
        "## DON'T RUN ##\n",
        "#### From KAGGLE: https://www.kaggle.com/code/manavtrivedi/0-967-nlp-sakura/notebook ####\n",
        "\n",
        "triplets = []\n",
        "pairs = set()  # membership operation using set is faster O(1) than that of list O(n)\n",
        "processed = []\n",
        "emails = []\n",
        "phone_nums = []\n",
        "urls = []\n",
        "streets = []\n",
        "\n",
        "# For each prediction, token mapping, offsets, tokens, and document in the dataset\n",
        "for p, token_map, offsets, tokens, doc, full_text in zip(\n",
        "    preds_final,\n",
        "    ds[\"token_map\"],\n",
        "    ds[\"offset_mapping\"],\n",
        "    ds[\"tokens\"],\n",
        "    ds[\"document\"],\n",
        "    ds[\"full_text\"]\n",
        "):\n",
        "\n",
        "    # Iterate through each token prediction and its corresponding offsets\n",
        "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
        "        label_pred = id2label[str(token_pred)]  # Predicted label from token\n",
        "        if start_idx + end_idx == 0:\n",
        "            continue\n",
        "        if token_map[start_idx] == -1:\n",
        "            start_idx += 1\n",
        "        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
        "            start_idx += 1\n",
        "        if start_idx >= len(token_map):\n",
        "            break\n",
        "        token_id = token_map[start_idx]  # Token ID at start index\n",
        "        if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n",
        "            continue\n",
        "        pair = (doc, token_id)\n",
        "        if pair not in pairs:\n",
        "            processed.append({\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]})\n",
        "            pairs.add(pair)\n",
        "\n",
        "    # email\n",
        "    for token_idx, token in enumerate(tokens):\n",
        "        if re.fullmatch(email_regex, token) is not None:\n",
        "            emails.append(\n",
        "                {\"document\": doc, \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
        "            )\n",
        "\n",
        "    # phone number\n",
        "    matches = phone_num_regex.findall(full_text)\n",
        "    if not matches:\n",
        "        continue\n",
        "    for match in matches:\n",
        "        target = [t.text for t in nlp.tokenizer(match)]\n",
        "        matched_spans = find_span(target, tokens)\n",
        "    for matched_span in matched_spans:\n",
        "        for intermediate, token_idx in enumerate(matched_span):\n",
        "            prefix = \"I\" if intermediate else \"B\"\n",
        "            phone_nums.append(\n",
        "                {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": tokens[token_idx]}\n",
        "            )\n",
        "\n",
        "    # url\n",
        "    matches = url_regex.findall(full_text)\n",
        "    if not matches:\n",
        "        continue\n",
        "    for match in matches:\n",
        "        target = [t.text for t in nlp.tokenizer(match)]\n",
        "        matched_spans = find_span(target, tokens)\n",
        "    for matched_span in matched_spans:\n",
        "        for intermediate, token_idx in enumerate(matched_span):\n",
        "            prefix = \"I\" if intermediate else \"B\"\n",
        "            urls.append(\n",
        "                {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-URL_PERSONAL\", \"token_str\": tokens[token_idx]}\n",
        "            )\n",
        "\n",
        "    # street\n",
        "#     matches = street_regex.findall(full_text)\n",
        "#     if not matches:\n",
        "#         continue\n",
        "#     for match in matches:\n",
        "#         target = [t.text for t in nlp.tokenizer(match)]\n",
        "#         matched_spans = find_span(target, tokens)\n",
        "#     for matched_span in matched_spans:\n",
        "#         for intermediate, token_idx in enumerate(matched_span):\n",
        "#             prefix = \"I\" if intermediate else \"B\"\n",
        "#             streets.append(\n",
        "#                 {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-STREET_ADDRESS\", \"token_str\": tokens[token_idx]}\n",
        "#             )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "047af7238e4d49229153ca3d5930adbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d65d8026b69f4c528e61fa223d0f1341",
              "IPY_MODEL_261b4af453d34b4b92c8b414ab520900",
              "IPY_MODEL_0f2e15b8074042f682af1e56aeb12bdc"
            ],
            "layout": "IPY_MODEL_1d70dd3091ad40fbb743a2120cc67650"
          }
        },
        "d65d8026b69f4c528e61fa223d0f1341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17a2f4f13d334774a3bf55fa7c0994d1",
            "placeholder": "​",
            "style": "IPY_MODEL_4f2b38729666480cb286d13ea9cf7ea5",
            "value": "Map (num_proc=3): 100%"
          }
        },
        "261b4af453d34b4b92c8b414ab520900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebef494a22074eb09bd9e94619a7dc80",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3fc3dc95ff0540d5ad07e636b4ff86dd",
            "value": 10
          }
        },
        "0f2e15b8074042f682af1e56aeb12bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30a14f42120d4526ae2b275d369080b1",
            "placeholder": "​",
            "style": "IPY_MODEL_e64bbb2ac48a4a40a4b5786fdbd74b5b",
            "value": " 10/10 [00:00&lt;00:00, 25.49 examples/s]"
          }
        },
        "1d70dd3091ad40fbb743a2120cc67650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17a2f4f13d334774a3bf55fa7c0994d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f2b38729666480cb286d13ea9cf7ea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebef494a22074eb09bd9e94619a7dc80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fc3dc95ff0540d5ad07e636b4ff86dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "30a14f42120d4526ae2b275d369080b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e64bbb2ac48a4a40a4b5786fdbd74b5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6e3825d776f468e97b0912e4096efa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_655c1999b018422ca9706068fc41dd7e",
              "IPY_MODEL_23c7835f47fa42fc9ad2892a64dda8d5",
              "IPY_MODEL_82280a5f18244842bc4ff723ec088504"
            ],
            "layout": "IPY_MODEL_991e2873f40c4b81959db78c73735118"
          }
        },
        "655c1999b018422ca9706068fc41dd7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a61df096756d42eb8df9226cbfd8be28",
            "placeholder": "​",
            "style": "IPY_MODEL_13397cd2819f4dcabd96fd6baad47e50",
            "value": "Map (num_proc=3): 100%"
          }
        },
        "23c7835f47fa42fc9ad2892a64dda8d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e2c5b829c314b409b620f210a9ba442",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a7df00216ce46bba5fd5c3536487e80",
            "value": 10
          }
        },
        "82280a5f18244842bc4ff723ec088504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b27d0fbeccdd4a40a12dd5fdc5122400",
            "placeholder": "​",
            "style": "IPY_MODEL_d89b70066b6c437c890e74a53678d2c2",
            "value": " 10/10 [00:00&lt;00:00, 17.58 examples/s]"
          }
        },
        "991e2873f40c4b81959db78c73735118": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a61df096756d42eb8df9226cbfd8be28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13397cd2819f4dcabd96fd6baad47e50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e2c5b829c314b409b620f210a9ba442": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a7df00216ce46bba5fd5c3536487e80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b27d0fbeccdd4a40a12dd5fdc5122400": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d89b70066b6c437c890e74a53678d2c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}