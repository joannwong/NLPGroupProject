{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some references**\n",
    "\n",
    "https://www.kaggle.com/code/minhsienweng/train-infer-pii-detection-deberta-v3\n",
    "\n",
    "(no training) https://www.kaggle.com/code/manavtrivedi/0-967-nlp-sakura/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\User\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datasets\n",
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.lang.en import English\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ignite.metrics import Fbeta\n",
    "from functools import partial\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.nn.functional import softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 labels, with the following labels:\n",
      " ['B-ID_NUM', 'I-PHONE_NUM', 'B-USERNAME', 'I-ID_NUM', 'B-STREET_ADDRESS', 'O', 'I-STREET_ADDRESS', 'B-NAME_STUDENT', 'B-EMAIL', 'B-URL_PERSONAL', 'I-NAME_STUDENT', 'I-URL_PERSONAL', 'B-PHONE_NUM']\n"
     ]
    }
   ],
   "source": [
    "#Finding out the number of labels\n",
    "data = json.load(open('data/train.json'))\n",
    "\n",
    "\n",
    "all_labels = set()\n",
    "\n",
    "for d in data:\n",
    "    all_labels = all_labels.union(set(d['labels']))\n",
    "\n",
    "print(f\"{len(list(all_labels))} labels, with the following labels:\\n {list(all_labels)}\")\n",
    "del data\n",
    "\n",
    "label2id = {label:index for index,label in enumerate(all_labels)}\n",
    "id2label = {index:label for index,label in enumerate(all_labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-ID_NUM', 'I-PHONE_NUM', 'B-USERNAME', 'I-ID_NUM', 'B-STREET_ADDRESS', 'O', 'I-STREET_ADDRESS', 'B-NAME_STUDENT', 'B-EMAIL', 'B-URL_PERSONAL', 'I-NAME_STUDENT', 'I-URL_PERSONAL', 'B-PHONE_NUM'}\n"
     ]
    }
   ],
   "source": [
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to one-hot vector\n",
    "def oh_encoder(labels):  #label: array of output for each sentence\n",
    "\n",
    "    # unique_labels = ['O', 'B-NAME_STUDENT','I-NAME_STUDENT','B-PHONE_NUM', 'I-PHONE_NUM','B-ID_NUM', 'I-ID_NUM',  'B-URL_PERSONAL','I-URL_PERSONAL',\n",
    "    #                   'B-STREET_ADDRESS', 'I-STREET_ADDRESS',  'B-EMAIL', 'B-USERNAME']\n",
    "    \n",
    "    \n",
    "    unique_labels = ['O', 'B-NAME_STUDENT','I-NAME_STUDENT','B-URL_PERSONAL', 'B-ID_NUM','I-ID_NUM','B-EMAIL','I-STREET_ADDRESS',\n",
    "                     'I-PHONE_NUM', 'B-USERNAME', 'B-PHONE_NUM','B-STREET_ADDRESS', 'I-URL_PERSONAL']\n",
    "    \n",
    "    labels_oh = []\n",
    "    for label in labels:    #label: str\n",
    "        label_oh = [float(0)]*len(unique_labels)\n",
    "        for k in range(len(unique_labels)):\n",
    "            if unique_labels[k] == label:\n",
    "                label_oh[k] = 1\n",
    "                #labels_oh.append(torch.tensor(label_oh, requires_grad=True))\n",
    "                labels_oh.append(label_oh)\n",
    "                break\n",
    "                \n",
    "\n",
    "    #return torch.tensor(labels_oh, requires_grad=True)\n",
    "    return torch.tensor(labels_oh, requires_grad=True, dtype=float)    #list of one-hot labels as tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Tokenizing sentences.\n",
    "# def tokenize(example, tokenizer, INFERENCE_MAX_LENGTH=3500):\n",
    "#     ''' \n",
    "#     Arguments:\n",
    "#     example: sentence\n",
    "#     tokenizer: following DeBERTa's\n",
    "#     INFERENCE_MAX_LENGTH: for truncation if needed\n",
    "\n",
    "#     Returns:\n",
    "#     dictionary of tokenized word id, with token_map, which maps characters to its initial idx\n",
    "#     '''\n",
    "#     text = []\n",
    "#     token_map = []\n",
    "#     labels = []\n",
    "    \n",
    "#     idx = 0\n",
    "    \n",
    "#     for t, label, ws in zip(example[\"tokens\"], \n",
    "#                      example['labels'],\n",
    "#                      example[\"trailing_whitespace\"]):\n",
    "        \n",
    "#         text.append(t)\n",
    "#         token_map.extend([idx]*len(t))\n",
    "#         labels.extend([label]*len(t))\n",
    "        \n",
    "#         if ws:\n",
    "#             text.append(\" \")\n",
    "#             token_map.append(-1)\n",
    "\n",
    "#         idx += 1\n",
    "    \n",
    "#     text = \"\".join(text)\n",
    "#     tokenized = tokenizer(text, return_offsets_mapping=True, truncation=False, max_length=INFERENCE_MAX_LENGTH, return_tensors='pt')\n",
    "\n",
    "#     labels = np.array(labels)\n",
    "\n",
    "\n",
    "    \n",
    "#     return {\n",
    "#         **tokenized,\n",
    "#         \"token_map\": token_map,\n",
    "\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer):\n",
    "    import numpy as np\n",
    "    # Preprocess the tokens and labels by adding trailing whitespace and labels\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for token, label, t_ws in zip(example[\"tokens\"], \n",
    "                                  example[\"labels\"],\n",
    "                                  example[\"trailing_whitespace\"]):\n",
    "        tokens.append(token)\n",
    "        labels.extend([label] * len(token))\n",
    "        # Added trailing whitespace and label if true and \n",
    "        if t_ws:\n",
    "            tokens.append(\" \")\n",
    "            # labels.append(oh_encoder(\"O\"))\n",
    "            labels.append(\"O\")\n",
    "    \n",
    "    text = \"\".join(tokens)\n",
    "    # print(f\"len(text)={len(text)}, len(tokens)={len(tokens)}\")\n",
    "    # tokenization without truncation\n",
    "    tokenized = tokenizer(text, return_offsets_mapping=True,\n",
    "                          truncation=False)\n",
    "    #labels = np.array(labels)\n",
    "    # Labels\n",
    "    token_labels = []\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # Added 'O' \n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            #token_labels.append(label2id[\"O\"]) \n",
    "            #token_labels.append(oh_encoder(\"O\"))\n",
    "            token_labels.append(\"O\")\n",
    "        else:\n",
    "            # case when the text starts with whitespace\n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "            # Convert label to id (int)\n",
    "            #label_id = label2id[labels[start_idx]]\n",
    "            label_id = labels[start_idx]\n",
    "            #token_labels.append(oh_encoder(label_id))\n",
    "            token_labels.append(label_id)\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": len(tokenized.input_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Their one hot encoding'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Their one hot encoding'''\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# # Eencode labels to columns\n",
    "# def encode_labels(data):\n",
    "#     df = pd.DataFrame(data)\n",
    "#     total = len(df)\n",
    "#     df[\"unique_labels\"] = df[\"labels\"].apply(lambda labels: \n",
    "#                                             list(set([label.split('-')[1] for label in labels if label != 'O'])))\n",
    "#     mlb = MultiLabelBinarizer()\n",
    "#     #one_hot_encoded = mlb.fit_transform(df['unique_labels'])\n",
    "#     one_hot_encoded = mlb.fit_transform(df['labels'])\n",
    "#     one_hot_df = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
    "#     df = pd.concat([df, one_hot_df], axis=1)\n",
    "#     # add 'POS' column that don't have \n",
    "#     df['others'] = df['unique_labels'].apply(lambda x: 1 if len(x) == 0 else 0)\n",
    "#     label_classes = list(mlb.classes_) + ['others']\n",
    "#     for col in label_classes:\n",
    "#         subtotal = df[col].sum()\n",
    "#         percent = subtotal/total * 100\n",
    "#         print(f'{col}: {subtotal}  ({percent:.1f}%)')\n",
    "#     return df, label_classes\n",
    "\n",
    "# data = json.load(open('data/train.json'))\n",
    "# df , label_classes = encode_labels(data)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "726 726\n",
      "[CLS] --> O\n",
      "▁Design --> O\n",
      "▁Thinking --> O\n",
      "▁for --> O\n",
      "▁innovation --> O\n",
      "▁reflex --> O\n",
      "ion --> O\n",
      "- --> O\n",
      "Av --> O\n",
      "ril --> O\n",
      "▁2021 --> O\n",
      "- --> O\n",
      "N --> B-NAME_STUDENT\n",
      "atha --> B-NAME_STUDENT\n",
      "lie --> B-NAME_STUDENT\n",
      "▁S --> I-NAME_STUDENT\n",
      "ylla --> I-NAME_STUDENT\n",
      "▁Challenge --> O\n",
      "▁& --> O\n",
      "▁selection --> O\n",
      "▁The --> O\n",
      "▁tool --> O\n",
      "▁I --> O\n",
      "▁use --> O\n",
      "▁to --> O\n",
      "▁help --> O\n",
      "▁all --> O\n",
      "▁stakeholders --> O\n",
      "▁finding --> O\n",
      "▁their --> O\n",
      "▁way --> O\n",
      "▁through --> O\n",
      "▁the --> O\n",
      "▁complexity --> O\n",
      "▁of --> O\n",
      "▁a --> O\n",
      "▁project --> O\n",
      "▁is --> O\n",
      "▁the --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      ". --> O\n",
      "▁What --> O\n",
      "▁exactly --> O\n",
      "▁is --> O\n",
      "▁a --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      "? --> O\n",
      "▁According --> O\n",
      "▁to --> O\n",
      "▁the --> O\n",
      "▁definition --> O\n",
      "▁of --> O\n",
      "▁Buz --> O\n",
      "an --> O\n",
      "▁T --> O\n",
      ". --> O\n",
      "▁and --> O\n",
      "▁Buz --> O\n",
      "an --> O\n",
      "▁B --> O\n",
      ". --> O\n",
      "▁( --> O\n",
      "1999 --> O\n",
      ", --> O\n",
      "▁Des --> O\n",
      "s --> O\n",
      "ine --> O\n",
      "- --> O\n",
      "moi --> O\n",
      "▁l --> O\n",
      "' --> O\n",
      "intelligence --> O\n",
      ". --> O\n",
      "▁Paris --> O\n",
      ": --> O\n",
      "▁Les --> O\n",
      "▁É --> O\n",
      "dition --> O\n",
      "s --> O\n",
      "▁d --> O\n",
      "' --> O\n",
      "Organ --> O\n",
      "isation --> O\n",
      ". --> O\n",
      ") --> O\n",
      ", --> O\n",
      "▁the --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      "▁( --> O\n",
      "or --> O\n",
      "▁heuristic --> O\n",
      "▁diagram --> O\n",
      ") --> O\n",
      "▁is --> O\n",
      "▁a --> O\n",
      "▁graphic --> O\n",
      "▁representation --> O\n",
      "▁technique --> O\n",
      "▁that --> O\n",
      "▁follows --> O\n",
      "▁the --> O\n",
      "▁natural --> O\n",
      "▁functioning --> O\n",
      "▁of --> O\n",
      "▁the --> O\n",
      "▁mind --> O\n",
      "▁and --> O\n",
      "▁allows --> O\n",
      "▁the --> O\n",
      "▁brain --> O\n",
      "' --> O\n",
      "s --> O\n",
      "▁potential --> O\n",
      "▁to --> O\n",
      "▁be --> O\n",
      "▁released --> O\n",
      ". --> O\n",
      "▁Cf --> O\n",
      "▁Annex --> O\n",
      "1 --> O\n",
      "▁This --> O\n",
      "▁tool --> O\n",
      "▁has --> O\n",
      "▁many --> O\n",
      "▁advantages --> O\n",
      ": --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁is --> O\n",
      "▁accessible --> O\n",
      "▁to --> O\n",
      "▁all --> O\n",
      "▁and --> O\n",
      "▁does --> O\n",
      "▁not --> O\n",
      "▁require --> O\n",
      "▁significant --> O\n",
      "▁material --> O\n",
      "▁investment --> O\n",
      "▁and --> O\n",
      "▁can --> O\n",
      "▁be --> O\n",
      "▁done --> O\n",
      "▁quickly --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁is --> O\n",
      "▁scalable --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁allows --> O\n",
      "▁categorization --> O\n",
      "▁and --> O\n",
      "▁linking --> O\n",
      "▁of --> O\n",
      "▁information --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁can --> O\n",
      "▁be --> O\n",
      "▁applied --> O\n",
      "▁to --> O\n",
      "▁any --> O\n",
      "▁type --> O\n",
      "▁of --> O\n",
      "▁situation --> O\n",
      ": --> O\n",
      "▁note --> O\n",
      "taking --> O\n",
      ", --> O\n",
      "▁problem --> O\n",
      "▁solving --> O\n",
      ", --> O\n",
      "▁analysis --> O\n",
      ", --> O\n",
      "▁creation --> O\n",
      "▁of --> O\n",
      "▁new --> O\n",
      "▁ideas --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁is --> O\n",
      "▁suitable --> O\n",
      "▁for --> O\n",
      "▁all --> O\n",
      "▁people --> O\n",
      "▁and --> O\n",
      "▁is --> O\n",
      "▁easy --> O\n",
      "▁to --> O\n",
      "▁learn --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁is --> O\n",
      "▁fun --> O\n",
      "▁and --> O\n",
      "▁encourages --> O\n",
      "▁exchanges --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁makes --> O\n",
      "▁visible --> O\n",
      "▁the --> O\n",
      "▁dimension --> O\n",
      "▁of --> O\n",
      "▁projects --> O\n",
      ", --> O\n",
      "▁opportunities --> O\n",
      ", --> O\n",
      "▁interconnections --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁synthesize --> O\n",
      "s --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁makes --> O\n",
      "▁the --> O\n",
      "▁project --> O\n",
      "▁understandable --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁allows --> O\n",
      "▁you --> O\n",
      "▁to --> O\n",
      "▁explore --> O\n",
      "▁ideas --> O\n",
      "▁The --> O\n",
      "▁creation --> O\n",
      "▁of --> O\n",
      "▁a --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      "▁starts --> O\n",
      "▁with --> O\n",
      "▁an --> O\n",
      "▁idea --> O\n",
      "/ --> O\n",
      "problem --> O\n",
      "▁located --> O\n",
      "▁at --> O\n",
      "▁its --> O\n",
      "▁center --> O\n",
      ". --> O\n",
      "▁This --> O\n",
      "▁starting --> O\n",
      "▁point --> O\n",
      "▁generates --> O\n",
      "▁ideas --> O\n",
      "/ --> O\n",
      "work --> O\n",
      "▁areas --> O\n",
      ", --> O\n",
      "▁incremented --> O\n",
      "▁around --> O\n",
      "▁this --> O\n",
      "▁center --> O\n",
      "▁in --> O\n",
      "▁a --> O\n",
      "▁radial --> O\n",
      "▁structure --> O\n",
      ", --> O\n",
      "▁which --> O\n",
      "▁in --> O\n",
      "▁turn --> O\n",
      "▁is --> O\n",
      "▁completed --> O\n",
      "▁with --> O\n",
      "▁as --> O\n",
      "▁many --> O\n",
      "▁branches --> O\n",
      "▁as --> O\n",
      "▁new --> O\n",
      "▁ideas --> O\n",
      ". --> O\n",
      "▁This --> O\n",
      "▁tool --> O\n",
      "▁enables --> O\n",
      "▁creativity --> O\n",
      "▁and --> O\n",
      "▁logic --> O\n",
      "▁to --> O\n",
      "▁be --> O\n",
      "▁mobilized --> O\n",
      ", --> O\n",
      "▁it --> O\n",
      "▁is --> O\n",
      "▁a --> O\n",
      "▁map --> O\n",
      "▁of --> O\n",
      "▁the --> O\n",
      "▁thoughts --> O\n",
      ". --> O\n",
      "▁Creativity --> O\n",
      "▁is --> O\n",
      "▁enhanced --> O\n",
      "▁because --> O\n",
      "▁participants --> O\n",
      "▁feel --> O\n",
      "▁comfortable --> O\n",
      "▁with --> O\n",
      "▁the --> O\n",
      "▁method --> O\n",
      ". --> O\n",
      "▁Application --> O\n",
      "▁& --> O\n",
      "▁Insight --> O\n",
      "▁I --> O\n",
      "▁start --> O\n",
      "▁the --> O\n",
      "▁process --> O\n",
      "▁of --> O\n",
      "▁the --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      "▁creation --> O\n",
      "▁with --> O\n",
      "▁the --> O\n",
      "▁stakeholders --> O\n",
      "▁standing --> O\n",
      "▁around --> O\n",
      "▁a --> O\n",
      "▁large --> O\n",
      "▁board --> O\n",
      "▁( --> O\n",
      "white --> O\n",
      "▁or --> O\n",
      "▁paper --> O\n",
      "▁board --> O\n",
      ") --> O\n",
      ". --> O\n",
      "▁In --> O\n",
      "▁the --> O\n",
      "▁center --> O\n",
      "▁of --> O\n",
      "▁the --> O\n",
      "▁board --> O\n",
      ", --> O\n",
      "▁I --> O\n",
      "▁write --> O\n",
      "▁and --> O\n",
      "▁highlight --> O\n",
      "▁the --> O\n",
      "▁topic --> O\n",
      "▁to --> O\n",
      "▁design --> O\n",
      ". --> O\n",
      "▁Through --> O\n",
      "▁a --> O\n",
      "▁series --> O\n",
      "▁of --> O\n",
      "▁questions --> O\n",
      ", --> O\n",
      "▁I --> O\n",
      "▁guide --> O\n",
      "▁the --> O\n",
      "▁stakeholders --> O\n",
      "▁in --> O\n",
      "▁modelling --> O\n",
      "▁the --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      ". --> O\n",
      "▁I --> O\n",
      "▁adapt --> O\n",
      "▁the --> O\n",
      "▁series --> O\n",
      "▁of --> O\n",
      "▁questions --> O\n",
      "▁according --> O\n",
      "▁to --> O\n",
      "▁the --> O\n",
      "▁topic --> O\n",
      "▁to --> O\n",
      "▁be --> O\n",
      "▁addressed --> O\n",
      ". --> O\n",
      "▁In --> O\n",
      "▁the --> O\n",
      "▁type --> O\n",
      "▁of --> O\n",
      "▁questions --> O\n",
      ", --> O\n",
      "▁we --> O\n",
      "▁can --> O\n",
      "▁use --> O\n",
      ": --> O\n",
      "▁who --> O\n",
      ", --> O\n",
      "▁what --> O\n",
      ", --> O\n",
      "▁when --> O\n",
      ", --> O\n",
      "▁where --> O\n",
      ", --> O\n",
      "▁why --> O\n",
      ", --> O\n",
      "▁how --> O\n",
      ", --> O\n",
      "▁how --> O\n",
      "▁much --> O\n",
      ". --> O\n",
      "▁The --> O\n",
      "▁use --> O\n",
      "▁of --> O\n",
      "▁the --> O\n",
      "▁“ --> O\n",
      "why --> O\n",
      "” --> O\n",
      "▁is --> O\n",
      "▁very --> O\n",
      "▁interesting --> O\n",
      "▁to --> O\n",
      "▁understand --> O\n",
      "▁the --> O\n",
      "▁origin --> O\n",
      ". --> O\n",
      "▁By --> O\n",
      "▁this --> O\n",
      "▁way --> O\n",
      ", --> O\n",
      "▁the --> O\n",
      "▁interviewed --> O\n",
      "▁person --> O\n",
      "▁free --> O\n",
      "s --> O\n",
      "▁itself --> O\n",
      "▁from --> O\n",
      "▁paradigms --> O\n",
      "▁and --> O\n",
      "▁thus --> O\n",
      "▁dares --> O\n",
      "▁to --> O\n",
      "▁propose --> O\n",
      "▁new --> O\n",
      "▁ideas --> O\n",
      "▁/ --> O\n",
      "▁ways --> O\n",
      "▁of --> O\n",
      "▁functioning --> O\n",
      ". --> O\n",
      "▁I --> O\n",
      "▁plan --> O\n",
      "▁two --> O\n",
      "▁hours --> O\n",
      "▁for --> O\n",
      "▁a --> O\n",
      "▁workshop --> O\n",
      ". --> O\n",
      "▁Design --> O\n",
      "▁Thinking --> O\n",
      "▁for --> O\n",
      "▁innovation --> O\n",
      "▁reflex --> O\n",
      "ion --> O\n",
      "- --> O\n",
      "Av --> O\n",
      "ril --> O\n",
      "▁2021 --> O\n",
      "- --> O\n",
      "N --> B-NAME_STUDENT\n",
      "atha --> B-NAME_STUDENT\n",
      "lie --> B-NAME_STUDENT\n",
      "▁S --> I-NAME_STUDENT\n",
      "ylla --> I-NAME_STUDENT\n",
      "▁After --> O\n",
      "▁modelling --> O\n",
      "▁the --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      "▁on --> O\n",
      "▁paper --> O\n",
      ", --> O\n",
      "▁I --> O\n",
      "▁propose --> O\n",
      "▁to --> O\n",
      "▁the --> O\n",
      "▁participants --> O\n",
      "▁a --> O\n",
      "▁digital --> O\n",
      "▁visualization --> O\n",
      "▁of --> O\n",
      "▁their --> O\n",
      "▁work --> O\n",
      "▁with --> O\n",
      "▁the --> O\n",
      "▁addition --> O\n",
      "▁of --> O\n",
      "▁color --> O\n",
      "▁codes --> O\n",
      ", --> O\n",
      "▁images --> O\n",
      "▁and --> O\n",
      "▁interconnections --> O\n",
      ". --> O\n",
      "▁This --> O\n",
      "▁second --> O\n",
      "▁workshop --> O\n",
      "▁also --> O\n",
      "▁last --> O\n",
      "s --> O\n",
      "▁two --> O\n",
      "▁hours --> O\n",
      "▁and --> O\n",
      "▁allows --> O\n",
      "▁the --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      "▁to --> O\n",
      "▁evolve --> O\n",
      ". --> O\n",
      "▁Once --> O\n",
      "▁familiarized --> O\n",
      "▁with --> O\n",
      "▁it --> O\n",
      ", --> O\n",
      "▁the --> O\n",
      "▁stakeholders --> O\n",
      "▁discover --> O\n",
      "▁the --> O\n",
      "▁power --> O\n",
      "▁of --> O\n",
      "▁the --> O\n",
      "▁tool --> O\n",
      ". --> O\n",
      "▁Then --> O\n",
      ", --> O\n",
      "▁the --> O\n",
      "▁second --> O\n",
      "▁workshop --> O\n",
      "▁brings --> O\n",
      "▁out --> O\n",
      "▁even --> O\n",
      "▁more --> O\n",
      "▁ideas --> O\n",
      "▁and --> O\n",
      "▁constructive --> O\n",
      "▁exchanges --> O\n",
      "▁between --> O\n",
      "▁the --> O\n",
      "▁stakeholders --> O\n",
      ". --> O\n",
      "▁Around --> O\n",
      "▁this --> O\n",
      "▁new --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      ", --> O\n",
      "▁they --> O\n",
      "▁have --> O\n",
      "▁learned --> O\n",
      "▁to --> O\n",
      "▁work --> O\n",
      "▁together --> O\n",
      "▁and --> O\n",
      "▁want --> O\n",
      "▁to --> O\n",
      "▁make --> O\n",
      "▁visible --> O\n",
      "▁the --> O\n",
      "▁untold --> O\n",
      "▁ideas --> O\n",
      ". --> O\n",
      "▁I --> O\n",
      "▁now --> O\n",
      "▁present --> O\n",
      "▁all --> O\n",
      "▁the --> O\n",
      "▁projects --> O\n",
      "▁I --> O\n",
      "▁manage --> O\n",
      "▁in --> O\n",
      "▁this --> O\n",
      "▁type --> O\n",
      "▁of --> O\n",
      "▁format --> O\n",
      "▁in --> O\n",
      "▁order --> O\n",
      "▁to --> O\n",
      "▁ease --> O\n",
      "▁rapid --> O\n",
      "▁understanding --> O\n",
      "▁for --> O\n",
      "▁decision --> O\n",
      "- --> O\n",
      "makers --> O\n",
      ". --> O\n",
      "▁These --> O\n",
      "▁presentations --> O\n",
      "▁are --> O\n",
      "▁the --> O\n",
      "▁core --> O\n",
      "▁of --> O\n",
      "▁my --> O\n",
      "▁business --> O\n",
      "▁models --> O\n",
      ". --> O\n",
      "▁The --> O\n",
      "▁decision --> O\n",
      "- --> O\n",
      "makers --> O\n",
      "▁are --> O\n",
      "▁thus --> O\n",
      "▁able --> O\n",
      "▁to --> O\n",
      "▁identify --> O\n",
      "▁the --> O\n",
      "▁opportunities --> O\n",
      "▁of --> O\n",
      "▁the --> O\n",
      "▁projects --> O\n",
      "▁and --> O\n",
      "▁can --> O\n",
      "▁take --> O\n",
      "▁quick --> O\n",
      "▁decisions --> O\n",
      "▁to --> O\n",
      "▁validate --> O\n",
      "▁them --> O\n",
      ". --> O\n",
      "▁They --> O\n",
      "▁find --> O\n",
      "▁answers --> O\n",
      "▁to --> O\n",
      "▁their --> O\n",
      "▁questions --> O\n",
      "▁thank --> O\n",
      "▁to --> O\n",
      "▁a --> O\n",
      "▁schematic --> O\n",
      "▁representation --> O\n",
      ". --> O\n",
      "▁Approach --> O\n",
      "▁What --> O\n",
      "▁I --> O\n",
      "▁find --> O\n",
      "▁amazing --> O\n",
      "▁with --> O\n",
      "▁the --> O\n",
      "▁facilitation --> O\n",
      "▁of --> O\n",
      "▁this --> O\n",
      "▁type --> O\n",
      "▁of --> O\n",
      "▁workshop --> O\n",
      "▁is --> O\n",
      "▁the --> O\n",
      "▁participants --> O\n",
      "▁commitment --> O\n",
      "▁for --> O\n",
      "▁the --> O\n",
      "▁project --> O\n",
      ". --> O\n",
      "▁This --> O\n",
      "▁tool --> O\n",
      "▁helps --> O\n",
      "▁to --> O\n",
      "▁give --> O\n",
      "▁meaning --> O\n",
      ". --> O\n",
      "▁The --> O\n",
      "▁participants --> O\n",
      "▁appropriate --> O\n",
      "▁the --> O\n",
      "▁story --> O\n",
      "▁and --> O\n",
      "▁want --> O\n",
      "▁to --> O\n",
      "▁keep --> O\n",
      "▁writing --> O\n",
      "▁it --> O\n",
      ". --> O\n",
      "▁Then --> O\n",
      ", --> O\n",
      "▁they --> O\n",
      "▁easily --> O\n",
      "▁become --> O\n",
      "▁actors --> O\n",
      "▁or --> O\n",
      "▁sponsors --> O\n",
      "▁of --> O\n",
      "▁the --> O\n",
      "▁project --> O\n",
      ". --> O\n",
      "▁A --> O\n",
      "▁trust --> O\n",
      "▁relationship --> O\n",
      "▁is --> O\n",
      "▁built --> O\n",
      ", --> O\n",
      "▁thus --> O\n",
      "▁facilitating --> O\n",
      "▁the --> O\n",
      "▁implementation --> O\n",
      "▁of --> O\n",
      "▁related --> O\n",
      "▁actions --> O\n",
      ". --> O\n",
      "▁Design --> O\n",
      "▁Thinking --> O\n",
      "▁for --> O\n",
      "▁innovation --> O\n",
      "▁reflex --> O\n",
      "ion --> O\n",
      "- --> O\n",
      "Av --> O\n",
      "ril --> O\n",
      "▁2021 --> O\n",
      "- --> O\n",
      "N --> B-NAME_STUDENT\n",
      "atha --> B-NAME_STUDENT\n",
      "lie --> B-NAME_STUDENT\n",
      "▁S --> I-NAME_STUDENT\n",
      "ylla --> I-NAME_STUDENT\n",
      "▁Annex --> O\n",
      "▁1 --> O\n",
      ": --> O\n",
      "▁Mind --> O\n",
      "▁Map --> O\n",
      "▁Shared --> O\n",
      "▁facilities --> O\n",
      "▁project --> O\n",
      "[SEP] --> O\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open('data/train.json'))\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "d = data[0]\n",
    "input_ids = tokenize(d, tokenizer)\n",
    "tensor = torch.tensor(input_ids['input_ids']) \n",
    "tokens = tokenizer.convert_ids_to_tokens(tensor)\n",
    "\n",
    "# print(tokens)\n",
    "# print(input_ids['labels'])\n",
    "# print(d['tokens'])\n",
    "# print(len(tokens),len(input_ids['labels']))\n",
    "\n",
    "# for id, label in zip(tokens, input_ids['labels']):\n",
    "#     print(id, '-->', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: DeBERTa\n",
    "\n",
    "Using a pretrained DeBERTa, we will build a classifier head on top of it to predict the class at token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Classifier(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim = 13):\n",
    "#         super(Classifier, self).__init__()\n",
    "\n",
    "#         self.dropout_prob = 0.3\n",
    "#         self.final_activation = nn.Softmax(dim = -1)\n",
    "\n",
    "#         self.linear = nn.Sequential(\n",
    "#             nn.ReLU(nn.Linear(input_dim, hidden_dim)),\n",
    "            \n",
    "#             nn.Dropout(self.dropout_prob),\n",
    "#             nn.ReLU(nn.Linear(hidden_dim, hidden_dim*2)),\n",
    "\n",
    "#             nn.Dropout(self.dropout_prob),\n",
    "#             nn.ReLU(nn.Linear(hidden_dim*2, hidden_dim)),\n",
    "\n",
    "#             nn.Linear(hidden_dim, output_dim)\n",
    "#         )\n",
    "       \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         logit = self.linear(x)\n",
    "#         return self.final_activation(logit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Classifier(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim = 13):\n",
    "#         super(Classifier, self).__init__()\n",
    "\n",
    "#         self.dropout_prob = 0.3\n",
    "#         self.final_activation = nn.Softmax(dim = -1)\n",
    "\n",
    "#         self.dropout = nn.Dropout(self.dropout_prob)\n",
    "#         self.linear1 = nn.ReLU(nn.Linear(input_dim,hidden_dim))\n",
    "#         self.linear2 = nn.ReLU(nn.Linear(hidden_dim, hidden_dim*2))                            \n",
    "#         self.linear3 = nn.ReLU(nn.Linear(hidden_dim*2, hidden_dim))\n",
    "#         self.output = nn.Linear(hidden_dim,output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         print('now start linear1')\n",
    "#         print(x.size())\n",
    "#         x = self.linear1(x)\n",
    "\n",
    "#         print('now start linear2')\n",
    "#         print(x.size())\n",
    "#         x = self.linear2(self.dropout(x))\n",
    "\n",
    "#         print('now start linear3')\n",
    "#         print(x.size())\n",
    "#         x = self.linear3(self.dropout(x))\n",
    "\n",
    "#         print('now start output')\n",
    "#         print(x.size())\n",
    "#         logit= self.output(x)\n",
    "        \n",
    "#         print('now start activation')\n",
    "#         output =  self.final_activation(logit)\n",
    "        \n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Deberta_Classif(Classifier):\n",
    "#     def __init__(self, model_name, classif_input, classif_hidden, classif_output = 13, finetune = False):\n",
    "#         super(Deberta_Classif, self).__init__(classif_input, classif_input)\n",
    "#         self.ft = finetune\n",
    "\n",
    "#         self.extractor = AutoModelForTokenClassification.from_pretrained(model_name).base_model\n",
    "\n",
    "#         if not finetune:\n",
    "#             for param in self.extractor.parameters():\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "#         self.extractor_num_param =  sum(p.numel() for p in self.extractor.parameters())\n",
    "#         self.extractor_num_param_grad = sum(p.numel() for p in self.extractor.parameters() if p.requires_grad)\n",
    "#         self.extractor_name = \"DeBERTa\"\n",
    "                \n",
    "\n",
    "#         self.classifier = Classifier(input_dim=classif_input, hidden_dim=classif_hidden ,output_dim=classif_output)\n",
    "#         self.classifier_num_param = sum(p.numel() for p in self.classifier.parameters() if p.requires_grad)\n",
    "\n",
    "        \n",
    "#     def count_param(self):\n",
    "        \n",
    "#         if self.ft:\n",
    "#             type = 'finetuned'\n",
    "#             num_param = self.extractor_num_param_grad + self.classifier_num_param\n",
    "#         else:\n",
    "#             type = 'non-finetuned'\n",
    "#             num_param = self.extractor_num_param + self.classifier_num_param\n",
    "\n",
    "#         print(f\"Number of parameters in {type} {self.extractor_name} model is {num_param:,}\")\n",
    "    \n",
    "    \n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         x = self.extractor(input_ids, attention_mask).last_hidden_state\n",
    "#         x = self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's trainable parameters:  9,997\n",
      "Model's total parameters:  141,314,317\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "model_name = \"microsoft/deberta-v3-small\"\n",
    "#model = Deberta_Classif(model_name, classif_input = 768, classif_hidden = 100, classif_output = 13, finetune=False)  #Extractor output has dim 768 \n",
    "config = AutoModelForTokenClassification.from_pretrained(model_name).config\n",
    "\n",
    "config.update({\n",
    "            'num_labels': 13,\n",
    "            'ignore_mismatched_sizes': True,\n",
    "        })\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, config = config, ignore_mismatched_sizes=True)\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Model's trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad): ,}\")\n",
    "print(f\"Model's total parameters: {sum(p.numel() for p in model.parameters()): ,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\User\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' \n",
    "model = model.to(device)\n",
    "epochs = 5\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of = 32, max_length=3500)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(outputs, labels, unique_labels = ['O', 'B-NAME_STUDENT','I-NAME_STUDENT','B-URL_PERSONAL', \n",
    "                                                          'B-ID_NUM','I-ID_NUM','B-EMAIL','I-STREET_ADDRESS',\n",
    "                                                          'I-PHONE_NUM', 'B-USERNAME', 'B-PHONE_NUM','B-STREET_ADDRESS', 'I-URL_PERSONAL']):    \n",
    "    try:\n",
    "        #print(\"Compute metrics\")\n",
    "        predictions = torch.argmax(softmax(outputs, dim=2), dim=2)\n",
    "        # Include prediction Remove ignored index (special tokens)\n",
    "        true_preds = []\n",
    "        true_labels = []\n",
    "        for pred, label in zip(predictions, labels):\n",
    "            true_preds.append([unique_labels[p] for p, l in zip(pred, label) if l[0] != -100])\n",
    "            true_labels.append([unique_labels[torch.argmax(l)] for p, l in zip(pred, label) if l[0] != -100])\n",
    "        \n",
    "        mlb = MultiLabelBinarizer(classes=unique_labels)\n",
    "        true_preds_bin = mlb.fit_transform(true_preds)\n",
    "        true_labels_bin = mlb.transform(true_labels)\n",
    "        # Compute recall, precision and f5 score\n",
    "        recall = recall_score(true_labels_bin, true_preds_bin, average='samples')\n",
    "        precision = precision_score(true_labels_bin, true_preds_bin, average='samples')\n",
    "        # Use modified f5 score to measure the performance\n",
    "        f5_score = (1 + 5*5) * (recall * precision / (5*5*precision + recall))\n",
    "        result = {'f5': f5_score,  \n",
    "                  'recall': recall,\n",
    "                  'precision': precision}\n",
    "        # print(f\"result = {result}\")\n",
    "        return result\n",
    "    except Exception as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=3): 100%|██████████| 5785/5785 [00:10<00:00, 529.48 examples/s]\n",
      "Map (num_proc=3): 100%|██████████| 1022/1022 [00:03<00:00, 287.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#Preparing the datasets for token classification\n",
    "data = json.load(open('data/train.json'))\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.15, random_state=42)  \n",
    "# train_labels = [oh_encoder(x['labels']) for x in train_data]\n",
    "\n",
    "trainset = datasets.Dataset.from_dict({\n",
    "    'full_text': [x['full_text'] for x in train_data],\n",
    "    'document': [x['document'] for x in train_data],\n",
    "    'tokens': [x['tokens'] for x in train_data],\n",
    "    'trailing_whitespace': [x['trailing_whitespace'] for x in train_data],\n",
    "    'labels' :[x['labels'] for x in train_data]\n",
    "})\n",
    "\n",
    "trainset = trainset.map(tokenize, fn_kwargs = {\"tokenizer\": tokenizer}, num_proc=3)\n",
    "#trainset['labels'] = oh_encoder(trainset['labels'])\n",
    "#train_labels = [oh_encoder(x['labels'] for x in train_data)]\n",
    "\n",
    "\n",
    "# val_labels = [oh_encoder(x['labels']) for x in val_data]\n",
    "\n",
    "valset = datasets.Dataset.from_dict({\n",
    "    'full_text': [x['full_text'] for x in val_data],\n",
    "    'document': [x['document'] for x in val_data],\n",
    "    'tokens': [x['tokens'] for x in val_data],\n",
    "    'trailing_whitespace': [x['trailing_whitespace'] for x in val_data],\n",
    "    'labels' :[x['labels'] for x in val_data]\n",
    "})\n",
    "valset = valset.map(tokenize, fn_kwargs = {\"tokenizer\": tokenizer}, num_proc=3)\n",
    "#valset['labels'] = oh_encoder(valset['labels'])\n",
    "\n",
    "\n",
    "#val_labels = [oh_encoder(x['labels'] for x in val_data)]\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data: 5785 || Number of validation data: 1022\n"
     ]
    }
   ],
   "source": [
    "#First item\n",
    "print(f\"Number of training data: {len(trainset)} || Number of validation data: {len(valset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not Required'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Not Required'''\n",
    "# def to_dict(data):\n",
    "#     dict_of_lists = {}\n",
    "#     for d in data:\n",
    "#         for key, value in d.items():\n",
    "#             if key in dict_of_lists:\n",
    "#                 dict_of_lists[key].append(value)\n",
    "#             else:\n",
    "#                 dict_of_lists[key] = [value]\n",
    "#     return dict_of_lists\n",
    "\n",
    "# trainset = to_dict(trainset)\n",
    "# #trainset['labels'] = train_labels\n",
    "\n",
    "# valset = to_dict(valset)\n",
    "# #valset['labels'] = val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Custom_data(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        self.data = data_dict['input_ids']\n",
    "        self.attention_mask = data_dict['attention_mask']\n",
    "        self.labels = data_dict['labels']\n",
    "        self.document = data_dict['document']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx]), torch.tensor(self.attention_mask[idx]), oh_encoder(self.labels[idx]), torch.tensor(self.document[idx])\n",
    "\n",
    "custom_train = Custom_data(trainset)\n",
    "custom_val = Custom_data(valset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Uncomment to check'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_collate(batch):\n",
    "    input_ids, attention_mask, labels, doc = zip(*batch)\n",
    "    # print(doc)\n",
    "    # print(max(len(input_ids[0]), len(input_ids[1])), max(len(labels[0]), len(labels[1])))\n",
    "    #print(input_ids[0], labels[0])\n",
    "\n",
    "    # Pad the input_ids and labels\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    padded_attention_mask = pad_sequence(attention_mask, batch_first = True, padding_value = 0)\n",
    "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=-100)  # Padding for labels in token classification\n",
    "\n",
    "    return padded_input_ids, padded_attention_mask, padded_labels, doc\n",
    "\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(custom_train, batch_size=batch_size, collate_fn = custom_collate, shuffle = True)\n",
    "val_dataloader = DataLoader(custom_val, batch_size=batch_size, collate_fn=custom_collate, shuffle=False)\n",
    "\n",
    "\n",
    "'''Uncomment to check'''\n",
    "# for i, (input_ids, attention_mask, labels, doc) in enumerate(val_dataloader):\n",
    "#     print(f'Batch {i + 1}:')\n",
    "#     print('Input IDs:', input_ids.size())\n",
    "#     print('Attention_mask:', attention_mask.size())\n",
    "#     print('Labels:', labels.size())\n",
    "#     print('Document:', doc)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, custom_val, batch_size, custom_collate, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    avg_val_loss = 0\n",
    "    avg_val_score = 0\n",
    "    #with torch.no_grad:\n",
    "    val_loss = 0\n",
    "    val_score = 0\n",
    "    val_dataloader = DataLoader(custom_val, batch_size = batch_size, collate_fn = custom_collate, shuffle = False)\n",
    "\n",
    "    for batch, (input_ids, attention_mask, labels, doc) in enumerate(val_dataloader):\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        results = compute_metrics(outputs, labels)\n",
    "\n",
    "        val_loss += loss\n",
    "        val_score += results['f5']\n",
    "\n",
    "        if batch%200 == 0 or batch+1 == len(val_dataloader): \n",
    "            print(\"********** For Validation Set **********\")                                                                       \n",
    "            print(f\"Completed {batch+1}/{len(val_dataloader)}, with current val_loss: {loss},\\n current results:{results}\") \n",
    "\n",
    "    avg_val_loss = val_loss / len(custom_val) \n",
    "    avg_val_score = val_score/len(val_dataloader)\n",
    "\n",
    "    print(f\"Average val_loss: {avg_val_loss}, avgerage val_score = {avg_val_score}\")\n",
    "\n",
    "    return avg_val_loss, avg_val_score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, custom_train, custom_val, batch_size, custom_collate, epochs, optimizer, criterion,  device):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        avg_train_loss = 0\n",
    "        avg_train_score = 0\n",
    "        train_loss = 0\n",
    "        train_score = 0\n",
    "\n",
    "        \n",
    "        train_dataloader = DataLoader(custom_train, batch_size = batch_size, collate_fn = custom_collate, shuffle = True)\n",
    "        print('Starting training...')\n",
    "        \n",
    "        for batch, (input_ids, attention_mask, labels, doc) in enumerate(train_dataloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask).logits\n",
    "            # print(outputs.size())\n",
    "            # print(labels.size())\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "            results = compute_metrics(outputs, labels)\n",
    "            train_loss += loss\n",
    "            train_score+= results['f5']\n",
    "\n",
    "            if batch%200 == 0 or batch+1 == len(train_dataloader):\n",
    "                print(f\"Completed {batch+1}/{len(train_dataloader)}, with current train_loss: {loss},\\n current results:{results}\")\n",
    "\n",
    "        avg_train_loss = train_loss / len(custom_train)    \n",
    "        avg_train_score = train_score / len(train_dataloader)\n",
    "        \n",
    "        print()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Average train_loss = {avg_train_loss}, average train_score = {avg_train_score}\")\n",
    "\n",
    "        print()\n",
    "        print(\"Starting to validate\")\n",
    "        val_loss, val_score = val(model, custom_val, batch_size, custom_collate, criterion, device)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Completed 1/724, with current train_loss: -214758.21917217053,\n",
      " current results:{'f5': 0.5611510791366906, 'recall': 0.75, 'precision': 0.07692307692307693}\n",
      "Completed 201/724, with current train_loss: -8473616.9874801,\n",
      " current results:{'f5': 0.24711473183978275, 'recall': 0.29166666666666663, 'precision': 0.05128205128205128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_23480\\227676074.py:21: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  f5_score = (1 + 5*5) * (recall * precision / (5*5*precision + recall))\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_23480\\227676074.py:21: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  f5_score = (1 + 5*5) * (recall * precision / (5*5*precision + recall))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 401/724, with current train_loss: -2140683.9119276796,\n",
      " current results:{'f5': 0.5611510791366906, 'recall': 0.75, 'precision': 0.07692307692307693}\n",
      "Completed 601/724, with current train_loss: -8764948.052711923,\n",
      " current results:{'f5': 0.5986842105263158, 'recall': 0.875, 'precision': 0.06730769230769232}\n",
      "Completed 724/724, with current train_loss: 3384.496364271985,\n",
      " current results:{'f5': 0.8863636363636365, 'recall': 1.0, 'precision': 0.23076923076923078}\n",
      "\n",
      "Starting to validate\n",
      "********** For Validation Set **********\n",
      "Completed 1/2893, with current train_loss: -6276141.381947178,\n",
      " current results:{'f5': 0.6850237717908083, 'recall': 0.875, 'precision': 0.10657051282051283}\n",
      "********** For Validation Set **********\n",
      "Completed 128/2893, with current train_loss: -4008119.1293233195,\n",
      " current results:{'f5': 0.5032156291712171, 'recall': 0.611111111111111, 'precision': 0.09294871794871795}\n",
      "Average val_loss: -1297548.976671241, avgerage val_score = 0.46468874419262407\n",
      "\n",
      "Epoch 1/5: Average train_loss = -595489.3060069953, average train_score = nan\n",
      "Starting training...\n",
      "Completed 1/724, with current train_loss: -10917208.220424453,\n",
      " current results:{'f5': 0.6635578162143126, 'recall': 0.825, 'precision': 0.11261655011655011}\n",
      "Completed 201/724, with current train_loss: -14465380.495653437,\n",
      " current results:{'f5': 0.48846011816839, 'recall': 0.6875, 'precision': 0.05929487179487179}\n",
      "Completed 401/724, with current train_loss: -21366869.987279046,\n",
      " current results:{'f5': 0.3672316384180791, 'recall': 0.5, 'precision': 0.04807692307692308}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_23480\\227676074.py:21: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  f5_score = (1 + 5*5) * (recall * precision / (5*5*precision + recall))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 601/724, with current train_loss: -30485861.373218972,\n",
      " current results:{'f5': 0.4304605722260992, 'recall': 0.5416666666666667, 'precision': 0.07019230769230769}\n",
      "Completed 724/724, with current train_loss: 2535.4783717623127,\n",
      " current results:{'f5': 0.6842105263157894, 'recall': 1.0, 'precision': 0.07692307692307693}\n",
      "\n",
      "Starting to validate\n",
      "********** For Validation Set **********\n",
      "Completed 1/2893, with current train_loss: -12299735.854082098,\n",
      " current results:{'f5': 0.5968389362769695, 'recall': 0.75, 'precision': 0.09775641025641026}\n",
      "********** For Validation Set **********\n",
      "Completed 128/2893, with current train_loss: -7932935.462604306,\n",
      " current results:{'f5': 0.33042846768336953, 'recall': 0.38888888888888884, 'precision': 0.06944444444444443}\n",
      "Average val_loss: -2554975.596225557, avgerage val_score = 0.4478004235548974\n",
      "\n",
      "Epoch 2/5: Average train_loss = -1789486.7280355915, average train_score = nan\n",
      "Starting training...\n",
      "Completed 1/724, with current train_loss: -8106959.40560964,\n",
      " current results:{'f5': 0.5586908991527739, 'recall': 0.7083333333333333, 'precision': 0.08894230769230768}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_collate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, custom_train, custom_val, batch_size, custom_collate, epochs, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     28\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 31\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     33\u001b[0m train_score\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf5\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(outputs, labels, unique_labels)\u001b[0m\n\u001b[0;32m      9\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pred, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels):\n\u001b[1;32m---> 11\u001b[0m     true_preds\u001b[38;5;241m.\u001b[39mappend([unique_labels[p] \u001b[38;5;28;01mfor\u001b[39;00m p, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pred, label) \u001b[38;5;28;01mif\u001b[39;00m l[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m])\n\u001b[0;32m     12\u001b[0m     true_labels\u001b[38;5;241m.\u001b[39mappend([unique_labels[torch\u001b[38;5;241m.\u001b[39margmax(l)] \u001b[38;5;28;01mfor\u001b[39;00m p, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pred, label) \u001b[38;5;28;01mif\u001b[39;00m l[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m])\n\u001b[0;32m     14\u001b[0m mlb \u001b[38;5;241m=\u001b[39m MultiLabelBinarizer(classes\u001b[38;5;241m=\u001b[39munique_labels)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "epochs = 2\n",
    "\n",
    "train(model, custom_train, custom_val, batch_size, custom_collate, epochs, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting from predictions to NER labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to label each token at NER stage\n",
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "    \n",
    "    idx = 0\n",
    "    spans = []\n",
    "    span = []\n",
    "\n",
    "    for i, token in enumerate(document):\n",
    "        if token != target[idx]:\n",
    "            idx = 0\n",
    "            span = []\n",
    "            continue\n",
    "        span.append(i)\n",
    "        idx += 1\n",
    "        if idx == len(target):\n",
    "            spans.append(span)\n",
    "            span = []\n",
    "            idx = 0\n",
    "            continue\n",
    "    \n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DON'T RUN ##\n",
    "#### From KAGGLE: https://www.kaggle.com/code/manavtrivedi/0-967-nlp-sakura/notebook ####\n",
    "\n",
    "triplets = []\n",
    "pairs = set()  # membership operation using set is faster O(1) than that of list O(n)\n",
    "processed = []\n",
    "emails = []\n",
    "phone_nums = []\n",
    "urls = []\n",
    "streets = []\n",
    "\n",
    "# For each prediction, token mapping, offsets, tokens, and document in the dataset\n",
    "for p, token_map, offsets, tokens, doc, full_text in zip(\n",
    "    preds_final, \n",
    "    ds[\"token_map\"], \n",
    "    ds[\"offset_mapping\"], \n",
    "    ds[\"tokens\"], \n",
    "    ds[\"document\"],\n",
    "    ds[\"full_text\"]\n",
    "):\n",
    "\n",
    "    # Iterate through each token prediction and its corresponding offsets\n",
    "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "        label_pred = id2label[str(token_pred)]  # Predicted label from token\n",
    "        if start_idx + end_idx == 0:\n",
    "            continue\n",
    "        if token_map[start_idx] == -1:\n",
    "            start_idx += 1\n",
    "        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "            start_idx += 1\n",
    "        if start_idx >= len(token_map):\n",
    "            break\n",
    "        token_id = token_map[start_idx]  # Token ID at start index\n",
    "        if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n",
    "            continue\n",
    "        pair = (doc, token_id)\n",
    "        if pair not in pairs:\n",
    "            processed.append({\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]})\n",
    "            pairs.add(pair)\n",
    "    \n",
    "    # email\n",
    "    for token_idx, token in enumerate(tokens):\n",
    "        if re.fullmatch(email_regex, token) is not None:\n",
    "            emails.append(\n",
    "                {\"document\": doc, \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "            )\n",
    "                \n",
    "    # phone number\n",
    "    matches = phone_num_regex.findall(full_text)\n",
    "    if not matches:\n",
    "        continue\n",
    "    for match in matches:\n",
    "        target = [t.text for t in nlp.tokenizer(match)]\n",
    "        matched_spans = find_span(target, tokens)\n",
    "    for matched_span in matched_spans:\n",
    "        for intermediate, token_idx in enumerate(matched_span):\n",
    "            prefix = \"I\" if intermediate else \"B\"\n",
    "            phone_nums.append(\n",
    "                {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": tokens[token_idx]}\n",
    "            )\n",
    "    \n",
    "    # url\n",
    "    matches = url_regex.findall(full_text)\n",
    "    if not matches:\n",
    "        continue\n",
    "    for match in matches:\n",
    "        target = [t.text for t in nlp.tokenizer(match)]\n",
    "        matched_spans = find_span(target, tokens)\n",
    "    for matched_span in matched_spans:\n",
    "        for intermediate, token_idx in enumerate(matched_span):\n",
    "            prefix = \"I\" if intermediate else \"B\"\n",
    "            urls.append(\n",
    "                {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-URL_PERSONAL\", \"token_str\": tokens[token_idx]}\n",
    "            )\n",
    "    \n",
    "    # street\n",
    "#     matches = street_regex.findall(full_text)\n",
    "#     if not matches:\n",
    "#         continue\n",
    "#     for match in matches:\n",
    "#         target = [t.text for t in nlp.tokenizer(match)]\n",
    "#         matched_spans = find_span(target, tokens)\n",
    "#     for matched_span in matched_spans:\n",
    "#         for intermediate, token_idx in enumerate(matched_span):\n",
    "#             prefix = \"I\" if intermediate else \"B\"\n",
    "#             streets.append(\n",
    "#                 {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-STREET_ADDRESS\", \"token_str\": tokens[token_idx]}\n",
    "#             )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
