{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some references**\n",
    "\n",
    "https://www.kaggle.com/code/minhsienweng/train-infer-pii-detection-deberta-v3\n",
    "\n",
    "(no training) https://www.kaggle.com/code/manavtrivedi/0-967-nlp-sakura/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datasets\n",
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.lang.en import English\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ignite.metrics import Fbeta\n",
    "from functools import partial\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.nn.functional import softmax\n",
    "from peft import get_peft_model, LoraConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 labels, with the following labels:\n",
      " ['B-ID_NUM', 'B-STREET_ADDRESS', 'B-USERNAME', 'B-URL_PERSONAL', 'I-STREET_ADDRESS', 'B-NAME_STUDENT', 'I-URL_PERSONAL', 'I-ID_NUM', 'I-PHONE_NUM', 'O', 'B-PHONE_NUM', 'B-EMAIL', 'I-NAME_STUDENT']\n"
     ]
    }
   ],
   "source": [
    "#Finding out the number of labels\n",
    "data = json.load(open('data/train.json'))\n",
    "\n",
    "\n",
    "all_labels = set()\n",
    "\n",
    "for d in data:\n",
    "    all_labels = all_labels.union(set(d['labels']))\n",
    "\n",
    "print(f\"{len(list(all_labels))} labels, with the following labels:\\n {list(all_labels)}\")\n",
    "del data\n",
    "\n",
    "label2id = {label:index for index,label in enumerate(all_labels)}\n",
    "id2label = {index:label for index,label in enumerate(all_labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-ID_NUM', 'B-STREET_ADDRESS', 'B-USERNAME', 'B-URL_PERSONAL', 'I-STREET_ADDRESS', 'B-NAME_STUDENT', 'I-URL_PERSONAL', 'I-ID_NUM', 'I-PHONE_NUM', 'O', 'B-PHONE_NUM', 'B-EMAIL', 'I-NAME_STUDENT'}\n"
     ]
    }
   ],
   "source": [
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to one-hot vector\n",
    "def oh_encoder(labels):  #label: array of output for each sentence\n",
    "\n",
    "    # unique_labels = ['O', 'B-NAME_STUDENT','I-NAME_STUDENT','B-PHONE_NUM', 'I-PHONE_NUM','B-ID_NUM', 'I-ID_NUM',  'B-URL_PERSONAL','I-URL_PERSONAL',\n",
    "    #                   'B-STREET_ADDRESS', 'I-STREET_ADDRESS',  'B-EMAIL', 'B-USERNAME']\n",
    "    \n",
    "    \n",
    "    unique_labels = ['O', 'B-NAME_STUDENT','I-NAME_STUDENT','B-URL_PERSONAL', 'B-ID_NUM','I-ID_NUM','B-EMAIL','I-STREET_ADDRESS',\n",
    "                     'I-PHONE_NUM', 'B-USERNAME', 'B-PHONE_NUM','B-STREET_ADDRESS', 'I-URL_PERSONAL']\n",
    "    \n",
    "    labels_oh = []\n",
    "    for label in labels:    #label: str\n",
    "        label_oh = [float(0)]*len(unique_labels)\n",
    "        for k in range(len(unique_labels)):\n",
    "            if unique_labels[k] == label:\n",
    "                label_oh[k] = 1\n",
    "                #labels_oh.append(torch.tensor(label_oh, requires_grad=True))\n",
    "                labels_oh.append(label_oh)\n",
    "                break\n",
    "                \n",
    "\n",
    "    #return torch.tensor(labels_oh, requires_grad=True)\n",
    "    return torch.tensor(labels_oh, requires_grad=True, dtype=float)    #list of one-hot labels as tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer):\n",
    "    import numpy as np\n",
    "    # Preprocess the tokens and labels by adding trailing whitespace and labels\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for token, label, t_ws in zip(example[\"tokens\"], \n",
    "                                  example[\"labels\"],\n",
    "                                  example[\"trailing_whitespace\"]):\n",
    "        tokens.append(token)\n",
    "        labels.extend([label] * len(token))\n",
    "        # Added trailing whitespace and label if true and \n",
    "        if t_ws:\n",
    "            tokens.append(\" \")\n",
    "            # labels.append(oh_encoder(\"O\"))\n",
    "            labels.append(\"O\")\n",
    "    \n",
    "    text = \"\".join(tokens)\n",
    "    # print(f\"len(text)={len(text)}, len(tokens)={len(tokens)}\")\n",
    "    # tokenization without truncation\n",
    "    tokenized = tokenizer(text, return_offsets_mapping=True,\n",
    "                          truncation=False)\n",
    "    #labels = np.array(labels)\n",
    "    # Labels\n",
    "    token_labels = []\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # Added 'O' \n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            #token_labels.append(label2id[\"O\"]) \n",
    "            #token_labels.append(oh_encoder(\"O\"))\n",
    "            token_labels.append(\"O\")\n",
    "        else:\n",
    "            # case when the text starts with whitespace\n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "            # Convert label to id (int)\n",
    "            #label_id = label2id[labels[start_idx]]\n",
    "            label_id = labels[start_idx]\n",
    "            #token_labels.append(oh_encoder(label_id))\n",
    "            token_labels.append(label_id)\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": len(tokenized.input_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(outputs, labels, unique_labels = ['O', 'B-NAME_STUDENT','I-NAME_STUDENT','B-URL_PERSONAL', \n",
    "                                                          'B-ID_NUM','I-ID_NUM','B-EMAIL','I-STREET_ADDRESS',\n",
    "                                                          'I-PHONE_NUM', 'B-USERNAME', 'B-PHONE_NUM','B-STREET_ADDRESS', 'I-URL_PERSONAL']):    \n",
    "    try:\n",
    "        #print(\"Compute metrics\")\n",
    "        predictions = torch.argmax(softmax(outputs, dim=2), dim=2)\n",
    "        # Include prediction Remove ignored index (special tokens)\n",
    "        true_preds = []\n",
    "        true_labels = []\n",
    "        for pred, label in zip(predictions, labels):\n",
    "            true_preds.append([unique_labels[p] for p, l in zip(pred, label) if l[0] != -100])\n",
    "            true_labels.append([unique_labels[torch.argmax(l)] for p, l in zip(pred, label) if l[0] != -100])\n",
    "        \n",
    "        mlb = MultiLabelBinarizer(classes=unique_labels)\n",
    "        true_preds_bin = mlb.fit_transform(true_preds)\n",
    "        true_labels_bin = mlb.transform(true_labels)\n",
    "        # Compute recall, precision and f5 score\n",
    "        recall = recall_score(true_labels_bin, true_preds_bin, average='samples')\n",
    "        precision = precision_score(true_labels_bin, true_preds_bin, average='samples')\n",
    "        # Use modified f5 score to measure the performance\n",
    "        f5_score = (1 + 5*5) * (recall * precision / (5*5*precision + recall))\n",
    "        result = {'f5': f5_score,  \n",
    "                  'recall': recall,\n",
    "                  'precision': precision}\n",
    "        # print(f\"result = {result}\")\n",
    "        return result\n",
    "    except Exception as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, num_classes=len(all_labels)):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        mask = (targets != -100).float()\n",
    "        targets = targets.clamp(min=0)\n",
    "        #targets = F.one_hot(targets, num_classes=self.num_classes).float()\n",
    "\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        # F_loss = F_loss.mean(dim=1)\n",
    "        F_loss = torch.mul(F_loss, mask).mean(dim=1)\n",
    "        \n",
    "        return F_loss.sum()/mask.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: DeBERTa (Custom)\n",
    "\n",
    "Using a pretrained DeBERTa, we will build a classifier head on top of it to predict the class at token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Classifier(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim = 13):\n",
    "#         super(Classifier, self).__init__()\n",
    "\n",
    "#         self.dropout_prob = 0.3\n",
    "#         self.final_activation = nn.Softmax(dim = -1)\n",
    "\n",
    "#         self.linear = nn.Sequential(\n",
    "#             nn.ReLU(nn.Linear(input_dim, hidden_dim)),\n",
    "            \n",
    "#             nn.Dropout(self.dropout_prob),\n",
    "#             nn.ReLU(nn.Linear(hidden_dim, hidden_dim*2)),\n",
    "\n",
    "#             nn.Dropout(self.dropout_prob),\n",
    "#             nn.ReLU(nn.Linear(hidden_dim*2, hidden_dim)),\n",
    "\n",
    "#             nn.Linear(hidden_dim, output_dim)\n",
    "#         )\n",
    "       \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         logit = self.linear(x)\n",
    "#         return self.final_activation(logit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Classifier(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim = 13):\n",
    "#         super(Classifier, self).__init__()\n",
    "\n",
    "#         self.dropout_prob = 0.3\n",
    "#         self.final_activation = nn.Softmax(dim = -1)\n",
    "\n",
    "#         self.dropout = nn.Dropout(self.dropout_prob)\n",
    "#         self.linear1 = nn.ReLU(nn.Linear(input_dim,hidden_dim))\n",
    "#         self.linear2 = nn.ReLU(nn.Linear(hidden_dim, hidden_dim*2))                            \n",
    "#         self.linear3 = nn.ReLU(nn.Linear(hidden_dim*2, hidden_dim))\n",
    "#         self.output = nn.Linear(hidden_dim,output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         print('now start linear1')\n",
    "#         print(x.size())\n",
    "#         x = self.linear1(x)\n",
    "\n",
    "#         print('now start linear2')\n",
    "#         print(x.size())\n",
    "#         x = self.linear2(self.dropout(x))\n",
    "\n",
    "#         print('now start linear3')\n",
    "#         print(x.size())\n",
    "#         x = self.linear3(self.dropout(x))\n",
    "\n",
    "#         print('now start output')\n",
    "#         print(x.size())\n",
    "#         logit= self.output(x)\n",
    "        \n",
    "#         print('now start activation')\n",
    "#         output =  self.final_activation(logit)\n",
    "        \n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Deberta_Classif(Classifier):\n",
    "#     def __init__(self, model_name, classif_input, classif_hidden, classif_output = 13, finetune = False):\n",
    "#         super(Deberta_Classif, self).__init__(classif_input, classif_input)\n",
    "#         self.ft = finetune\n",
    "\n",
    "#         self.extractor = AutoModelForTokenClassification.from_pretrained(model_name).base_model\n",
    "\n",
    "#         if not finetune:\n",
    "#             for param in self.extractor.parameters():\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "#         self.extractor_num_param =  sum(p.numel() for p in self.extractor.parameters())\n",
    "#         self.extractor_num_param_grad = sum(p.numel() for p in self.extractor.parameters() if p.requires_grad)\n",
    "#         self.extractor_name = \"DeBERTa\"\n",
    "                \n",
    "\n",
    "#         self.classifier = Classifier(input_dim=classif_input, hidden_dim=classif_hidden ,output_dim=classif_output)\n",
    "#         self.classifier_num_param = sum(p.numel() for p in self.classifier.parameters() if p.requires_grad)\n",
    "\n",
    "        \n",
    "#     def count_param(self):\n",
    "        \n",
    "#         if self.ft:\n",
    "#             type = 'finetuned'\n",
    "#             num_param = self.extractor_num_param_grad + self.classifier_num_param\n",
    "#         else:\n",
    "#             type = 'non-finetuned'\n",
    "#             num_param = self.extractor_num_param + self.classifier_num_param\n",
    "\n",
    "#         print(f\"Number of parameters in {type} {self.extractor_name} model is {num_param:,}\")\n",
    "    \n",
    "    \n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         x = self.extractor(input_ids, attention_mask).last_hidden_state\n",
    "#         x = self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's trainable parameters:  9,997\n",
      "Model's total parameters:  141,314,317\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "# model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "model_name = \"microsoft/deberta-v3-small\"\n",
    "#model = Deberta_Classif(model_name, classif_input = 768, classif_hidden = 100, classif_output = 13, finetune=False)  #Extractor output has dim 768 \n",
    "config = AutoModelForTokenClassification.from_pretrained(model_name).config\n",
    "\n",
    "config.update({'num_labels': 13})\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"FEATURE_EXTRACTION\",\n",
    "        inference_mode=False,\n",
    "        target_modules=[\"query_proj\", \"key_proj\", \"value_proj\", \"output.dense\"])\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#                 load_in_4bit=True,\n",
    "#                 bnb_4bit_quant_type=\"nf4\",\n",
    "#                 bnb_4bit_use_double_quant=False,\n",
    "#                 bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#                 llm_int8_skip_modules=['classifier']\n",
    "#             )\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_skip_modules=['classifier']\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, config = config, \n",
    "                                                        ignore_mismatched_sizes=True,)\n",
    "                                                        #quantization_config=bnb_config)\n",
    "# model = get_peft_model(model, peft_config)\n",
    "\n",
    "# print(model)\n",
    "\n",
    "for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Model's trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad): ,}\")\n",
    "print(f\"Model's total parameters: {sum(p.numel() for p in model.parameters()): ,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' \n",
    "model = model.to(device)\n",
    "epochs = 5\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of = 32, max_length=3500)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split completed\n",
      "trainset loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=3): 100%|██████████| 5785/5785 [00:11<00:00, 504.10 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset mapped\n",
      "valset loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=3): 100%|██████████| 1022/1022 [00:04<00:00, 222.41 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valset mapped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Preparing the datasets for token classification\n",
    "data = json.load(open('data/train.json'))\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.15, random_state=42)  \n",
    "print('Data split completed')\n",
    "\n",
    "# # Limit to 100 for testing\n",
    "# train_data = train_data[:100]\n",
    "# val_data = val_data[:100]\n",
    "\n",
    "trainset = datasets.Dataset.from_dict({\n",
    "    'full_text': [x['full_text'] for x in train_data],\n",
    "    'document': [x['document'] for x in train_data],\n",
    "    'tokens': [x['tokens'] for x in train_data],\n",
    "    'trailing_whitespace': [x['trailing_whitespace'] for x in train_data],\n",
    "    'labels' :[x['labels'] for x in train_data]\n",
    "    # 'labels' :[oh_encoder(x['labels']) for x in train_data] \n",
    "})\n",
    "print('trainset loaded')\n",
    "\n",
    "trainset = trainset.map(tokenize, fn_kwargs = {\"tokenizer\": tokenizer}, num_proc=3)\n",
    "#train_labels = [oh_encoder(x['labels'] for x in train_data)]\n",
    "print('trainset mapped')\n",
    "\n",
    "# val_labels = [oh_encoder(x['labels']) for x in val_data]\n",
    "\n",
    "valset = datasets.Dataset.from_dict({\n",
    "    'full_text': [x['full_text'] for x in val_data],\n",
    "    'document': [x['document'] for x in val_data],\n",
    "    'tokens': [x['tokens'] for x in val_data],\n",
    "    'trailing_whitespace': [x['trailing_whitespace'] for x in val_data],\n",
    "    'labels' :[x['labels'] for x in val_data]\n",
    "    # 'labels' :[oh_encoder(x['labels']) for x in val_data]\n",
    "})\n",
    "print('valset loaded')\n",
    "\n",
    "valset = valset.map(tokenize, fn_kwargs = {\"tokenizer\": tokenizer}, num_proc=3)\n",
    "print('valset mapped')\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data: 5785 || Number of validation data: 1022\n"
     ]
    }
   ],
   "source": [
    "#First item\n",
    "print(f\"Number of training data: {len(trainset)} || Number of validation data: {len(valset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not Required'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Not Required'''\n",
    "# def to_dict(data):\n",
    "#     dict_of_lists = {}\n",
    "#     for d in data:\n",
    "#         for key, value in d.items():\n",
    "#             if key in dict_of_lists:\n",
    "#                 dict_of_lists[key].append(value)\n",
    "#             else:\n",
    "#                 dict_of_lists[key] = [value]\n",
    "#     return dict_of_lists\n",
    "\n",
    "# trainset = to_dict(trainset)\n",
    "# #trainset['labels'] = train_labels\n",
    "\n",
    "# valset = to_dict(valset)\n",
    "# #valset['labels'] = val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_data(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        self.data = data_dict['input_ids']\n",
    "        self.attention_mask = data_dict['attention_mask']\n",
    "        self.labels = data_dict['labels']\n",
    "        self.doc_no = data_dict['document']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx]), torch.tensor(self.attention_mask[idx]), oh_encoder(self.labels[idx]) , torch.tensor(self.doc_no[idx])\n",
    "\n",
    "custom_train = Custom_data(trainset)\n",
    "custom_val = Custom_data(valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Uncomment to check'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_collate(batch):\n",
    "    '''\n",
    "    For padding\n",
    "    '''\n",
    "    input_ids, attention_mask, one_hot_labels, document = zip(*batch)\n",
    "    # Pad the input_ids and labels\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    padded_attention_mask = pad_sequence(attention_mask, batch_first = True, padding_value = 0)\n",
    "    padded_labels = pad_sequence(one_hot_labels, batch_first=True, padding_value=-100)  \n",
    "    \n",
    "    return padded_input_ids, padded_attention_mask, padded_labels, document\n",
    "\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(custom_train, batch_size=batch_size, collate_fn = custom_collate)\n",
    "val_dataloader = DataLoader(custom_val, batch_size=batch_size, collate_fn=custom_collate)\n",
    "\n",
    "'''Uncomment to check'''\n",
    "# for i, (input_ids, attention_mask, labels, doc) in enumerate(val_dataloader):\n",
    "#     print(f'Batch {i + 1}:')\n",
    "#     print('Input IDs:', input_ids.size())\n",
    "#     print('Attention_mask:', attention_mask.size())\n",
    "#     print('Labels:', labels.size())\n",
    "#     print('Document:', doc)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, custom_val, batch_size, custom_collate, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    avg_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        val_score = {'f5': 0, 'recall': 0, 'precision': 0}\n",
    "        val_dataloader = DataLoader(custom_val, batch_size = batch_size, collate_fn = custom_collate, shuffle = False)\n",
    "\n",
    "        for batch, (input_ids, attention_mask, labels, doc) in enumerate(val_dataloader):\n",
    "            \n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            results = compute_metrics(outputs, labels)\n",
    "\n",
    "            val_loss += loss\n",
    "            val_score['f5'] += results['f5']\n",
    "            val_score['recall'] += results['recall']\n",
    "            val_score['precision'] += results['precision']\n",
    "\n",
    "            if batch%200 == 0 or batch+1 == len(val_dataloader): \n",
    "                print(\"********** For Validation Set **********\")                                                                       \n",
    "                print(f\"Completed {batch+1}/{len(val_dataloader)}, with current val_loss: {loss: .4e},\\n current results:{results}\") \n",
    "\n",
    "    avg_val_loss = val_loss / len(custom_val) \n",
    "    for k in val_score:\n",
    "        val_score[k] /= len(val_dataloader)\n",
    "    # avg_val_score = val_score/len(val_dataloader)\n",
    "\n",
    "    print(f\"Average val_loss: {avg_val_loss: .4e}, avgerage val_score = {val_score}\")\n",
    "\n",
    "    return avg_val_loss, val_score['f5']    #Return only f5\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train(model, custom_train, custom_val, batch_size, custom_collate, epochs, optimizer, criterion,  device):\n",
    "    \n",
    "    model.train()\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_score = -float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        avg_train_loss = 0\n",
    "        train_loss = 0\n",
    "        train_score = {'f5': 0, 'recall': 0, 'precision': 0}\n",
    "\n",
    "        \n",
    "        train_dataloader = DataLoader(custom_train, batch_size = batch_size, collate_fn = custom_collate, shuffle = True)\n",
    "        print('Starting training...')\n",
    "        \n",
    "        for batch, (input_ids, attention_mask, labels, doc) in enumerate(train_dataloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask).logits\n",
    "            # print(outputs.size())\n",
    "            # print(labels.size())\n",
    "            # print(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # print(f\"Loss: {loss:.4e}\")\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "            results = compute_metrics(outputs, labels)\n",
    "            train_loss += loss \n",
    "            train_score['f5']+= results['f5']\n",
    "            train_score['recall'] += results['recall']\n",
    "            train_score['precision'] += results['precision']\n",
    "\n",
    "\n",
    "\n",
    "            if batch%200 == 0 or batch+1 == len(train_dataloader):\n",
    "                results = compute_metrics(outputs, labels)\n",
    "                print(f\"Completed {batch+1}/{len(train_dataloader)}, with current train_loss: {loss: .4e},\\n current results:{results}\")\n",
    "\n",
    "        avg_train_loss = train_loss / len(custom_train)    \n",
    "        for k in train_score:\n",
    "            train_score[k] /= len(train_dataloader)\n",
    "        #avg_train_score = train_score / len(train_dataloader)\n",
    "\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)                \n",
    "        \n",
    "        print()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Average train_loss = {avg_train_loss: .4e}, average train_score = {train_score}\")\n",
    "\n",
    "        print()\n",
    "        print(\"Starting to validate\")\n",
    "        val_loss, val_score = val(model, custom_val, batch_size, custom_collate, criterion, device)\n",
    "\n",
    "        print(f\"Epoch time: {epoch_mins}m {epoch_secs}s\")\n",
    "        print()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'deberta_small_best_loss.pt')\n",
    "\n",
    "        if val_score > best_val_score:\n",
    "            best_val_score = val_score\n",
    "            torch.save(model.state_dict(), 'deberta_small_best_score.pt')\n",
    "  \n",
    "        \n",
    "    torch.save(model.state_dict(), f'deberta_small_{epochs}.pt')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the function below will output the loss and results every 200 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Completed 1/724, with current train_loss:  9.2220e-05,\n",
      " current results:{'f5': 0.7573015464427856, 'recall': 1.0, 'precision': 0.10715326340326341}\n",
      "Completed 201/724, with current train_loss:  9.6862e-06,\n",
      " current results:{'f5': 1.0, 'recall': 1.0, 'precision': 1.0}\n",
      "Completed 401/724, with current train_loss:  1.0302e-05,\n",
      " current results:{'f5': 0.7572815533980582, 'recall': 0.75, 'precision': 1.0}\n",
      "Completed 601/724, with current train_loss:  9.7343e-06,\n",
      " current results:{'f5': 0.9196141479099676, 'recall': 0.9166666666666666, 'precision': 1.0}\n",
      "Completed 724/724, with current train_loss:  1.2926e-05,\n",
      " current results:{'f5': 1.0, 'recall': 1.0, 'precision': 1.0}\n",
      "\n",
      "Epoch 1/5: Average train_loss =  1.7887e-06, average train_score = {'f5': 0.9069239026625755, 'recall': 0.9136423802946586, 'precision': 0.9153994696985028}\n",
      "\n",
      "Starting to validate\n",
      "********** For Validation Set **********\n",
      "Completed 1/128, with current val_loss:  1.1929e-05,\n",
      " current results:{'f5': 0.8387096774193548, 'recall': 0.8333333333333333, 'precision': 1.0}\n",
      "********** For Validation Set **********\n",
      "Completed 128/128, with current val_loss:  1.1006e-05,\n",
      " current results:{'f5': 0.7844827586206895, 'recall': 0.7777777777777777, 'precision': 1.0}\n",
      "Average val_loss:  1.1354e-06, avgerage val_score = 0.9020369408828613\n",
      "Epoch time: 24m 41s\n",
      "Starting training...\n",
      "Completed 1/724, with current train_loss:  1.6771e-05,\n",
      " current results:{'f5': 1.0, 'recall': 1.0, 'precision': 1.0}\n",
      "Completed 201/724, with current train_loss:  9.4129e-06,\n",
      " current results:{'f5': 0.9095295536791315, 'recall': 0.90625, 'precision': 1.0}\n",
      "Completed 401/724, with current train_loss:  1.2999e-05,\n",
      " current results:{'f5': 0.8387096774193548, 'recall': 0.8333333333333333, 'precision': 1.0}\n",
      "Completed 601/724, with current train_loss:  1.0479e-05,\n",
      " current results:{'f5': 1.0, 'recall': 1.0, 'precision': 1.0}\n",
      "Completed 724/724, with current train_loss:  1.5993e-05,\n",
      " current results:{'f5': 0.3421052631578947, 'recall': 0.3333333333333333, 'precision': 1.0}\n",
      "\n",
      "Epoch 2/5: Average train_loss =  1.2181e-06, average train_score = {'f5': 0.913532321081982, 'recall': 0.9106641344383047, 'precision': 0.9994820441988951}\n",
      "\n",
      "Starting to validate\n",
      "********** For Validation Set **********\n",
      "Completed 1/128, with current val_loss:  1.1442e-05,\n",
      " current results:{'f5': 0.8387096774193548, 'recall': 0.8333333333333333, 'precision': 1.0}\n",
      "********** For Validation Set **********\n",
      "Completed 128/128, with current val_loss:  1.0752e-05,\n",
      " current results:{'f5': 0.7844827586206895, 'recall': 0.7777777777777777, 'precision': 1.0}\n",
      "Average val_loss:  1.1123e-06, avgerage val_score = 0.9020369408828613\n",
      "Epoch time: 21m 55s\n",
      "Starting training...\n",
      "Completed 1/724, with current train_loss:  6.1387e-06,\n",
      " current results:{'f5': 1.0, 'recall': 1.0, 'precision': 1.0}\n",
      "Completed 201/724, with current train_loss:  8.6033e-06,\n",
      " current results:{'f5': 0.6855983772819473, 'recall': 0.6770833333333333, 'precision': 1.0}\n",
      "Completed 401/724, with current train_loss:  1.0630e-05,\n",
      " current results:{'f5': 1.0, 'recall': 1.0, 'precision': 1.0}\n",
      "Completed 601/724, with current train_loss:  1.1864e-05,\n",
      " current results:{'f5': 1.0, 'recall': 1.0, 'precision': 1.0}\n",
      "Completed 724/724, with current train_loss:  1.8164e-05,\n",
      " current results:{'f5': 1.0, 'recall': 1.0, 'precision': 1.0}\n",
      "\n",
      "Epoch 3/5: Average train_loss =  1.2195e-06, average train_score = {'f5': 0.9143227015068888, 'recall': 0.9114698434622465, 'precision': 0.9995683701657458}\n",
      "\n",
      "Starting to validate\n",
      "********** For Validation Set **********\n",
      "Completed 1/128, with current val_loss:  9.6663e-06,\n",
      " current results:{'f5': 0.8387096774193548, 'recall': 0.8333333333333333, 'precision': 1.0}\n",
      "********** For Validation Set **********\n",
      "Completed 128/128, with current val_loss:  9.0846e-06,\n",
      " current results:{'f5': 0.7844827586206895, 'recall': 0.7777777777777777, 'precision': 1.0}\n",
      "Average val_loss:  9.4907e-07, avgerage val_score = 0.9020228798971214\n",
      "Epoch time: 21m 48s\n",
      "Starting training...\n",
      "Completed 1/724, with current train_loss:  1.0468e-05,\n",
      " current results:{'f5': 0.7572815533980582, 'recall': 0.75, 'precision': 1.0}\n",
      "Completed 201/724, with current train_loss:  8.0253e-06,\n",
      " current results:{'f5': 0.9196141479099676, 'recall': 0.9166666666666666, 'precision': 1.0}\n",
      "Completed 401/724, with current train_loss:  1.0765e-05,\n",
      " current results:{'f5': 0.9196141479099676, 'recall': 0.9166666666666666, 'precision': 1.0}\n",
      "Completed 601/724, with current train_loss:  6.0576e-06,\n",
      " current results:{'f5': 0.9196141479099676, 'recall': 0.9166666666666666, 'precision': 1.0}\n",
      "Completed 724/724, with current train_loss:  2.5435e-05,\n",
      " current results:{'f5': 1.0, 'recall': 1.0, 'precision': 1.0}\n",
      "\n",
      "Epoch 4/5: Average train_loss =  1.2140e-06, average train_score = {'f5': 0.9143331798295222, 'recall': 0.911469843462246, 'precision': 0.9993957182320442}\n",
      "\n",
      "Starting to validate\n",
      "********** For Validation Set **********\n",
      "Completed 1/128, with current val_loss:  1.1864e-05,\n",
      " current results:{'f5': 0.8387096774193548, 'recall': 0.8333333333333333, 'precision': 1.0}\n",
      "********** For Validation Set **********\n",
      "Completed 128/128, with current val_loss:  1.1037e-05,\n",
      " current results:{'f5': 0.7844827586206895, 'recall': 0.7777777777777777, 'precision': 1.0}\n",
      "Average val_loss:  1.1435e-06, avgerage val_score = 0.9020221926231358\n",
      "Epoch time: 21m 59s\n",
      "Starting training...\n",
      "Completed 1/724, with current train_loss:  7.4558e-06,\n",
      " current results:{'f5': 0.8387096774193548, 'recall': 0.8333333333333333, 'precision': 1.0}\n",
      "Completed 201/724, with current train_loss:  6.4399e-06,\n",
      " current results:{'f5': 1.0, 'recall': 1.0, 'precision': 1.0}\n",
      "Completed 401/724, with current train_loss:  7.2405e-06,\n",
      " current results:{'f5': 0.9397590361445782, 'recall': 0.9375, 'precision': 1.0}\n",
      "Completed 601/724, with current train_loss:  5.9830e-06,\n",
      " current results:{'f5': 0.9196141479099676, 'recall': 0.9166666666666666, 'precision': 1.0}\n",
      "Completed 724/724, with current train_loss:  1.5245e-05,\n",
      " current results:{'f5': 0.3421052631578947, 'recall': 0.3333333333333333, 'precision': 1.0}\n",
      "\n",
      "Epoch 5/5: Average train_loss =  1.2230e-06, average train_score = {'f5': 0.913539222471489, 'recall': 0.910664134438305, 'precision': 0.9993957182320442}\n",
      "\n",
      "Starting to validate\n",
      "********** For Validation Set **********\n",
      "Completed 1/128, with current val_loss:  1.1193e-05,\n",
      " current results:{'f5': 0.8387096774193548, 'recall': 0.8333333333333333, 'precision': 1.0}\n",
      "********** For Validation Set **********\n",
      "Completed 128/128, with current val_loss:  1.0518e-05,\n",
      " current results:{'f5': 0.7844827586206895, 'recall': 0.7777777777777777, 'precision': 1.0}\n",
      "Average val_loss:  1.0919e-06, avgerage val_score = 0.9020369408828613\n",
      "Epoch time: 23m 45s\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "epochs = 5\n",
    "criterion = FocalLoss()\n",
    "\n",
    "train(model, custom_train, custom_val, batch_size, custom_collate, epochs, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting from predictions to NER labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to label each token at NER stage\n",
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "    \n",
    "    idx = 0\n",
    "    spans = []\n",
    "    span = []\n",
    "\n",
    "    for i, token in enumerate(document):\n",
    "        if token != target[idx]:\n",
    "            idx = 0\n",
    "            span = []\n",
    "            continue\n",
    "        span.append(i)\n",
    "        idx += 1\n",
    "        if idx == len(target):\n",
    "            spans.append(span)\n",
    "            span = []\n",
    "            idx = 0\n",
    "            continue\n",
    "    \n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DON'T RUN ##\n",
    "#### From KAGGLE: https://www.kaggle.com/code/manavtrivedi/0-967-nlp-sakura/notebook ####\n",
    "\n",
    "triplets = []\n",
    "pairs = set()  # membership operation using set is faster O(1) than that of list O(n)\n",
    "processed = []\n",
    "emails = []\n",
    "phone_nums = []\n",
    "urls = []\n",
    "streets = []\n",
    "\n",
    "# For each prediction, token mapping, offsets, tokens, and document in the dataset\n",
    "for p, token_map, offsets, tokens, doc, full_text in zip(\n",
    "    preds_final, \n",
    "    ds[\"token_map\"], \n",
    "    ds[\"offset_mapping\"], \n",
    "    ds[\"tokens\"], \n",
    "    ds[\"document\"],\n",
    "    ds[\"full_text\"]\n",
    "):\n",
    "\n",
    "    # Iterate through each token prediction and its corresponding offsets\n",
    "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "        label_pred = id2label[str(token_pred)]  # Predicted label from token\n",
    "        if start_idx + end_idx == 0:\n",
    "            continue\n",
    "        if token_map[start_idx] == -1:\n",
    "            start_idx += 1\n",
    "        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "            start_idx += 1\n",
    "        if start_idx >= len(token_map):\n",
    "            break\n",
    "        token_id = token_map[start_idx]  # Token ID at start index\n",
    "        if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n",
    "            continue\n",
    "        pair = (doc, token_id)\n",
    "        if pair not in pairs:\n",
    "            processed.append({\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]})\n",
    "            pairs.add(pair)\n",
    "    \n",
    "    # email\n",
    "    for token_idx, token in enumerate(tokens):\n",
    "        if re.fullmatch(email_regex, token) is not None:\n",
    "            emails.append(\n",
    "                {\"document\": doc, \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "            )\n",
    "                \n",
    "    # phone number\n",
    "    matches = phone_num_regex.findall(full_text)\n",
    "    if not matches:\n",
    "        continue\n",
    "    for match in matches:\n",
    "        target = [t.text for t in nlp.tokenizer(match)]\n",
    "        matched_spans = find_span(target, tokens)\n",
    "    for matched_span in matched_spans:\n",
    "        for intermediate, token_idx in enumerate(matched_span):\n",
    "            prefix = \"I\" if intermediate else \"B\"\n",
    "            phone_nums.append(\n",
    "                {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": tokens[token_idx]}\n",
    "            )\n",
    "    \n",
    "    # url\n",
    "    matches = url_regex.findall(full_text)\n",
    "    if not matches:\n",
    "        continue\n",
    "    for match in matches:\n",
    "        target = [t.text for t in nlp.tokenizer(match)]\n",
    "        matched_spans = find_span(target, tokens)\n",
    "    for matched_span in matched_spans:\n",
    "        for intermediate, token_idx in enumerate(matched_span):\n",
    "            prefix = \"I\" if intermediate else \"B\"\n",
    "            urls.append(\n",
    "                {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-URL_PERSONAL\", \"token_str\": tokens[token_idx]}\n",
    "            )\n",
    "    \n",
    "    # street\n",
    "#     matches = street_regex.findall(full_text)\n",
    "#     if not matches:\n",
    "#         continue\n",
    "#     for match in matches:\n",
    "#         target = [t.text for t in nlp.tokenizer(match)]\n",
    "#         matched_spans = find_span(target, tokens)\n",
    "#     for matched_span in matched_spans:\n",
    "#         for intermediate, token_idx in enumerate(matched_span):\n",
    "#             prefix = \"I\" if intermediate else \"B\"\n",
    "#             streets.append(\n",
    "#                 {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-STREET_ADDRESS\", \"token_str\": tokens[token_idx]}\n",
    "#             )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
