{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\User\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datasets\n",
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.lang.en import English\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ignite.metrics import Fbeta\n",
    "from functools import partial\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 labels, with the following labels:\n",
      " ['B-STREET_ADDRESS', 'I-ID_NUM', 'B-URL_PERSONAL', 'I-PHONE_NUM', 'B-ID_NUM', 'I-STREET_ADDRESS', 'B-NAME_STUDENT', 'I-URL_PERSONAL', 'I-NAME_STUDENT', 'B-EMAIL', 'B-USERNAME', 'O', 'B-PHONE_NUM']\n"
     ]
    }
   ],
   "source": [
    "#Finding out the number of labels\n",
    "data = json.load(open('data/train.json'))\n",
    "\n",
    "all_labels = set()\n",
    "\n",
    "for d in data:\n",
    "    all_labels = all_labels.union(set(d['labels']))\n",
    "\n",
    "print(f\"{len(list(all_labels))} labels, with the following labels:\\n {list(all_labels)}\")\n",
    "del data\n",
    "\n",
    "label2id = {label:index for index,label in enumerate(all_labels)}\n",
    "id2label = {index:label for index,label in enumerate(all_labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to one-hot vector\n",
    "def oh_encoder(labels):  #label: array of output for each sentence\n",
    "\n",
    "    # unique_labels = ['O', 'B-NAME_STUDENT','I-NAME_STUDENT','B-PHONE_NUM', 'I-PHONE_NUM','B-ID_NUM', 'I-ID_NUM',  'B-URL_PERSONAL','I-URL_PERSONAL',\n",
    "    #                   'B-STREET_ADDRESS', 'I-STREET_ADDRESS',  'B-EMAIL', 'B-USERNAME']\n",
    "    \n",
    "    \n",
    "    unique_labels = ['O', 'B-NAME_STUDENT','I-NAME_STUDENT','B-URL_PERSONAL', 'B-ID_NUM','I-ID_NUM','B-EMAIL','I-STREET_ADDRESS',\n",
    "                     'I-PHONE_NUM', 'B-USERNAME', 'B-PHONE_NUM','B-STREET_ADDRESS', 'I-URL_PERSONAL']\n",
    "    \n",
    "    labels_oh = []\n",
    "    for label in labels:    #label: str\n",
    "        label_oh = [float(0)]*len(unique_labels)\n",
    "        for k in range(len(unique_labels)):\n",
    "            if unique_labels[k] == label:\n",
    "                label_oh[k] = 1\n",
    "                #labels_oh.append(torch.tensor(label_oh, requires_grad=True))\n",
    "                labels_oh.append(label_oh)\n",
    "                break\n",
    "                \n",
    "\n",
    "    #return torch.tensor(labels_oh, requires_grad=True)\n",
    "    return torch.tensor(labels_oh, requires_grad=True, dtype=float)    #list of one-hot labels as tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Tokenizing sentences.\n",
    "# def tokenize(example, tokenizer, INFERENCE_MAX_LENGTH=3500):\n",
    "#     ''' \n",
    "#     Arguments:\n",
    "#     example: sentence\n",
    "#     tokenizer: following DeBERTa's\n",
    "#     INFERENCE_MAX_LENGTH: for truncation if needed\n",
    "\n",
    "#     Returns:\n",
    "#     dictionary of tokenized word id, with token_map, which maps characters to its initial idx\n",
    "#     '''\n",
    "#     text = []\n",
    "#     token_map = []\n",
    "#     labels = []\n",
    "    \n",
    "#     idx = 0\n",
    "    \n",
    "#     for t, label, ws in zip(example[\"tokens\"], \n",
    "#                      example['labels'],\n",
    "#                      example[\"trailing_whitespace\"]):\n",
    "        \n",
    "#         text.append(t)\n",
    "#         token_map.extend([idx]*len(t))\n",
    "#         labels.extend([label]*len(t))\n",
    "        \n",
    "#         if ws:\n",
    "#             text.append(\" \")\n",
    "#             token_map.append(-1)\n",
    "\n",
    "#         idx += 1\n",
    "    \n",
    "#     text = \"\".join(text)\n",
    "#     tokenized = tokenizer(text, return_offsets_mapping=True, truncation=False, max_length=INFERENCE_MAX_LENGTH, return_tensors='pt')\n",
    "\n",
    "#     labels = np.array(labels)\n",
    "\n",
    "\n",
    "    \n",
    "#     return {\n",
    "#         **tokenized,\n",
    "#         \"token_map\": token_map,\n",
    "\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer):\n",
    "    import numpy as np\n",
    "    # Preprocess the tokens and labels by adding trailing whitespace and labels\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for token, label, t_ws in zip(example[\"tokens\"], \n",
    "                                  example[\"labels\"],\n",
    "                                  example[\"trailing_whitespace\"]):\n",
    "        tokens.append(token)\n",
    "        labels.extend([label] * len(token))\n",
    "        # Added trailing whitespace and label if true and \n",
    "        if t_ws:\n",
    "            tokens.append(\" \")\n",
    "            # labels.append(oh_encoder(\"O\"))\n",
    "            labels.append(\"O\")\n",
    "    \n",
    "    text = \"\".join(tokens)\n",
    "    # print(f\"len(text)={len(text)}, len(tokens)={len(tokens)}\")\n",
    "    # tokenization without truncation\n",
    "    tokenized = tokenizer(text, return_offsets_mapping=True,\n",
    "                          truncation=False)\n",
    "    #labels = np.array(labels)\n",
    "    # Labels\n",
    "    token_labels = []\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # Added 'O' \n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            #token_labels.append(label2id[\"O\"]) \n",
    "            #token_labels.append(oh_encoder(\"O\"))\n",
    "            token_labels.append(\"O\")\n",
    "        else:\n",
    "            # case when the text starts with whitespace\n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "            # Convert label to id (int)\n",
    "            #label_id = label2id[labels[start_idx]]\n",
    "            label_id = labels[start_idx]\n",
    "            #token_labels.append(oh_encoder(label_id))\n",
    "            token_labels.append(label_id)\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": len(tokenized.input_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Their one hot encoding'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Their one hot encoding'''\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# # Eencode labels to columns\n",
    "# def encode_labels(data):\n",
    "#     df = pd.DataFrame(data)\n",
    "#     total = len(df)\n",
    "#     df[\"unique_labels\"] = df[\"labels\"].apply(lambda labels: \n",
    "#                                             list(set([label.split('-')[1] for label in labels if label != 'O'])))\n",
    "#     mlb = MultiLabelBinarizer()\n",
    "#     #one_hot_encoded = mlb.fit_transform(df['unique_labels'])\n",
    "#     one_hot_encoded = mlb.fit_transform(df['labels'])\n",
    "#     one_hot_df = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
    "#     df = pd.concat([df, one_hot_df], axis=1)\n",
    "#     # add 'POS' column that don't have \n",
    "#     df['others'] = df['unique_labels'].apply(lambda x: 1 if len(x) == 0 else 0)\n",
    "#     label_classes = list(mlb.classes_) + ['others']\n",
    "#     for col in label_classes:\n",
    "#         subtotal = df[col].sum()\n",
    "#         percent = subtotal/total * 100\n",
    "#         print(f'{col}: {subtotal}  ({percent:.1f}%)')\n",
    "#     return df, label_classes\n",
    "\n",
    "# data = json.load(open('data/train.json'))\n",
    "# df , label_classes = encode_labels(data)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "726 726\n",
      "[CLS] --> O\n",
      "▁Design --> O\n",
      "▁Thinking --> O\n",
      "▁for --> O\n",
      "▁innovation --> O\n",
      "▁reflex --> O\n",
      "ion --> O\n",
      "- --> O\n",
      "Av --> O\n",
      "ril --> O\n",
      "▁2021 --> O\n",
      "- --> O\n",
      "N --> B-NAME_STUDENT\n",
      "atha --> B-NAME_STUDENT\n",
      "lie --> B-NAME_STUDENT\n",
      "▁S --> I-NAME_STUDENT\n",
      "ylla --> I-NAME_STUDENT\n",
      "▁Challenge --> O\n",
      "▁& --> O\n",
      "▁selection --> O\n",
      "▁The --> O\n",
      "▁tool --> O\n",
      "▁I --> O\n",
      "▁use --> O\n",
      "▁to --> O\n",
      "▁help --> O\n",
      "▁all --> O\n",
      "▁stakeholders --> O\n",
      "▁finding --> O\n",
      "▁their --> O\n",
      "▁way --> O\n",
      "▁through --> O\n",
      "▁the --> O\n",
      "▁complexity --> O\n",
      "▁of --> O\n",
      "▁a --> O\n",
      "▁project --> O\n",
      "▁is --> O\n",
      "▁the --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      ". --> O\n",
      "▁What --> O\n",
      "▁exactly --> O\n",
      "▁is --> O\n",
      "▁a --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      "? --> O\n",
      "▁According --> O\n",
      "▁to --> O\n",
      "▁the --> O\n",
      "▁definition --> O\n",
      "▁of --> O\n",
      "▁Buz --> O\n",
      "an --> O\n",
      "▁T --> O\n",
      ". --> O\n",
      "▁and --> O\n",
      "▁Buz --> O\n",
      "an --> O\n",
      "▁B --> O\n",
      ". --> O\n",
      "▁( --> O\n",
      "1999 --> O\n",
      ", --> O\n",
      "▁Des --> O\n",
      "s --> O\n",
      "ine --> O\n",
      "- --> O\n",
      "moi --> O\n",
      "▁l --> O\n",
      "' --> O\n",
      "intelligence --> O\n",
      ". --> O\n",
      "▁Paris --> O\n",
      ": --> O\n",
      "▁Les --> O\n",
      "▁É --> O\n",
      "dition --> O\n",
      "s --> O\n",
      "▁d --> O\n",
      "' --> O\n",
      "Organ --> O\n",
      "isation --> O\n",
      ". --> O\n",
      ") --> O\n",
      ", --> O\n",
      "▁the --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      "▁( --> O\n",
      "or --> O\n",
      "▁heuristic --> O\n",
      "▁diagram --> O\n",
      ") --> O\n",
      "▁is --> O\n",
      "▁a --> O\n",
      "▁graphic --> O\n",
      "▁representation --> O\n",
      "▁technique --> O\n",
      "▁that --> O\n",
      "▁follows --> O\n",
      "▁the --> O\n",
      "▁natural --> O\n",
      "▁functioning --> O\n",
      "▁of --> O\n",
      "▁the --> O\n",
      "▁mind --> O\n",
      "▁and --> O\n",
      "▁allows --> O\n",
      "▁the --> O\n",
      "▁brain --> O\n",
      "' --> O\n",
      "s --> O\n",
      "▁potential --> O\n",
      "▁to --> O\n",
      "▁be --> O\n",
      "▁released --> O\n",
      ". --> O\n",
      "▁Cf --> O\n",
      "▁Annex --> O\n",
      "1 --> O\n",
      "▁This --> O\n",
      "▁tool --> O\n",
      "▁has --> O\n",
      "▁many --> O\n",
      "▁advantages --> O\n",
      ": --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁is --> O\n",
      "▁accessible --> O\n",
      "▁to --> O\n",
      "▁all --> O\n",
      "▁and --> O\n",
      "▁does --> O\n",
      "▁not --> O\n",
      "▁require --> O\n",
      "▁significant --> O\n",
      "▁material --> O\n",
      "▁investment --> O\n",
      "▁and --> O\n",
      "▁can --> O\n",
      "▁be --> O\n",
      "▁done --> O\n",
      "▁quickly --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁is --> O\n",
      "▁scalable --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁allows --> O\n",
      "▁categorization --> O\n",
      "▁and --> O\n",
      "▁linking --> O\n",
      "▁of --> O\n",
      "▁information --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁can --> O\n",
      "▁be --> O\n",
      "▁applied --> O\n",
      "▁to --> O\n",
      "▁any --> O\n",
      "▁type --> O\n",
      "▁of --> O\n",
      "▁situation --> O\n",
      ": --> O\n",
      "▁note --> O\n",
      "taking --> O\n",
      ", --> O\n",
      "▁problem --> O\n",
      "▁solving --> O\n",
      ", --> O\n",
      "▁analysis --> O\n",
      ", --> O\n",
      "▁creation --> O\n",
      "▁of --> O\n",
      "▁new --> O\n",
      "▁ideas --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁is --> O\n",
      "▁suitable --> O\n",
      "▁for --> O\n",
      "▁all --> O\n",
      "▁people --> O\n",
      "▁and --> O\n",
      "▁is --> O\n",
      "▁easy --> O\n",
      "▁to --> O\n",
      "▁learn --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁is --> O\n",
      "▁fun --> O\n",
      "▁and --> O\n",
      "▁encourages --> O\n",
      "▁exchanges --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁makes --> O\n",
      "▁visible --> O\n",
      "▁the --> O\n",
      "▁dimension --> O\n",
      "▁of --> O\n",
      "▁projects --> O\n",
      ", --> O\n",
      "▁opportunities --> O\n",
      ", --> O\n",
      "▁interconnections --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁synthesize --> O\n",
      "s --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁makes --> O\n",
      "▁the --> O\n",
      "▁project --> O\n",
      "▁understandable --> O\n",
      "▁• --> O\n",
      "▁It --> O\n",
      "▁allows --> O\n",
      "▁you --> O\n",
      "▁to --> O\n",
      "▁explore --> O\n",
      "▁ideas --> O\n",
      "▁The --> O\n",
      "▁creation --> O\n",
      "▁of --> O\n",
      "▁a --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      "▁starts --> O\n",
      "▁with --> O\n",
      "▁an --> O\n",
      "▁idea --> O\n",
      "/ --> O\n",
      "problem --> O\n",
      "▁located --> O\n",
      "▁at --> O\n",
      "▁its --> O\n",
      "▁center --> O\n",
      ". --> O\n",
      "▁This --> O\n",
      "▁starting --> O\n",
      "▁point --> O\n",
      "▁generates --> O\n",
      "▁ideas --> O\n",
      "/ --> O\n",
      "work --> O\n",
      "▁areas --> O\n",
      ", --> O\n",
      "▁incremented --> O\n",
      "▁around --> O\n",
      "▁this --> O\n",
      "▁center --> O\n",
      "▁in --> O\n",
      "▁a --> O\n",
      "▁radial --> O\n",
      "▁structure --> O\n",
      ", --> O\n",
      "▁which --> O\n",
      "▁in --> O\n",
      "▁turn --> O\n",
      "▁is --> O\n",
      "▁completed --> O\n",
      "▁with --> O\n",
      "▁as --> O\n",
      "▁many --> O\n",
      "▁branches --> O\n",
      "▁as --> O\n",
      "▁new --> O\n",
      "▁ideas --> O\n",
      ". --> O\n",
      "▁This --> O\n",
      "▁tool --> O\n",
      "▁enables --> O\n",
      "▁creativity --> O\n",
      "▁and --> O\n",
      "▁logic --> O\n",
      "▁to --> O\n",
      "▁be --> O\n",
      "▁mobilized --> O\n",
      ", --> O\n",
      "▁it --> O\n",
      "▁is --> O\n",
      "▁a --> O\n",
      "▁map --> O\n",
      "▁of --> O\n",
      "▁the --> O\n",
      "▁thoughts --> O\n",
      ". --> O\n",
      "▁Creativity --> O\n",
      "▁is --> O\n",
      "▁enhanced --> O\n",
      "▁because --> O\n",
      "▁participants --> O\n",
      "▁feel --> O\n",
      "▁comfortable --> O\n",
      "▁with --> O\n",
      "▁the --> O\n",
      "▁method --> O\n",
      ". --> O\n",
      "▁Application --> O\n",
      "▁& --> O\n",
      "▁Insight --> O\n",
      "▁I --> O\n",
      "▁start --> O\n",
      "▁the --> O\n",
      "▁process --> O\n",
      "▁of --> O\n",
      "▁the --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      "▁creation --> O\n",
      "▁with --> O\n",
      "▁the --> O\n",
      "▁stakeholders --> O\n",
      "▁standing --> O\n",
      "▁around --> O\n",
      "▁a --> O\n",
      "▁large --> O\n",
      "▁board --> O\n",
      "▁( --> O\n",
      "white --> O\n",
      "▁or --> O\n",
      "▁paper --> O\n",
      "▁board --> O\n",
      ") --> O\n",
      ". --> O\n",
      "▁In --> O\n",
      "▁the --> O\n",
      "▁center --> O\n",
      "▁of --> O\n",
      "▁the --> O\n",
      "▁board --> O\n",
      ", --> O\n",
      "▁I --> O\n",
      "▁write --> O\n",
      "▁and --> O\n",
      "▁highlight --> O\n",
      "▁the --> O\n",
      "▁topic --> O\n",
      "▁to --> O\n",
      "▁design --> O\n",
      ". --> O\n",
      "▁Through --> O\n",
      "▁a --> O\n",
      "▁series --> O\n",
      "▁of --> O\n",
      "▁questions --> O\n",
      ", --> O\n",
      "▁I --> O\n",
      "▁guide --> O\n",
      "▁the --> O\n",
      "▁stakeholders --> O\n",
      "▁in --> O\n",
      "▁modelling --> O\n",
      "▁the --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      ". --> O\n",
      "▁I --> O\n",
      "▁adapt --> O\n",
      "▁the --> O\n",
      "▁series --> O\n",
      "▁of --> O\n",
      "▁questions --> O\n",
      "▁according --> O\n",
      "▁to --> O\n",
      "▁the --> O\n",
      "▁topic --> O\n",
      "▁to --> O\n",
      "▁be --> O\n",
      "▁addressed --> O\n",
      ". --> O\n",
      "▁In --> O\n",
      "▁the --> O\n",
      "▁type --> O\n",
      "▁of --> O\n",
      "▁questions --> O\n",
      ", --> O\n",
      "▁we --> O\n",
      "▁can --> O\n",
      "▁use --> O\n",
      ": --> O\n",
      "▁who --> O\n",
      ", --> O\n",
      "▁what --> O\n",
      ", --> O\n",
      "▁when --> O\n",
      ", --> O\n",
      "▁where --> O\n",
      ", --> O\n",
      "▁why --> O\n",
      ", --> O\n",
      "▁how --> O\n",
      ", --> O\n",
      "▁how --> O\n",
      "▁much --> O\n",
      ". --> O\n",
      "▁The --> O\n",
      "▁use --> O\n",
      "▁of --> O\n",
      "▁the --> O\n",
      "▁“ --> O\n",
      "why --> O\n",
      "” --> O\n",
      "▁is --> O\n",
      "▁very --> O\n",
      "▁interesting --> O\n",
      "▁to --> O\n",
      "▁understand --> O\n",
      "▁the --> O\n",
      "▁origin --> O\n",
      ". --> O\n",
      "▁By --> O\n",
      "▁this --> O\n",
      "▁way --> O\n",
      ", --> O\n",
      "▁the --> O\n",
      "▁interviewed --> O\n",
      "▁person --> O\n",
      "▁free --> O\n",
      "s --> O\n",
      "▁itself --> O\n",
      "▁from --> O\n",
      "▁paradigms --> O\n",
      "▁and --> O\n",
      "▁thus --> O\n",
      "▁dares --> O\n",
      "▁to --> O\n",
      "▁propose --> O\n",
      "▁new --> O\n",
      "▁ideas --> O\n",
      "▁/ --> O\n",
      "▁ways --> O\n",
      "▁of --> O\n",
      "▁functioning --> O\n",
      ". --> O\n",
      "▁I --> O\n",
      "▁plan --> O\n",
      "▁two --> O\n",
      "▁hours --> O\n",
      "▁for --> O\n",
      "▁a --> O\n",
      "▁workshop --> O\n",
      ". --> O\n",
      "▁Design --> O\n",
      "▁Thinking --> O\n",
      "▁for --> O\n",
      "▁innovation --> O\n",
      "▁reflex --> O\n",
      "ion --> O\n",
      "- --> O\n",
      "Av --> O\n",
      "ril --> O\n",
      "▁2021 --> O\n",
      "- --> O\n",
      "N --> B-NAME_STUDENT\n",
      "atha --> B-NAME_STUDENT\n",
      "lie --> B-NAME_STUDENT\n",
      "▁S --> I-NAME_STUDENT\n",
      "ylla --> I-NAME_STUDENT\n",
      "▁After --> O\n",
      "▁modelling --> O\n",
      "▁the --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      "▁on --> O\n",
      "▁paper --> O\n",
      ", --> O\n",
      "▁I --> O\n",
      "▁propose --> O\n",
      "▁to --> O\n",
      "▁the --> O\n",
      "▁participants --> O\n",
      "▁a --> O\n",
      "▁digital --> O\n",
      "▁visualization --> O\n",
      "▁of --> O\n",
      "▁their --> O\n",
      "▁work --> O\n",
      "▁with --> O\n",
      "▁the --> O\n",
      "▁addition --> O\n",
      "▁of --> O\n",
      "▁color --> O\n",
      "▁codes --> O\n",
      ", --> O\n",
      "▁images --> O\n",
      "▁and --> O\n",
      "▁interconnections --> O\n",
      ". --> O\n",
      "▁This --> O\n",
      "▁second --> O\n",
      "▁workshop --> O\n",
      "▁also --> O\n",
      "▁last --> O\n",
      "s --> O\n",
      "▁two --> O\n",
      "▁hours --> O\n",
      "▁and --> O\n",
      "▁allows --> O\n",
      "▁the --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      "▁to --> O\n",
      "▁evolve --> O\n",
      ". --> O\n",
      "▁Once --> O\n",
      "▁familiarized --> O\n",
      "▁with --> O\n",
      "▁it --> O\n",
      ", --> O\n",
      "▁the --> O\n",
      "▁stakeholders --> O\n",
      "▁discover --> O\n",
      "▁the --> O\n",
      "▁power --> O\n",
      "▁of --> O\n",
      "▁the --> O\n",
      "▁tool --> O\n",
      ". --> O\n",
      "▁Then --> O\n",
      ", --> O\n",
      "▁the --> O\n",
      "▁second --> O\n",
      "▁workshop --> O\n",
      "▁brings --> O\n",
      "▁out --> O\n",
      "▁even --> O\n",
      "▁more --> O\n",
      "▁ideas --> O\n",
      "▁and --> O\n",
      "▁constructive --> O\n",
      "▁exchanges --> O\n",
      "▁between --> O\n",
      "▁the --> O\n",
      "▁stakeholders --> O\n",
      ". --> O\n",
      "▁Around --> O\n",
      "▁this --> O\n",
      "▁new --> O\n",
      "▁mind --> O\n",
      "▁map --> O\n",
      ", --> O\n",
      "▁they --> O\n",
      "▁have --> O\n",
      "▁learned --> O\n",
      "▁to --> O\n",
      "▁work --> O\n",
      "▁together --> O\n",
      "▁and --> O\n",
      "▁want --> O\n",
      "▁to --> O\n",
      "▁make --> O\n",
      "▁visible --> O\n",
      "▁the --> O\n",
      "▁untold --> O\n",
      "▁ideas --> O\n",
      ". --> O\n",
      "▁I --> O\n",
      "▁now --> O\n",
      "▁present --> O\n",
      "▁all --> O\n",
      "▁the --> O\n",
      "▁projects --> O\n",
      "▁I --> O\n",
      "▁manage --> O\n",
      "▁in --> O\n",
      "▁this --> O\n",
      "▁type --> O\n",
      "▁of --> O\n",
      "▁format --> O\n",
      "▁in --> O\n",
      "▁order --> O\n",
      "▁to --> O\n",
      "▁ease --> O\n",
      "▁rapid --> O\n",
      "▁understanding --> O\n",
      "▁for --> O\n",
      "▁decision --> O\n",
      "- --> O\n",
      "makers --> O\n",
      ". --> O\n",
      "▁These --> O\n",
      "▁presentations --> O\n",
      "▁are --> O\n",
      "▁the --> O\n",
      "▁core --> O\n",
      "▁of --> O\n",
      "▁my --> O\n",
      "▁business --> O\n",
      "▁models --> O\n",
      ". --> O\n",
      "▁The --> O\n",
      "▁decision --> O\n",
      "- --> O\n",
      "makers --> O\n",
      "▁are --> O\n",
      "▁thus --> O\n",
      "▁able --> O\n",
      "▁to --> O\n",
      "▁identify --> O\n",
      "▁the --> O\n",
      "▁opportunities --> O\n",
      "▁of --> O\n",
      "▁the --> O\n",
      "▁projects --> O\n",
      "▁and --> O\n",
      "▁can --> O\n",
      "▁take --> O\n",
      "▁quick --> O\n",
      "▁decisions --> O\n",
      "▁to --> O\n",
      "▁validate --> O\n",
      "▁them --> O\n",
      ". --> O\n",
      "▁They --> O\n",
      "▁find --> O\n",
      "▁answers --> O\n",
      "▁to --> O\n",
      "▁their --> O\n",
      "▁questions --> O\n",
      "▁thank --> O\n",
      "▁to --> O\n",
      "▁a --> O\n",
      "▁schematic --> O\n",
      "▁representation --> O\n",
      ". --> O\n",
      "▁Approach --> O\n",
      "▁What --> O\n",
      "▁I --> O\n",
      "▁find --> O\n",
      "▁amazing --> O\n",
      "▁with --> O\n",
      "▁the --> O\n",
      "▁facilitation --> O\n",
      "▁of --> O\n",
      "▁this --> O\n",
      "▁type --> O\n",
      "▁of --> O\n",
      "▁workshop --> O\n",
      "▁is --> O\n",
      "▁the --> O\n",
      "▁participants --> O\n",
      "▁commitment --> O\n",
      "▁for --> O\n",
      "▁the --> O\n",
      "▁project --> O\n",
      ". --> O\n",
      "▁This --> O\n",
      "▁tool --> O\n",
      "▁helps --> O\n",
      "▁to --> O\n",
      "▁give --> O\n",
      "▁meaning --> O\n",
      ". --> O\n",
      "▁The --> O\n",
      "▁participants --> O\n",
      "▁appropriate --> O\n",
      "▁the --> O\n",
      "▁story --> O\n",
      "▁and --> O\n",
      "▁want --> O\n",
      "▁to --> O\n",
      "▁keep --> O\n",
      "▁writing --> O\n",
      "▁it --> O\n",
      ". --> O\n",
      "▁Then --> O\n",
      ", --> O\n",
      "▁they --> O\n",
      "▁easily --> O\n",
      "▁become --> O\n",
      "▁actors --> O\n",
      "▁or --> O\n",
      "▁sponsors --> O\n",
      "▁of --> O\n",
      "▁the --> O\n",
      "▁project --> O\n",
      ". --> O\n",
      "▁A --> O\n",
      "▁trust --> O\n",
      "▁relationship --> O\n",
      "▁is --> O\n",
      "▁built --> O\n",
      ", --> O\n",
      "▁thus --> O\n",
      "▁facilitating --> O\n",
      "▁the --> O\n",
      "▁implementation --> O\n",
      "▁of --> O\n",
      "▁related --> O\n",
      "▁actions --> O\n",
      ". --> O\n",
      "▁Design --> O\n",
      "▁Thinking --> O\n",
      "▁for --> O\n",
      "▁innovation --> O\n",
      "▁reflex --> O\n",
      "ion --> O\n",
      "- --> O\n",
      "Av --> O\n",
      "ril --> O\n",
      "▁2021 --> O\n",
      "- --> O\n",
      "N --> B-NAME_STUDENT\n",
      "atha --> B-NAME_STUDENT\n",
      "lie --> B-NAME_STUDENT\n",
      "▁S --> I-NAME_STUDENT\n",
      "ylla --> I-NAME_STUDENT\n",
      "▁Annex --> O\n",
      "▁1 --> O\n",
      ": --> O\n",
      "▁Mind --> O\n",
      "▁Map --> O\n",
      "▁Shared --> O\n",
      "▁facilities --> O\n",
      "▁project --> O\n",
      "[SEP] --> O\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open('data/train.json'))\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "d = data[0]\n",
    "input_ids = tokenize(d, tokenizer)\n",
    "tensor = torch.tensor(input_ids['input_ids']) \n",
    "tokens = tokenizer.convert_ids_to_tokens(tensor)\n",
    "\n",
    "# print(tokens)\n",
    "# print(input_ids['labels'])\n",
    "# print(d['tokens'])\n",
    "print(len(tokens),len(input_ids['labels']))\n",
    "\n",
    "for id, label in zip(tokens, input_ids['labels']):\n",
    "    print(id, '-->', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: DeBERTa\n",
    "\n",
    "Using a pretrained DeBERTa, we will build a classifier head on top of it to predict the class at token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Classifier(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim = 13):\n",
    "#         super(Classifier, self).__init__()\n",
    "\n",
    "#         self.dropout_prob = 0.3\n",
    "#         self.final_activation = nn.Softmax(dim = -1)\n",
    "\n",
    "#         self.linear = nn.Sequential(\n",
    "#             nn.ReLU(nn.Linear(input_dim, hidden_dim)),\n",
    "            \n",
    "#             nn.Dropout(self.dropout_prob),\n",
    "#             nn.ReLU(nn.Linear(hidden_dim, hidden_dim*2)),\n",
    "\n",
    "#             nn.Dropout(self.dropout_prob),\n",
    "#             nn.ReLU(nn.Linear(hidden_dim*2, hidden_dim)),\n",
    "\n",
    "#             nn.Linear(hidden_dim, output_dim)\n",
    "#         )\n",
    "       \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         logit = self.linear(x)\n",
    "#         return self.final_activation(logit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Classifier(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim = 13):\n",
    "#         super(Classifier, self).__init__()\n",
    "\n",
    "#         self.dropout_prob = 0.3\n",
    "#         self.final_activation = nn.Softmax(dim = -1)\n",
    "\n",
    "#         self.dropout = nn.Dropout(self.dropout_prob)\n",
    "#         self.linear1 = nn.ReLU(nn.Linear(input_dim,hidden_dim))\n",
    "#         self.linear2 = nn.ReLU(nn.Linear(hidden_dim, hidden_dim*2))                            \n",
    "#         self.linear3 = nn.ReLU(nn.Linear(hidden_dim*2, hidden_dim))\n",
    "#         self.output = nn.Linear(hidden_dim,output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         print('now start linear1')\n",
    "#         print(x.size())\n",
    "#         x = self.linear1(x)\n",
    "\n",
    "#         print('now start linear2')\n",
    "#         print(x.size())\n",
    "#         x = self.linear2(self.dropout(x))\n",
    "\n",
    "#         print('now start linear3')\n",
    "#         print(x.size())\n",
    "#         x = self.linear3(self.dropout(x))\n",
    "\n",
    "#         print('now start output')\n",
    "#         print(x.size())\n",
    "#         logit= self.output(x)\n",
    "        \n",
    "#         print('now start activation')\n",
    "#         output =  self.final_activation(logit)\n",
    "        \n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Deberta_Classif(Classifier):\n",
    "#     def __init__(self, model_name, classif_input, classif_hidden, classif_output = 13, finetune = False):\n",
    "#         super(Deberta_Classif, self).__init__(classif_input, classif_input)\n",
    "#         self.ft = finetune\n",
    "\n",
    "#         self.extractor = AutoModelForTokenClassification.from_pretrained(model_name).base_model\n",
    "\n",
    "#         if not finetune:\n",
    "#             for param in self.extractor.parameters():\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "#         self.extractor_num_param =  sum(p.numel() for p in self.extractor.parameters())\n",
    "#         self.extractor_num_param_grad = sum(p.numel() for p in self.extractor.parameters() if p.requires_grad)\n",
    "#         self.extractor_name = \"DeBERTa\"\n",
    "                \n",
    "\n",
    "#         self.classifier = Classifier(input_dim=classif_input, hidden_dim=classif_hidden ,output_dim=classif_output)\n",
    "#         self.classifier_num_param = sum(p.numel() for p in self.classifier.parameters() if p.requires_grad)\n",
    "\n",
    "        \n",
    "#     def count_param(self):\n",
    "        \n",
    "#         if self.ft:\n",
    "#             type = 'finetuned'\n",
    "#             num_param = self.extractor_num_param_grad + self.classifier_num_param\n",
    "#         else:\n",
    "#             type = 'non-finetuned'\n",
    "#             num_param = self.extractor_num_param + self.classifier_num_param\n",
    "\n",
    "#         print(f\"Number of parameters in {type} {self.extractor_name} model is {num_param:,}\")\n",
    "    \n",
    "    \n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         x = self.extractor(input_ids, attention_mask).last_hidden_state\n",
    "#         x = self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([13, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's trainable parameters:  9,997\n",
      "Model's total parameters:  183,841,549\n"
     ]
    }
   ],
   "source": [
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "#model = Deberta_Classif(model_name, classif_input = 768, classif_hidden = 100, classif_output = 13, finetune=False)  #Extractor output has dim 768 \n",
    "config = AutoModelForTokenClassification.from_pretrained(model_name).config\n",
    "\n",
    "config.update({\n",
    "            'num_labels': 13,\n",
    "            'ignore_mismatched_sizes': True,\n",
    "        })\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, config = config, ignore_mismatched_sizes=True)\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Model's trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad): ,}\")\n",
    "print(f\"Model's total parameters: {sum(p.numel() for p in model.parameters()): ,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' \n",
    "model = model.to(device)\n",
    "epochs = 5\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of = 32, max_length=3500)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "f5 = Fbeta(beta=5).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=3): 100%|██████████| 5785/5785 [00:10<00:00, 549.59 examples/s] \n",
      "Map (num_proc=3): 100%|██████████| 1022/1022 [00:03<00:00, 283.52 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#Preparing the datasets for token classification\n",
    "data = json.load(open('data/train.json'))\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.15, random_state=42)  \n",
    "# train_labels = [oh_encoder(x['labels']) for x in train_data]\n",
    "\n",
    "trainset = datasets.Dataset.from_dict({\n",
    "    'full_text': [x['full_text'] for x in train_data],\n",
    "    'document': [x['document'] for x in train_data],\n",
    "    'tokens': [x['tokens'] for x in train_data],\n",
    "    'trailing_whitespace': [x['trailing_whitespace'] for x in train_data],\n",
    "    'labels' :[x['labels'] for x in train_data]\n",
    "})\n",
    "\n",
    "trainset = trainset.map(tokenize, fn_kwargs = {\"tokenizer\": tokenizer}, num_proc=3)\n",
    "#trainset['labels'] = oh_encoder(trainset['labels'])\n",
    "#train_labels = [oh_encoder(x['labels'] for x in train_data)]\n",
    "\n",
    "\n",
    "# val_labels = [oh_encoder(x['labels']) for x in val_data]\n",
    "\n",
    "valset = datasets.Dataset.from_dict({\n",
    "    'full_text': [x['full_text'] for x in val_data],\n",
    "    'document': [x['document'] for x in val_data],\n",
    "    'tokens': [x['tokens'] for x in val_data],\n",
    "    'trailing_whitespace': [x['trailing_whitespace'] for x in val_data],\n",
    "    'labels' :[x['labels'] for x in val_data]\n",
    "})\n",
    "valset = valset.map(tokenize, fn_kwargs = {\"tokenizer\": tokenizer}, num_proc=3)\n",
    "#valset['labels'] = oh_encoder(valset['labels'])\n",
    "\n",
    "\n",
    "#val_labels = [oh_encoder(x['labels'] for x in val_data)]\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data: 5785 || Number of validation data: 1022\n"
     ]
    }
   ],
   "source": [
    "#First item\n",
    "print(f\"Number of training data: {len(trainset)} || Number of validation data: {len(valset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict(data):\n",
    "    dict_of_lists = {}\n",
    "    for d in data:\n",
    "        for key, value in d.items():\n",
    "            if key in dict_of_lists:\n",
    "                dict_of_lists[key].append(value)\n",
    "            else:\n",
    "                dict_of_lists[key] = [value]\n",
    "    return dict_of_lists\n",
    "\n",
    "trainset = to_dict(trainset)\n",
    "#trainset['labels'] = train_labels\n",
    "\n",
    "valset = to_dict(valset)\n",
    "#valset['labels'] = val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19404,\n",
       " 9815,\n",
       " 13931,\n",
       " 16803,\n",
       " 16279,\n",
       " 15120,\n",
       " 8819,\n",
       " 1239,\n",
       " 17101,\n",
       " 14770,\n",
       " 18714,\n",
       " 22062,\n",
       " 13496,\n",
       " 16934,\n",
       " 13957,\n",
       " 21531,\n",
       " 20307,\n",
       " 16904,\n",
       " 7124,\n",
       " 19786,\n",
       " 9236,\n",
       " 6954,\n",
       " 21784,\n",
       " 7271,\n",
       " 11868,\n",
       " 7250,\n",
       " 10778,\n",
       " 22034,\n",
       " 22355,\n",
       " 14650,\n",
       " 16825,\n",
       " 18359,\n",
       " 19856,\n",
       " 9746,\n",
       " 8880,\n",
       " 11451,\n",
       " 4698,\n",
       " 16351,\n",
       " 14125,\n",
       " 1353,\n",
       " 15319,\n",
       " 11738,\n",
       " 10476,\n",
       " 20798,\n",
       " 18084,\n",
       " 21140,\n",
       " 14892,\n",
       " 22394,\n",
       " 11962,\n",
       " 20325,\n",
       " 19373,\n",
       " 21337,\n",
       " 18865,\n",
       " 13845,\n",
       " 20946,\n",
       " 9038,\n",
       " 19823,\n",
       " 11063,\n",
       " 17521,\n",
       " 19227,\n",
       " 8393,\n",
       " 20819,\n",
       " 8744,\n",
       " 14798,\n",
       " 11914,\n",
       " 8908,\n",
       " 20616,\n",
       " 9080,\n",
       " 20932,\n",
       " 16741,\n",
       " 6002,\n",
       " 5613,\n",
       " 21580,\n",
       " 13347,\n",
       " 20577,\n",
       " 16165,\n",
       " 22659,\n",
       " 14656,\n",
       " 19838,\n",
       " 9361,\n",
       " 22378,\n",
       " 20424,\n",
       " 12859,\n",
       " 17359,\n",
       " 16415,\n",
       " 17968,\n",
       " 6497,\n",
       " 14899,\n",
       " 18955,\n",
       " 10592,\n",
       " 10356,\n",
       " 15948,\n",
       " 21984,\n",
       " 20829,\n",
       " 11000,\n",
       " 21280,\n",
       " 10880,\n",
       " 15139,\n",
       " 10188,\n",
       " 11392,\n",
       " 3540,\n",
       " 7795,\n",
       " 11487,\n",
       " 10044,\n",
       " 16195,\n",
       " 17905,\n",
       " 17326,\n",
       " 9520,\n",
       " 15029,\n",
       " 20890,\n",
       " 7293,\n",
       " 16277,\n",
       " 10345,\n",
       " 18759,\n",
       " 13133,\n",
       " 15839,\n",
       " 21258,\n",
       " 13353,\n",
       " 14947,\n",
       " 20969,\n",
       " 13793,\n",
       " 18414,\n",
       " 5484,\n",
       " 16613,\n",
       " 16690,\n",
       " 16903,\n",
       " 20675,\n",
       " 15556,\n",
       " 9848,\n",
       " 12866,\n",
       " 12007,\n",
       " 11203,\n",
       " 8766,\n",
       " 1578,\n",
       " 20120,\n",
       " 9195,\n",
       " 16295,\n",
       " 17077,\n",
       " 19530,\n",
       " 16087,\n",
       " 15680,\n",
       " 12703,\n",
       " 12516,\n",
       " 21440,\n",
       " 16148,\n",
       " 14678,\n",
       " 21438,\n",
       " 12079,\n",
       " 7037,\n",
       " 22251,\n",
       " 8672,\n",
       " 12043,\n",
       " 18133,\n",
       " 19162,\n",
       " 10290,\n",
       " 22448,\n",
       " 14726,\n",
       " 13253,\n",
       " 12742,\n",
       " 10717,\n",
       " 21056,\n",
       " 10505,\n",
       " 4328,\n",
       " 14547,\n",
       " 9213,\n",
       " 11414,\n",
       " 18242,\n",
       " 14945,\n",
       " 22183,\n",
       " 21971,\n",
       " 18522,\n",
       " 8447,\n",
       " 19349,\n",
       " 9047,\n",
       " 12936,\n",
       " 17887,\n",
       " 20362,\n",
       " 19539,\n",
       " 21807,\n",
       " 9469,\n",
       " 15129,\n",
       " 20445,\n",
       " 14782,\n",
       " 14988,\n",
       " 15327,\n",
       " 19855,\n",
       " 19075,\n",
       " 21080,\n",
       " 9501,\n",
       " 14477,\n",
       " 16117,\n",
       " 288,\n",
       " 21261,\n",
       " 5896,\n",
       " 18561,\n",
       " 18320,\n",
       " 9986,\n",
       " 10303,\n",
       " 16885,\n",
       " 12501,\n",
       " 12959,\n",
       " 10088,\n",
       " 10153,\n",
       " 22282,\n",
       " 15472,\n",
       " 19387,\n",
       " 20172,\n",
       " 22056,\n",
       " 12125,\n",
       " 18263,\n",
       " 12534,\n",
       " 22184,\n",
       " 21497,\n",
       " 18991,\n",
       " 20431,\n",
       " 12041,\n",
       " 12385,\n",
       " 19760,\n",
       " 22629,\n",
       " 20658,\n",
       " 9708,\n",
       " 17187,\n",
       " 15454,\n",
       " 16508,\n",
       " 21798,\n",
       " 13042,\n",
       " 9665,\n",
       " 15357,\n",
       " 12040,\n",
       " 9009,\n",
       " 8480,\n",
       " 18068,\n",
       " 20390,\n",
       " 8936,\n",
       " 671,\n",
       " 14969,\n",
       " 1221,\n",
       " 14252,\n",
       " 11988,\n",
       " 9569,\n",
       " 16491,\n",
       " 13877,\n",
       " 18390,\n",
       " 21836,\n",
       " 20454,\n",
       " 14544,\n",
       " 20967,\n",
       " 10363,\n",
       " 18881,\n",
       " 11991,\n",
       " 472,\n",
       " 14701,\n",
       " 1763,\n",
       " 9737,\n",
       " 15131,\n",
       " 14792,\n",
       " 4855,\n",
       " 18553,\n",
       " 21940,\n",
       " 19587,\n",
       " 10804,\n",
       " 13597,\n",
       " 9581,\n",
       " 17384,\n",
       " 18774,\n",
       " 10961,\n",
       " 12063,\n",
       " 10970,\n",
       " 12480,\n",
       " 12127,\n",
       " 11437,\n",
       " 11863,\n",
       " 13475,\n",
       " 11542,\n",
       " 21065,\n",
       " 20085,\n",
       " 5167,\n",
       " 11108,\n",
       " 20691,\n",
       " 12325,\n",
       " 17952,\n",
       " 14011,\n",
       " 20182,\n",
       " 12972,\n",
       " 17310,\n",
       " 17865,\n",
       " 19098,\n",
       " 18741,\n",
       " 15464,\n",
       " 19401,\n",
       " 8909,\n",
       " 14699,\n",
       " 10403,\n",
       " 5671,\n",
       " 5736,\n",
       " 16818,\n",
       " 14064,\n",
       " 16003,\n",
       " 15708,\n",
       " 13041,\n",
       " 8942,\n",
       " 21346,\n",
       " 18712,\n",
       " 15991,\n",
       " 5023,\n",
       " 9190,\n",
       " 9679,\n",
       " 15785,\n",
       " 9590,\n",
       " 9245,\n",
       " 12519,\n",
       " 10763,\n",
       " 5225,\n",
       " 9425,\n",
       " 17022,\n",
       " 13981,\n",
       " 19658,\n",
       " 14715,\n",
       " 15757,\n",
       " 21964,\n",
       " 11508,\n",
       " 13362,\n",
       " 11334,\n",
       " 11357,\n",
       " 11878,\n",
       " 14259,\n",
       " 12290,\n",
       " 18708,\n",
       " 21917,\n",
       " 13557,\n",
       " 12479,\n",
       " 15121,\n",
       " 18478,\n",
       " 20312,\n",
       " 19320,\n",
       " 21423,\n",
       " 10769,\n",
       " 12172,\n",
       " 17132,\n",
       " 12653,\n",
       " 9833,\n",
       " 18104,\n",
       " 3565,\n",
       " 14121,\n",
       " 11235,\n",
       " 16614,\n",
       " 11125,\n",
       " 14664,\n",
       " 13663,\n",
       " 21233,\n",
       " 18266,\n",
       " 8816,\n",
       " 2061,\n",
       " 10331,\n",
       " 16948,\n",
       " 21461,\n",
       " 18525,\n",
       " 15642,\n",
       " 17881,\n",
       " 3915,\n",
       " 13330,\n",
       " 18751,\n",
       " 7490,\n",
       " 21156,\n",
       " 11149,\n",
       " 12587,\n",
       " 9243,\n",
       " 21367,\n",
       " 21281,\n",
       " 9883,\n",
       " 9247,\n",
       " 9906,\n",
       " 9307,\n",
       " 15833,\n",
       " 19273,\n",
       " 22051,\n",
       " 17090,\n",
       " 10140,\n",
       " 10119,\n",
       " 10747,\n",
       " 11272,\n",
       " 16802,\n",
       " 11922,\n",
       " 10632,\n",
       " 18134,\n",
       " 20389,\n",
       " 9698,\n",
       " 20979,\n",
       " 11781,\n",
       " 6220,\n",
       " 19930,\n",
       " 8954,\n",
       " 4236,\n",
       " 10455,\n",
       " 10510,\n",
       " 20619,\n",
       " 7075,\n",
       " 12782,\n",
       " 22662,\n",
       " 9404,\n",
       " 12091,\n",
       " 10170,\n",
       " 20978,\n",
       " 15549,\n",
       " 15935,\n",
       " 21654,\n",
       " 10969,\n",
       " 3003,\n",
       " 17734,\n",
       " 20699,\n",
       " 18649,\n",
       " 18323,\n",
       " 16866,\n",
       " 21094,\n",
       " 19104,\n",
       " 18762,\n",
       " 3732,\n",
       " 12131,\n",
       " 21433,\n",
       " 10994,\n",
       " 21195,\n",
       " 17610,\n",
       " 14694,\n",
       " 12099,\n",
       " 16225,\n",
       " 10605,\n",
       " 16173,\n",
       " 13718,\n",
       " 12324,\n",
       " 9241,\n",
       " 7645,\n",
       " 9399,\n",
       " 22684,\n",
       " 22367,\n",
       " 19026,\n",
       " 12887,\n",
       " 12601,\n",
       " 9221,\n",
       " 6296,\n",
       " 21933,\n",
       " 9792,\n",
       " 14991,\n",
       " 18601,\n",
       " 3885,\n",
       " 6611,\n",
       " 10333,\n",
       " 12143,\n",
       " 16809,\n",
       " 3538,\n",
       " 14390,\n",
       " 10370,\n",
       " 7025,\n",
       " 14778,\n",
       " 19534,\n",
       " 16202,\n",
       " 21661,\n",
       " 11571,\n",
       " 19394,\n",
       " 8688,\n",
       " 20522,\n",
       " 14569,\n",
       " 22616,\n",
       " 21145,\n",
       " 10208,\n",
       " 22339,\n",
       " 10464,\n",
       " 17794,\n",
       " 21394,\n",
       " 10572,\n",
       " 11912,\n",
       " 21007,\n",
       " 11924,\n",
       " 21410,\n",
       " 17387,\n",
       " 21711,\n",
       " 13273,\n",
       " 21517,\n",
       " 21773,\n",
       " 16096,\n",
       " 21415,\n",
       " 11708,\n",
       " 13910,\n",
       " 20971,\n",
       " 10200,\n",
       " 7000,\n",
       " 21573,\n",
       " 8938,\n",
       " 19704,\n",
       " 11189,\n",
       " 18435,\n",
       " 12034,\n",
       " 14565,\n",
       " 20749,\n",
       " 12803,\n",
       " 8798,\n",
       " 17037,\n",
       " 19139,\n",
       " 11675,\n",
       " 14690,\n",
       " 21063,\n",
       " 16272,\n",
       " 9032,\n",
       " 13471,\n",
       " 14907,\n",
       " 16500,\n",
       " 8984,\n",
       " 12820,\n",
       " 17661,\n",
       " 12542,\n",
       " 12049,\n",
       " 15792,\n",
       " 9016,\n",
       " 11602,\n",
       " 12227,\n",
       " 17970,\n",
       " 20927,\n",
       " 3294,\n",
       " 17669,\n",
       " 18505,\n",
       " 11930,\n",
       " 21746,\n",
       " 9349,\n",
       " 22181,\n",
       " 11050,\n",
       " 12368,\n",
       " 17046,\n",
       " 19736,\n",
       " 16159,\n",
       " 13801,\n",
       " 19218,\n",
       " 21615,\n",
       " 13394,\n",
       " 14874,\n",
       " 10567,\n",
       " 17006,\n",
       " 20266,\n",
       " 11351,\n",
       " 22678,\n",
       " 3241,\n",
       " 609,\n",
       " 14123,\n",
       " 22112,\n",
       " 12735,\n",
       " 16141,\n",
       " 9132,\n",
       " 8785,\n",
       " 10108,\n",
       " 15290,\n",
       " 10811,\n",
       " 13119,\n",
       " 21215,\n",
       " 8621,\n",
       " 21102,\n",
       " 11198,\n",
       " 21623,\n",
       " 18109,\n",
       " 20207,\n",
       " 22152,\n",
       " 22371,\n",
       " 22242,\n",
       " 20768,\n",
       " 11499,\n",
       " 17208,\n",
       " 18906,\n",
       " 10795,\n",
       " 16756,\n",
       " 21572,\n",
       " 13392,\n",
       " 20086,\n",
       " 19196,\n",
       " 11849,\n",
       " 2797,\n",
       " 11229,\n",
       " 18662,\n",
       " 19535,\n",
       " 18771,\n",
       " 17931,\n",
       " 8875,\n",
       " 1810,\n",
       " 9420,\n",
       " 14164,\n",
       " 21237,\n",
       " 22144,\n",
       " 17462,\n",
       " 21184,\n",
       " 17043,\n",
       " 9536,\n",
       " 9394,\n",
       " 21756,\n",
       " 21686,\n",
       " 5634,\n",
       " 18951,\n",
       " 20094,\n",
       " 7382,\n",
       " 21663,\n",
       " 20480,\n",
       " 12318,\n",
       " 11410,\n",
       " 18753,\n",
       " 17584,\n",
       " 22100,\n",
       " 18780,\n",
       " 17559,\n",
       " 13531,\n",
       " 18384,\n",
       " 18116,\n",
       " 6457,\n",
       " 14289,\n",
       " 22581,\n",
       " 13030,\n",
       " 14498,\n",
       " 19480,\n",
       " 21431,\n",
       " 15084,\n",
       " 16939,\n",
       " 18769,\n",
       " 11791,\n",
       " 18884,\n",
       " 13336,\n",
       " 14022,\n",
       " 10036,\n",
       " 13235,\n",
       " 14902,\n",
       " 22594,\n",
       " 16259,\n",
       " 19143,\n",
       " 22376,\n",
       " 3935,\n",
       " 18795,\n",
       " 9710,\n",
       " 10414,\n",
       " 9485,\n",
       " 10275,\n",
       " 20579,\n",
       " 12335,\n",
       " 18274,\n",
       " 8165,\n",
       " 11975,\n",
       " 20585,\n",
       " 15956,\n",
       " 12636,\n",
       " 14970,\n",
       " 17452,\n",
       " 9117,\n",
       " 17428,\n",
       " 17139,\n",
       " 17614,\n",
       " 10281,\n",
       " 10872,\n",
       " 20907,\n",
       " 12925,\n",
       " 21271,\n",
       " 12347,\n",
       " 22451,\n",
       " 15502,\n",
       " 18531,\n",
       " 17546,\n",
       " 7733,\n",
       " 21733,\n",
       " 15919,\n",
       " 15118,\n",
       " 17381,\n",
       " 21855,\n",
       " 15150,\n",
       " 9434,\n",
       " 19103,\n",
       " 18896,\n",
       " 5144,\n",
       " 10082,\n",
       " 16109,\n",
       " 15331,\n",
       " 20663,\n",
       " 16651,\n",
       " 14612,\n",
       " 9905,\n",
       " 14468,\n",
       " 17086,\n",
       " 10829,\n",
       " 12522,\n",
       " 14041,\n",
       " 1546,\n",
       " 11057,\n",
       " 10302,\n",
       " 4287,\n",
       " 11727,\n",
       " 13313,\n",
       " 22606,\n",
       " 11119,\n",
       " 5628,\n",
       " 9933,\n",
       " 12011,\n",
       " 21624,\n",
       " 11594,\n",
       " 10005,\n",
       " 17213,\n",
       " 19662,\n",
       " 15595,\n",
       " 11275,\n",
       " 18844,\n",
       " 9820,\n",
       " 14280,\n",
       " 15881,\n",
       " 10007,\n",
       " 21792,\n",
       " 12429,\n",
       " 15879,\n",
       " 20400,\n",
       " 18488,\n",
       " 17045,\n",
       " 12527,\n",
       " 12599,\n",
       " 18466,\n",
       " 15508,\n",
       " 8853,\n",
       " 16590,\n",
       " 18275,\n",
       " 18132,\n",
       " 14976,\n",
       " 10501,\n",
       " 15820,\n",
       " 21218,\n",
       " 8108,\n",
       " 7003,\n",
       " 17581,\n",
       " 5397,\n",
       " 13199,\n",
       " 11072,\n",
       " 22085,\n",
       " 12248,\n",
       " 21243,\n",
       " 11388,\n",
       " 20262,\n",
       " 12239,\n",
       " 3231,\n",
       " 10344,\n",
       " 22213,\n",
       " 19215,\n",
       " 21325,\n",
       " 11425,\n",
       " 11719,\n",
       " 9545,\n",
       " 13138,\n",
       " 12523,\n",
       " 11441,\n",
       " 10640,\n",
       " 13817,\n",
       " 19324,\n",
       " 17447,\n",
       " 9885,\n",
       " 19345,\n",
       " 13926,\n",
       " 9691,\n",
       " 13227,\n",
       " 18001,\n",
       " 16469,\n",
       " 11387,\n",
       " 7006,\n",
       " 11841,\n",
       " 16055,\n",
       " 12087,\n",
       " 14099,\n",
       " 8741,\n",
       " 7786,\n",
       " 12259,\n",
       " 18636,\n",
       " 10074,\n",
       " 10116,\n",
       " 3835,\n",
       " 21082,\n",
       " 15463,\n",
       " 11573,\n",
       " 13068,\n",
       " 11920,\n",
       " 20653,\n",
       " 17568,\n",
       " 8836,\n",
       " 10057,\n",
       " 16083,\n",
       " 13526,\n",
       " 16547,\n",
       " 20856,\n",
       " 11178,\n",
       " 8009,\n",
       " 12183,\n",
       " 19621,\n",
       " 22368,\n",
       " 14974,\n",
       " 14040,\n",
       " 12802,\n",
       " 9066,\n",
       " 18786,\n",
       " 9692,\n",
       " 18594,\n",
       " 10251,\n",
       " 19959,\n",
       " 10621,\n",
       " 12076,\n",
       " 10678,\n",
       " 18402,\n",
       " 17092,\n",
       " 22599,\n",
       " 21146,\n",
       " 13017,\n",
       " 15567,\n",
       " 12885,\n",
       " 19791,\n",
       " 18610,\n",
       " 10266,\n",
       " 3249,\n",
       " 11543,\n",
       " 10355,\n",
       " 21141,\n",
       " 20991,\n",
       " 19285,\n",
       " 12774,\n",
       " 9506,\n",
       " 6101,\n",
       " 20893,\n",
       " 17064,\n",
       " 22623,\n",
       " 19004,\n",
       " 9517,\n",
       " 20621,\n",
       " 10800,\n",
       " 18403,\n",
       " 6686,\n",
       " 15512,\n",
       " 21938,\n",
       " 17982,\n",
       " 15518,\n",
       " 19111,\n",
       " 9072,\n",
       " 9160,\n",
       " 5576,\n",
       " 21148,\n",
       " 17023,\n",
       " 5780,\n",
       " 22428,\n",
       " 17888,\n",
       " 20438,\n",
       " 15413,\n",
       " 19841,\n",
       " 19119,\n",
       " 20739,\n",
       " 21283,\n",
       " 11243,\n",
       " 14047,\n",
       " 19308,\n",
       " 10927,\n",
       " 19717,\n",
       " 18169,\n",
       " 21204,\n",
       " 4374,\n",
       " 13220,\n",
       " 1817,\n",
       " 16169,\n",
       " 9049,\n",
       " 7065,\n",
       " 9682,\n",
       " 12046,\n",
       " 12240,\n",
       " 19331,\n",
       " 9097,\n",
       " 12396,\n",
       " 10185,\n",
       " 10986,\n",
       " 22503,\n",
       " 19079,\n",
       " 13598,\n",
       " 14609,\n",
       " 13023,\n",
       " 21590,\n",
       " 21981,\n",
       " 14844,\n",
       " 17615,\n",
       " 10086,\n",
       " 11051,\n",
       " 18537,\n",
       " 22155,\n",
       " 9838,\n",
       " 15336,\n",
       " 21167,\n",
       " 21061,\n",
       " 10921,\n",
       " 18404,\n",
       " 11433,\n",
       " 17510,\n",
       " 19113,\n",
       " 10873,\n",
       " 9591,\n",
       " 13509,\n",
       " 18905,\n",
       " 19656,\n",
       " 20521,\n",
       " 7867,\n",
       " 20575,\n",
       " 20958,\n",
       " 6169,\n",
       " 13456,\n",
       " 14861,\n",
       " 20909,\n",
       " 19641,\n",
       " 9478,\n",
       " 9729,\n",
       " 1290,\n",
       " 14015,\n",
       " 20951,\n",
       " 10354,\n",
       " 7332,\n",
       " 21597,\n",
       " 9765,\n",
       " 10536,\n",
       " 11407,\n",
       " 16403,\n",
       " 21949,\n",
       " 11148,\n",
       " 15905,\n",
       " 19790,\n",
       " 17738,\n",
       " 11175,\n",
       " 21342,\n",
       " 19239,\n",
       " 18476,\n",
       " 13744,\n",
       " 9754,\n",
       " 9336,\n",
       " 17825,\n",
       " 14994,\n",
       " 12872,\n",
       " 19507,\n",
       " 10248,\n",
       " 12839,\n",
       " 18686,\n",
       " 14834,\n",
       " 19510,\n",
       " 21692,\n",
       " 18386,\n",
       " 19940,\n",
       " 16234,\n",
       " 9869,\n",
       " 13157,\n",
       " 15085,\n",
       " 9181,\n",
       " 21872,\n",
       " 4351,\n",
       " 20532,\n",
       " 20404,\n",
       " 17788,\n",
       " 19177,\n",
       " 10662,\n",
       " 12132,\n",
       " 9087,\n",
       " 21088,\n",
       " 12761,\n",
       " 10000,\n",
       " 15241,\n",
       " 14277,\n",
       " 11368,\n",
       " 22222,\n",
       " 11588,\n",
       " 14026,\n",
       " 11941,\n",
       " 17470,\n",
       " 16359,\n",
       " 16338,\n",
       " 13455,\n",
       " 7858,\n",
       " 13952,\n",
       " 7469,\n",
       " 16119,\n",
       " 2842,\n",
       " 22398,\n",
       " 22073,\n",
       " 15778,\n",
       " 616,\n",
       " 10984,\n",
       " 9197,\n",
       " 14230,\n",
       " 21832,\n",
       " 11498,\n",
       " 11428,\n",
       " 17880,\n",
       " 15266,\n",
       " 16139,\n",
       " 20257,\n",
       " 17156,\n",
       " 10726,\n",
       " 9865,\n",
       " 11296,\n",
       " 2955,\n",
       " 17852,\n",
       " 22537,\n",
       " 8895,\n",
       " 10522,\n",
       " 13675,\n",
       " 16137,\n",
       " 17047,\n",
       " 21154,\n",
       " 9813,\n",
       " 20777,\n",
       " ...]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset['document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Custom_data(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        self.data = data_dict['input_ids']\n",
    "        self.attention_mask = data_dict['attention_mask']\n",
    "        self.labels = data_dict['labels']\n",
    "        self.document = data_dict['document']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx]), torch.tensor(self.attention_mask[idx]), oh_encoder(self.labels[idx]), torch.tensor(self.document[idx])\n",
    "\n",
    "custom_train = Custom_data(trainset)\n",
    "custom_val = Custom_data(valset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19404)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_train[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Uncomment to check'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_collate(batch):\n",
    "    input_ids, attention_mask, labels, doc = zip(*batch)\n",
    "    # print(doc)\n",
    "    # print(max(len(input_ids[0]), len(input_ids[1])), max(len(labels[0]), len(labels[1])))\n",
    "    #print(input_ids[0], labels[0])\n",
    "\n",
    "    # Pad the input_ids and labels\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    padded_attention_mask = pad_sequence(attention_mask, batch_first = True, padding_value = 0)\n",
    "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=-100)  # Padding for labels in token classification\n",
    "\n",
    "    return padded_input_ids, padded_attention_mask, padded_labels, doc\n",
    "\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(custom_train, batch_size=batch_size, collate_fn = custom_collate)\n",
    "val_dataloader = DataLoader(custom_val, batch_size=batch_size, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "'''Uncomment to check'''\n",
    "# for i, (input_ids, attention_mask, labels, doc) in enumerate(train_dataloader):\n",
    "#     print(f'Batch {i + 1}:')\n",
    "#     print('Input IDs:', input_ids.size())\n",
    "#     print('Attention_mask:', attention_mask.size())\n",
    "#     print('Labels:', labels.size())\n",
    "#     print('Document:', doc)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "Define F5 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, valset, batch_size, custom_collate, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    avg_val_loss = 0\n",
    "    avg_val_score = 0\n",
    "    with torch.no_grad:\n",
    "        val_loss = 0\n",
    "        val_score = 0\n",
    "        val_dataloader = DataLoader(custom_val, batch_size=batch_size, collate_fn=custom_collate)\n",
    "\n",
    "        for (input_ids, attention_mask, labels, doc) in val_dataloader:                \n",
    "            \n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "            #val_score += \n",
    "\n",
    "    avg_val_loss = val_loss / len(valset) \n",
    "    avg_val_score = val_score/len(valset)\n",
    "\n",
    "    print(f\"Average val_loss: {avg_val_loss}, avgerage val_score = {avg_val_score}\")\n",
    "\n",
    "    return avg_val_loss, avg_val_score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainset, batch_size, custom_collate, epochs, optimizer, criterion,  device):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        avg_train_loss = 0\n",
    "        avg_train_score = 0\n",
    "        train_loss = 0\n",
    "        train_score = 0\n",
    "\n",
    "        \n",
    "        train_dataloader = DataLoader(custom_train, batch_size=batch_size, collate_fn = custom_collate)\n",
    "        \n",
    "        for (input_ids, attention_mask, labels, doc) in train_dataloader:     \n",
    "            optimizer.zero_grad()\n",
    "            input_ids = input_ids.to(device)\n",
    "            # print(input_ids.size())\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask).logits\n",
    "            # print(outputs.size())\n",
    "            # print(labels.size())\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss\n",
    "            #train_score += \n",
    "\n",
    "        avg_train_loss = train_loss / len(trainset)    \n",
    "        avg_train_score = train_score / len(trainset)\n",
    "        val_loss, val_score = val(model, valset, batch_size, collator, criterion,  f5, device)\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Average train_loss = {avg_train_loss}, average train_score = {avg_train_score}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_collate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[119], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, trainset, batch_size, custom_collate, epochs, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# print(outputs.size())\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# print(labels.size())\u001b[39;00m\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     27\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train(model, trainset, batch_size, custom_collate, epochs, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_dataloader):\n",
    "    if i == 0:\n",
    "        print(batch)\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting from predictions to NER labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to label each token at NER stage\n",
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "    \n",
    "    idx = 0\n",
    "    spans = []\n",
    "    span = []\n",
    "\n",
    "    for i, token in enumerate(document):\n",
    "        if token != target[idx]:\n",
    "            idx = 0\n",
    "            span = []\n",
    "            continue\n",
    "        span.append(i)\n",
    "        idx += 1\n",
    "        if idx == len(target):\n",
    "            spans.append(span)\n",
    "            span = []\n",
    "            idx = 0\n",
    "            continue\n",
    "    \n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DON'T RUN ##\n",
    "#### From KAGGLE: https://www.kaggle.com/code/manavtrivedi/0-967-nlp-sakura/notebook ####\n",
    "\n",
    "triplets = []\n",
    "pairs = set()  # membership operation using set is faster O(1) than that of list O(n)\n",
    "processed = []\n",
    "emails = []\n",
    "phone_nums = []\n",
    "urls = []\n",
    "streets = []\n",
    "\n",
    "# For each prediction, token mapping, offsets, tokens, and document in the dataset\n",
    "for p, token_map, offsets, tokens, doc, full_text in zip(\n",
    "    preds_final, \n",
    "    ds[\"token_map\"], \n",
    "    ds[\"offset_mapping\"], \n",
    "    ds[\"tokens\"], \n",
    "    ds[\"document\"],\n",
    "    ds[\"full_text\"]\n",
    "):\n",
    "\n",
    "    # Iterate through each token prediction and its corresponding offsets\n",
    "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "        label_pred = id2label[str(token_pred)]  # Predicted label from token\n",
    "        if start_idx + end_idx == 0:\n",
    "            continue\n",
    "        if token_map[start_idx] == -1:\n",
    "            start_idx += 1\n",
    "        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "            start_idx += 1\n",
    "        if start_idx >= len(token_map):\n",
    "            break\n",
    "        token_id = token_map[start_idx]  # Token ID at start index\n",
    "        if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n",
    "            continue\n",
    "        pair = (doc, token_id)\n",
    "        if pair not in pairs:\n",
    "            processed.append({\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]})\n",
    "            pairs.add(pair)\n",
    "    \n",
    "    # email\n",
    "    for token_idx, token in enumerate(tokens):\n",
    "        if re.fullmatch(email_regex, token) is not None:\n",
    "            emails.append(\n",
    "                {\"document\": doc, \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "            )\n",
    "                \n",
    "    # phone number\n",
    "    matches = phone_num_regex.findall(full_text)\n",
    "    if not matches:\n",
    "        continue\n",
    "    for match in matches:\n",
    "        target = [t.text for t in nlp.tokenizer(match)]\n",
    "        matched_spans = find_span(target, tokens)\n",
    "    for matched_span in matched_spans:\n",
    "        for intermediate, token_idx in enumerate(matched_span):\n",
    "            prefix = \"I\" if intermediate else \"B\"\n",
    "            phone_nums.append(\n",
    "                {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": tokens[token_idx]}\n",
    "            )\n",
    "    \n",
    "    # url\n",
    "    matches = url_regex.findall(full_text)\n",
    "    if not matches:\n",
    "        continue\n",
    "    for match in matches:\n",
    "        target = [t.text for t in nlp.tokenizer(match)]\n",
    "        matched_spans = find_span(target, tokens)\n",
    "    for matched_span in matched_spans:\n",
    "        for intermediate, token_idx in enumerate(matched_span):\n",
    "            prefix = \"I\" if intermediate else \"B\"\n",
    "            urls.append(\n",
    "                {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-URL_PERSONAL\", \"token_str\": tokens[token_idx]}\n",
    "            )\n",
    "    \n",
    "    # street\n",
    "#     matches = street_regex.findall(full_text)\n",
    "#     if not matches:\n",
    "#         continue\n",
    "#     for match in matches:\n",
    "#         target = [t.text for t in nlp.tokenizer(match)]\n",
    "#         matched_spans = find_span(target, tokens)\n",
    "#     for matched_span in matched_spans:\n",
    "#         for intermediate, token_idx in enumerate(matched_span):\n",
    "#             prefix = \"I\" if intermediate else \"B\"\n",
    "#             streets.append(\n",
    "#                 {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-STREET_ADDRESS\", \"token_str\": tokens[token_idx]}\n",
    "#             )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
