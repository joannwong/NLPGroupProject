{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datasets\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments\\\n",
    "    , AutoModelForSequenceClassification, AutoConfig\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.lang.en import English\n",
    "from ignite.metrics import Fbeta\n",
    "from functools import partial\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Finding out the number of labels\n",
    "# data = json.load(open('data/train.json'))\n",
    "\n",
    "# all_labels = set()\n",
    "\n",
    "# for d in data:\n",
    "#     all_labels = all_labels.union(set(d['labels']))\n",
    "\n",
    "# print(f\"{len(list(all_labels))} labels, with the following labels:\\n {list(all_labels)}\")\n",
    "# del data\n",
    "\n",
    "# label2id = {label:index for index,label in enumerate(all_labels)}\n",
    "# id2label = {index:label for index,label in enumerate(all_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to one-hot vector\n",
    "# def oh_encoder(labels):  #label: array of output for each sentence\n",
    "#     unique_labels = ['O', 'B-NAME_STUDENT','I-NAME_STUDENT','B-URL_PERSONAL', 'B-ID_NUM','I-ID_NUM','B-EMAIL','I-STREET_ADDRESS',\n",
    "#                      'I-PHONE_NUM', 'B-USERNAME', 'B-PHONE_NUM','B-STREET_ADDRESS', 'I-URL_PERSONAL']\n",
    "    \n",
    "#     labels_oh = []\n",
    "#     for label in labels:    #label: str\n",
    "#         label_oh = [float(0)]*len(unique_labels)\n",
    "#         for k in range(len(unique_labels)):\n",
    "#             if unique_labels[k] == label:\n",
    "#                 label_oh[k] = float(1)\n",
    "#                 labels_oh.append(torch.tensor(label_oh, requires_grad=True))\n",
    "#                 break\n",
    "        \n",
    "#     #return torch.tensor(labels_oh, requires_grad=True)\n",
    "#     return labels_oh    #list of one-hot labels as tensors\n",
    "\n",
    "#Change to one-hot vector\n",
    "def oh_encoder(labels):  #label: array of output for each sentence\n",
    "\n",
    "    # unique_labels = ['O', 'B-NAME_STUDENT','I-NAME_STUDENT','B-PHONE_NUM', 'I-PHONE_NUM','B-ID_NUM', 'I-ID_NUM',  'B-URL_PERSONAL','I-URL_PERSONAL',\n",
    "    #                   'B-STREET_ADDRESS', 'I-STREET_ADDRESS',  'B-EMAIL', 'B-USERNAME']\n",
    "    \n",
    "    \n",
    "    unique_labels = ['O', 'B-NAME_STUDENT','I-NAME_STUDENT','B-URL_PERSONAL', 'B-ID_NUM','I-ID_NUM','B-EMAIL','I-STREET_ADDRESS',\n",
    "                     'I-PHONE_NUM', 'B-USERNAME', 'B-PHONE_NUM','B-STREET_ADDRESS', 'I-URL_PERSONAL']\n",
    "    \n",
    "    labels_oh = []\n",
    "    for label in labels:    #label: str\n",
    "        label_oh = [float(0)]*len(unique_labels)\n",
    "        for k in range(len(unique_labels)):\n",
    "            if unique_labels[k] == label:\n",
    "                label_oh[k] = float(1)\n",
    "                labels_oh.append(torch.tensor(label_oh, requires_grad=True))\n",
    "                break\n",
    "\n",
    "    #return torch.tensor(labels_oh, requires_grad=True)\n",
    "    return labels_oh    #list of one-hot labels as tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing sentences.\n",
    "def tokenize(example, tokenizer):\n",
    "    # Preprocess the tokens and labels by adding trailing whitespace and labels\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for token, label, t_ws in zip(example[\"tokens\"], \n",
    "                                  example[\"labels\"],\n",
    "                                  example[\"trailing_whitespace\"]):\n",
    "        tokens.append(token)\n",
    "        labels.extend([label] * len(token))\n",
    "        # Added trailing whitespace and label if true and \n",
    "        if t_ws:\n",
    "            tokens.append(\" \")\n",
    "            labels.append(\"O\")  \n",
    "    \n",
    "    text = \"\".join(tokens)\n",
    "    # print(f\"len(text)={len(text)}, len(tokens)={len(tokens)}\")\n",
    "    # tokenization without truncation\n",
    "    tokenized = tokenizer(text, return_offsets_mapping=True,\n",
    "                          truncation=False)\n",
    "    # labels = np.array(labels)\n",
    "    # Labels\n",
    "    token_labels = []\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # Added 'O' \n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            #token_labels.append(label2id[\"O\"]) \n",
    "            token_labels.append(\"O\") \n",
    "        else:\n",
    "            # case when the text starts with whitespace\n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "            # Convert label to id (int)\n",
    "            #label_id = label2id[labels[start_idx]]\n",
    "            label_id = labels[start_idx]\n",
    "            token_labels.append(label_id)\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": len(tokenized.input_ids)}\n",
    "    \n",
    "# def tokenize(example, tokenizer, INFERENCE_MAX_LENGTH=3500):\n",
    "#     ''' \n",
    "#     Arguments:\n",
    "#     example: sentence\n",
    "#     tokenizer: following DeBERTa's\n",
    "#     INFERENCE_MAX_LENGTH: for truncation if needed\n",
    "\n",
    "#     Returns:\n",
    "#     dictionary of tokenized word id, with token_map, which maps characters to its initial idx\n",
    "#     '''\n",
    "#     text = []\n",
    "#     token_map = []\n",
    "#     idx = 0\n",
    "    \n",
    "#     for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "#         text.append(t)\n",
    "#         token_map.extend([idx]*len(t))\n",
    "#         if ws:\n",
    "#             text.append(\" \")\n",
    "#             token_map.append(-1)\n",
    "#         idx += 1\n",
    "#     tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=False, max_length=INFERENCE_MAX_LENGTH, return_tensors='pt'\n",
    "#                          , add_special_tokens=False)\n",
    "#     return {\n",
    "#         **tokenized,\n",
    "#         \"token_map\": token_map,\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict(data):\n",
    "    '''\n",
    "    Change from a list of dictionary to a dictionary\n",
    "    '''\n",
    "    dict_of_lists = {}\n",
    "    for d in data:\n",
    "        for key, value in d.items():\n",
    "            if key in dict_of_lists:\n",
    "                dict_of_lists[key].append(value)\n",
    "            else:\n",
    "                dict_of_lists[key] = [value]\n",
    "    return dict_of_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = json.load(open('../data/train.json'))\n",
    "# d = data[0]\n",
    "\n",
    "# input_ids = tokenize(d, tokenizer)['input_ids'].numpy()[0]\n",
    "# # print(input_ids)\n",
    "# print(len(input_ids))\n",
    "# tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "# # print(tokens)\n",
    "# # print(d['tokens'])\n",
    "# print(len(d['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split completed\n",
      "trainset loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2db39750cd48c1a0023bf2bb33d25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/5785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset mapped\n",
      "valset loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523f9eed5d7549d4844975308e891cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/1022 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valset mapped\n"
     ]
    }
   ],
   "source": [
    "#Preparing the datasets for token classification\n",
    "data = json.load(open('../data/train.json'))\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.15, random_state=42)  \n",
    "print('Data split completed')\n",
    "# # train_labels = [oh_encoder(x['labels']) for x in train_data]\n",
    "\n",
    "trainset = datasets.Dataset.from_dict({\n",
    "    'full_text': [x['full_text'] for x in train_data],\n",
    "    'document': [x['document'] for x in train_data],\n",
    "    'tokens': [x['tokens'] for x in train_data],\n",
    "    'trailing_whitespace': [x['trailing_whitespace'] for x in train_data],\n",
    "    'labels' :[x['labels'] for x in train_data]\n",
    "    # 'labels' :[oh_encoder(x['labels']) for x in train_data] \n",
    "})\n",
    "print('trainset loaded')\n",
    "\n",
    "trainset = trainset.map(tokenize, fn_kwargs = {\"tokenizer\": tokenizer}, num_proc=3)\n",
    "#train_labels = [oh_encoder(x['labels'] for x in train_data)]\n",
    "print('trainset mapped')\n",
    "\n",
    "# val_labels = [oh_encoder(x['labels']) for x in val_data]\n",
    "\n",
    "valset = datasets.Dataset.from_dict({\n",
    "    'full_text': [x['full_text'] for x in val_data],\n",
    "    'document': [x['document'] for x in val_data],\n",
    "    'tokens': [x['tokens'] for x in val_data],\n",
    "    'trailing_whitespace': [x['trailing_whitespace'] for x in val_data],\n",
    "    'labels' :[x['labels'] for x in val_data]\n",
    "    # 'labels' :[oh_encoder(x['labels']) for x in val_data]\n",
    "})\n",
    "print('valset loaded')\n",
    "\n",
    "valset = valset.map(tokenize, fn_kwargs = {\"tokenizer\": tokenizer}, num_proc=3)\n",
    "print('valset mapped')\n",
    "\n",
    "#val_labels = [oh_encoder(x['labels'] for x in val_data)]\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert from a list of dict to 1 big dict\n",
    "trainset = to_dict(trainset)\n",
    "#trainset['labels'] = train_labels\n",
    "\n",
    "valset = to_dict(valset)\n",
    "# #valset['labels'] = val_labels\n",
    "\n",
    "# Apply one hot encoding to labels\n",
    "trainset['one_hot_labels'] = [[oh_encoder(item) for item in sublist] for sublist in trainset['labels']]\n",
    "valset['one_hot_labels'] = [[oh_encoder(item) for item in sublist] for sublist in valset['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_data(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        self.data = data_dict['input_ids']\n",
    "        self.attention_mask = data_dict['attention_mask']\n",
    "        self.labels = data_dict['one_hot_labels']\n",
    "        self.doc_no = data_dict['document']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx]), torch.tensor(self.attention_mask[idx]), self.labels[idx] , torch.tensor(self.doc_no[idx])\n",
    "\n",
    "# Assuming custom_train is properly initialized as Custom_data\n",
    "custom_train = Custom_data(trainset)\n",
    "custom_val = Custom_data(valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Largely ok but sth is broken here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    '''\n",
    "    For padding\n",
    "    '''\n",
    "    input_ids, attention_mask, one_hot_labels, document = zip(*batch)\n",
    "    # Pad the input_ids and labels\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    \n",
    "    padded_attention_mask = pad_sequence(attention_mask, batch_first = True, padding_value = 0)\n",
    "    \n",
    "    padded_labels = pad_sequence(one_hot_labels, batch_first=True, padding_value=0)  # Padding for labels in token classification\n",
    "    \n",
    "    padded_doc = pad_sequence(document, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return padded_input_ids, padded_attention_mask, padded_labels, padded_doc\n",
    "\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(custom_train, batch_size=batch_size, collate_fn = custom_collate)\n",
    "val_dataloader = DataLoader(custom_val, batch_size=batch_size, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough workings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (4150190818.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[20], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    *custom_train[0]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m can't use starred expression here\n"
     ]
    }
   ],
   "source": [
    "*custom_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(custom_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2893"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m      2\u001b[0m     a,b,c,d \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "Cell \u001b[1;32mIn[61], line 22\u001b[0m, in \u001b[0;36mcustom_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     20\u001b[0m padded_attention_mask \u001b[38;5;241m=\u001b[39m pad_sequence(attention_mask, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Assuming labels and document need padding as well; adjust if not necessary\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m padded_labels \u001b[38;5;241m=\u001b[39m pad_sequence(labels, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     23\u001b[0m padded_doc \u001b[38;5;241m=\u001b[39m pad_sequence(document, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m padded_input_ids, padded_attention_mask, padded_labels, padded_doc\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\utils\\rnn.py:399\u001b[0m, in \u001b[0;36mpad_sequence\u001b[1;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[0;32m    395\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[1;32m--> 399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mpad_sequence(sequences, batch_first, padding_value)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "for b in train_dataloader:\n",
    "    a,b,c,d = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: DeBerta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, valset, batch_size, collator, criterion,  f5, device):\n",
    "    model.eval()\n",
    "\n",
    "    avg_val_loss = 0\n",
    "    avg_val_score = 0\n",
    "    with torch.no_grad:\n",
    "        val_loss = 0\n",
    "        val_score = 0\n",
    "        val_dataloader = DataLoader(custom_val, batch_size=batch_size, collate_fn=custom_collate)\n",
    "\n",
    "        for (input_ids, attention_mask, labels) in val_dataloader:                \n",
    "            \n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "            val_score += f5(outputs, labels)\n",
    "\n",
    "    avg_val_loss = val_loss / len(valset) \n",
    "    avg_val_score = val_score/len(valset)\n",
    "\n",
    "    print(f\"Average val_loss: {avg_val_loss}, avgerage val_score = {avg_val_score}\")\n",
    "\n",
    "    return avg_val_loss, avg_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainset, batch_size, collator, epochs, optimizer, criterion,  f5, device):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        avg_train_loss = 0\n",
    "        avg_train_score = 0\n",
    "        train_loss = 0\n",
    "        train_score = 0\n",
    "\n",
    "        train_dataloader = DataLoader(custom_train, batch_size=batch_size, collate_fn = custom_collate)\n",
    "        \n",
    "        for (input_ids, attention_mask, labels) in train_dataloader:     \n",
    "            optimizer.zero_grad()\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "\n",
    "            outputs = model(input_ids, attention_mask).logits\n",
    "            #print(outputs.logits)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss\n",
    "            train_score += f5(outputs, labels)\n",
    "\n",
    "        avg_train_loss = train_loss / len(trainset)    \n",
    "        avg_train_score = train_score / len(trainset)\n",
    "        val_loss, val_score = val(model, valset, batch_size, collator, criterion,  f5, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Average train_loss = {avg_train_loss}, average train_score = {avg_train_score}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = model.classifier.in_features\n",
    "model.classifier = nn.Sequential( \n",
    "    nn.Linear(in_features, 13)\n",
    ")\n",
    "\n",
    "# Unfreeze parameters in classifier layer\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Check which layers are frozen\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Layer: {name} | Frozen: {'NO' if param.requires_grad else 'YES'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' \n",
    "model = model.to(device)\n",
    "epochs = 5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "f5 = Fbeta(beta=5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainset, batch_size, collator, epochs, optimizer, criterion,  f5, device):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        avg_train_loss = 0\n",
    "        avg_train_score = 0\n",
    "        train_loss = 0\n",
    "        train_score = 0\n",
    "\n",
    "        train_dataloader = DataLoader(custom_train, batch_size=batch_size, collate_fn = custom_collate)\n",
    "        \n",
    "        for (input_ids, attention_mask, labels) in train_dataloader:     \n",
    "            optimizer.zero_grad()\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "\n",
    "            outputs = model(input_ids, attention_mask).logits\n",
    "            #print(outputs.logits)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss\n",
    "            train_score += f5(outputs, labels)\n",
    "\n",
    "        avg_train_loss = train_loss / len(trainset)    \n",
    "        avg_train_score = train_score / len(trainset)\n",
    "        val_loss, val_score = val(model, valset, batch_size, collator, criterion,  f5, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Average train_loss = {avg_train_loss}, average train_score = {avg_train_score}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m----> 2\u001b[0m train(model, trainset, batch_size, custom_collate, epochs, optimizer, criterion, f5, device)\n",
      "Cell \u001b[1;32mIn[29], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, trainset, batch_size, collator, epochs, optimizer, criterion, f5, device)\u001b[0m\n\u001b[0;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#print(outputs.logits)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "train(model, trainset, batch_size, custom_collate, epochs, optimizer, criterion, f5, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: DeBERTa\n",
    "\n",
    "Using a pretrained DeBERTa, we will build a classifier head on top of it to predict the class at token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim = 13):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.dropout_prob = 0.3\n",
    "        self.final_activation = nn.Softmax(dim = -1)\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.ReLU(nn.Linear(input_dim, hidden_dim)),\n",
    "            \n",
    "            nn.Dropout(self.dropout_prob),\n",
    "            nn.ReLU(nn.Linear(hidden_dim, hidden_dim*2)),\n",
    "\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "            nn.ReLU(nn.Linear(hidden_dim*2, hidden_dim)),\n",
    "\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        logit = self.linear(x)\n",
    "        return self.final_activation(logit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim = 13):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.dropout_prob = 0.3\n",
    "        self.final_activation = nn.Softmax(dim = -1)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        self.linear1 = nn.ReLU(nn.Linear(input_dim,hidden_dim))\n",
    "        self.linear2 = nn.ReLU(nn.Linear(hidden_dim, hidden_dim*2))                            \n",
    "        self.linear3 = nn.ReLU(nn.Linear(hidden_dim*2, hidden_dim))\n",
    "        self.output = nn.Linear(hidden_dim,output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('now start linear1')\n",
    "        print(x.size())\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        print('now start linear2')\n",
    "        print(x.size())\n",
    "        x = self.linear2(self.dropout(x))\n",
    "\n",
    "        print('now start linear3')\n",
    "        print(x.size())\n",
    "        x = self.linear3(self.dropout(x))\n",
    "\n",
    "        print('now start output')\n",
    "        print(x.size())\n",
    "        logit= self.output(x)\n",
    "        \n",
    "        print('now start activation')\n",
    "        output =  self.final_activation(logit)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deberta_Classif(Classifier):\n",
    "    def __init__(self, model_name, classif_input, classif_hidden, classif_output = 13, finetune = False):\n",
    "        super(Deberta_Classif, self).__init__(classif_input, classif_input)\n",
    "        self.ft = finetune\n",
    "\n",
    "        self.extractor = AutoModelForTokenClassification.from_pretrained(model_name).base_model\n",
    "\n",
    "        if not finetune:\n",
    "            for param in self.extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.extractor_num_param =  sum(p.numel() for p in self.extractor.parameters())\n",
    "        self.extractor_num_param_grad = sum(p.numel() for p in self.extractor.parameters() if p.requires_grad)\n",
    "        self.extractor_name = \"DeBERTa\"\n",
    "                \n",
    "\n",
    "        self.classifier = Classifier(input_dim=classif_input, hidden_dim=classif_hidden ,output_dim=classif_output)\n",
    "        self.classifier_num_param = sum(p.numel() for p in self.classifier.parameters() if p.requires_grad)\n",
    "\n",
    "        \n",
    "    def count_param(self):\n",
    "        \n",
    "        if self.ft:\n",
    "            type = 'finetuned'\n",
    "            num_param = self.extractor_num_param_grad + self.classifier_num_param\n",
    "        else:\n",
    "            type = 'non-finetuned'\n",
    "            num_param = self.extractor_num_param + self.classifier_num_param\n",
    "\n",
    "        print(f\"Number of parameters in {type} {self.extractor_name} model is {num_param:,}\")\n",
    "    \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.extractor(input_ids, attention_mask).last_hidden_state\n",
    "        x = self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([13, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's trainable parameters:  9,997\n",
      "Model's total parameters:  183,841,549\n"
     ]
    }
   ],
   "source": [
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "#model = Deberta_Classif(model_name, classif_input = 768, classif_hidden = 100, classif_output = 13, finetune=False)  #Extractor output has dim 768 \n",
    "config = AutoModelForTokenClassification.from_pretrained(model_name).config\n",
    "\n",
    "config.update({\n",
    "            'num_labels': 13,\n",
    "            'ignore_mismatched_sizes': True,\n",
    "        })\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, config = config, ignore_mismatched_sizes=True)\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Model's trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad): ,}\")\n",
    "print(f\"Model's total parameters: {sum(p.numel() for p in model.parameters()): ,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' \n",
    "model = model.to(device)\n",
    "epochs = 5\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of = 32, max_length=3500)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "f5 = Fbeta(beta=5).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7dc5da0d3746a186266a72d7d77e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/5785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56529294e2a4446fba6decb75c5cd88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/1022 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Preparing the datasets for token classification\n",
    "data = json.load(open('../data/train.json'))\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.15, random_state=42)  \n",
    "# train_labels = [oh_encoder(x['labels']) for x in train_data]\n",
    "\n",
    "trainset = Dataset.from_dict({\n",
    "    # 'full_text': [x['full_text'] for x in train_data],\n",
    "    # 'document': [x['document'] for x in train_data],\n",
    "    'tokens': [x['tokens'] for x in train_data],\n",
    "    'trailing_whitespace': [x['trailing_whitespace'] for x in train_data],\n",
    "    'labels' :[oh_encoder(x['labels']) for x in train_data]\n",
    "})\n",
    "\n",
    "print('trainset loaded')\n",
    "trainset = trainset.map(tokenize, fn_kwargs = {\"tokenizer\": tokenizer}, num_proc=3)\n",
    "#train_labels = [oh_encoder(x['labels'] for x in train_data)]\n",
    "print('trainset mapped')\n",
    "\n",
    "# val_labels = [oh_encoder(x['labels']) for x in val_data]\n",
    "\n",
    "valset = Dataset.from_dict({\n",
    "    # 'full_text': [x['full_text'] for x in val_data],\n",
    "    # 'document': [x['document'] for x in val_data],\n",
    "    'tokens': [x['tokens'] for x in val_data],\n",
    "    'trailing_whitespace': [x['trailing_whitespace'] for x in val_data],\n",
    "    'labels' :[oh_encoder(x['labels']) for x in val_data]\n",
    "})\n",
    "print('valset loaded')\n",
    "valset = valset.map(tokenize, fn_kwargs = {\"tokenizer\": tokenizer}, num_proc=3)\n",
    "print('valset mapped')\n",
    "\n",
    "#val_labels = [oh_encoder(x['labels'] for x in val_data)]\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data: 5785 || Number of validation data: 1022\n"
     ]
    }
   ],
   "source": [
    "#First item\n",
    "print(f\"Number of training data: {len(trainset)} || Number of validation data: {len(valset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict(data):\n",
    "    dict_of_lists = {}\n",
    "    for d in data:\n",
    "        for key, value in d.items():\n",
    "            if key in dict_of_lists:\n",
    "                dict_of_lists[key].append(value)\n",
    "            else:\n",
    "                dict_of_lists[key] = [value]\n",
    "    return dict_of_lists\n",
    "\n",
    "trainset = to_dict(trainset)\n",
    "#trainset['labels'] = train_labels\n",
    "\n",
    "valset = to_dict(valset)\n",
    "#valset['labels'] = val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class Custom_data(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        self.data = data_dict['input_ids']\n",
    "        self.attention_mask = data_dict['attention_mask']\n",
    "        self.labels = data_dict['labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx][0]), torch.tensor(self.attention_mask[idx][0]), torch.tensor(self.labels[idx][0])\n",
    "\n",
    "# Assuming custom_train is properly initialized as Custom_data\n",
    "custom_train = Custom_data(trainset)\n",
    "custom_val = Custom_data(valset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    input_ids, attention_mask, labels = zip(*batch)\n",
    "    # Pad the input_ids and labels\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    padded_attention_mask = pad_sequence(attention_mask, batch_first = True, padding_value = 0)\n",
    "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=-100)  # Padding for labels in token classification\n",
    "    return padded_input_ids, padded_attention_mask, padded_labels\n",
    "\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(custom_train, batch_size=batch_size, collate_fn = custom_collate)\n",
    "val_dataloader = DataLoader(custom_val, batch_size=batch_size, collate_fn=custom_collate)\n",
    "\n",
    "# for i, (input_ids, attention_mask, labels) in enumerate(train_dataloader):\n",
    "#     print(f'Batch {i + 1}:')\n",
    "#     print('Input IDs:', input_ids)\n",
    "#     print('Attention_mask:', attention_mask)\n",
    "#     print('Labels:', labels)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    data, attention, labels = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  6738, 78580,  ..., 31401,   260,     2],\n",
       "        [    1,  1391,   367,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, valset, batch_size, collator, criterion,  f5, device):\n",
    "    model.eval()\n",
    "\n",
    "    avg_val_loss = 0\n",
    "    avg_val_score = 0\n",
    "    with torch.no_grad:\n",
    "        val_loss = 0\n",
    "        val_score = 0\n",
    "        val_dataloader = DataLoader(custom_val, batch_size=batch_size, collate_fn=custom_collate)\n",
    "\n",
    "        for (input_ids, attention_mask, labels) in val_dataloader:                \n",
    "            \n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "            val_score += f5(outputs, labels)\n",
    "\n",
    "    avg_val_loss = val_loss / len(valset) \n",
    "    avg_val_score = val_score/len(valset)\n",
    "\n",
    "    print(f\"Average val_loss: {avg_val_loss}, avgerage val_score = {avg_val_score}\")\n",
    "\n",
    "    return avg_val_loss, avg_val_score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainset, batch_size, collator, epochs, optimizer, criterion,  f5, device):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        avg_train_loss = 0\n",
    "        avg_train_score = 0\n",
    "        train_loss = 0\n",
    "        train_score = 0\n",
    "\n",
    "        \n",
    "        train_dataloader = DataLoader(custom_train, batch_size=batch_size, collate_fn = custom_collate)\n",
    "        \n",
    "        for (input_ids, attention_mask, labels) in train_dataloader:     \n",
    "            optimizer.zero_grad()\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "\n",
    "            outputs = model(input_ids, attention_mask).logits\n",
    "            #print(outputs.logits)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss\n",
    "            train_score += f5(outputs, labels)\n",
    "\n",
    "        avg_train_loss = train_loss / len(trainset)    \n",
    "        avg_train_score = train_score / len(trainset)\n",
    "        val_loss, val_score = val(model, valset, batch_size, collator, criterion,  f5, device)\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Average train_loss = {avg_train_loss}, average train_score = {avg_train_score}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[123], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, trainset, batch_size, collator, epochs, optimizer, criterion, f5, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (input_ids, attention_mask, labels) \u001b[38;5;129;01min\u001b[39;00m train_dataloader:     \n\u001b[0;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 16\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     18\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mlong()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "train(model, trainset, batch_size, collator, epochs, optimizer, criterion, f5, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting from predictions to NER labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to label each token at NER stage\n",
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "    \n",
    "    idx = 0\n",
    "    spans = []\n",
    "    span = []\n",
    "\n",
    "    for i, token in enumerate(document):\n",
    "        if token != target[idx]:\n",
    "            idx = 0\n",
    "            span = []\n",
    "            continue\n",
    "        span.append(i)\n",
    "        idx += 1\n",
    "        if idx == len(target):\n",
    "            spans.append(span)\n",
    "            span = []\n",
    "            idx = 0\n",
    "            continue\n",
    "    \n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DON'T RUN ##\n",
    "#### From KAGGLE: https://www.kaggle.com/code/manavtrivedi/0-967-nlp-sakura/notebook ####\n",
    "\n",
    "triplets = []\n",
    "pairs = set()  # membership operation using set is faster O(1) than that of list O(n)\n",
    "processed = []\n",
    "emails = []\n",
    "phone_nums = []\n",
    "urls = []\n",
    "streets = []\n",
    "\n",
    "# For each prediction, token mapping, offsets, tokens, and document in the dataset\n",
    "for p, token_map, offsets, tokens, doc, full_text in zip(\n",
    "    preds_final, \n",
    "    ds[\"token_map\"], \n",
    "    ds[\"offset_mapping\"], \n",
    "    ds[\"tokens\"], \n",
    "    ds[\"document\"],\n",
    "    ds[\"full_text\"]\n",
    "):\n",
    "\n",
    "    # Iterate through each token prediction and its corresponding offsets\n",
    "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "        label_pred = id2label[str(token_pred)]  # Predicted label from token\n",
    "        if start_idx + end_idx == 0:\n",
    "            continue\n",
    "        if token_map[start_idx] == -1:\n",
    "            start_idx += 1\n",
    "        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "            start_idx += 1\n",
    "        if start_idx >= len(token_map):\n",
    "            break\n",
    "        token_id = token_map[start_idx]  # Token ID at start index\n",
    "        if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n",
    "            continue\n",
    "        pair = (doc, token_id)\n",
    "        if pair not in pairs:\n",
    "            processed.append({\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]})\n",
    "            pairs.add(pair)\n",
    "    \n",
    "    # email\n",
    "    for token_idx, token in enumerate(tokens):\n",
    "        if re.fullmatch(email_regex, token) is not None:\n",
    "            emails.append(\n",
    "                {\"document\": doc, \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "            )\n",
    "                \n",
    "    # phone number\n",
    "    matches = phone_num_regex.findall(full_text)\n",
    "    if not matches:\n",
    "        continue\n",
    "    for match in matches:\n",
    "        target = [t.text for t in nlp.tokenizer(match)]\n",
    "        matched_spans = find_span(target, tokens)\n",
    "    for matched_span in matched_spans:\n",
    "        for intermediate, token_idx in enumerate(matched_span):\n",
    "            prefix = \"I\" if intermediate else \"B\"\n",
    "            phone_nums.append(\n",
    "                {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": tokens[token_idx]}\n",
    "            )\n",
    "    \n",
    "    # url\n",
    "    matches = url_regex.findall(full_text)\n",
    "    if not matches:\n",
    "        continue\n",
    "    for match in matches:\n",
    "        target = [t.text for t in nlp.tokenizer(match)]\n",
    "        matched_spans = find_span(target, tokens)\n",
    "    for matched_span in matched_spans:\n",
    "        for intermediate, token_idx in enumerate(matched_span):\n",
    "            prefix = \"I\" if intermediate else \"B\"\n",
    "            urls.append(\n",
    "                {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-URL_PERSONAL\", \"token_str\": tokens[token_idx]}\n",
    "            )\n",
    "    \n",
    "    # street\n",
    "#     matches = street_regex.findall(full_text)\n",
    "#     if not matches:\n",
    "#         continue\n",
    "#     for match in matches:\n",
    "#         target = [t.text for t in nlp.tokenizer(match)]\n",
    "#         matched_spans = find_span(target, tokens)\n",
    "#     for matched_span in matched_spans:\n",
    "#         for intermediate, token_idx in enumerate(matched_span):\n",
    "#             prefix = \"I\" if intermediate else \"B\"\n",
    "#             streets.append(\n",
    "#                 {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-STREET_ADDRESS\", \"token_str\": tokens[token_idx]}\n",
    "#             )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
