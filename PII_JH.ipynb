{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datasets\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments\\\n",
    "    , AutoConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.lang.en import English\n",
    "from ignite.metrics import Fbeta\n",
    "from functools import partial\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Finding out the number of labels\n",
    "# data = json.load(open('data/train.json'))\n",
    "\n",
    "# all_labels = set()\n",
    "\n",
    "# for d in data:\n",
    "#     all_labels = all_labels.union(set(d['labels']))\n",
    "\n",
    "# print(f\"{len(list(all_labels))} labels, with the following labels:\\n {list(all_labels)}\")\n",
    "# del data\n",
    "\n",
    "# label2id = {label:index for index,label in enumerate(all_labels)}\n",
    "# id2label = {index:label for index,label in enumerate(all_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to one-hot vector\n",
    "def oh_encoder(labels):  #label: array of output for each sentence\n",
    "    unique_labels = ['O', 'B-NAME_STUDENT','I-NAME_STUDENT','B-URL_PERSONAL', 'B-ID_NUM','I-ID_NUM','B-EMAIL','I-STREET_ADDRESS',\n",
    "                     'I-PHONE_NUM', 'B-USERNAME', 'B-PHONE_NUM','B-STREET_ADDRESS', 'I-URL_PERSONAL']\n",
    "    \n",
    "    labels_oh = []\n",
    "    for label in labels:    #label: str\n",
    "        label_oh = [float(0)]*len(unique_labels)\n",
    "        for k in range(len(unique_labels)):\n",
    "            if unique_labels[k] == label:\n",
    "                label_oh[k] = float(1)\n",
    "                # labels_oh.append(torch.tensor(label_oh, requires_grad=True))\n",
    "                labels_oh.append(label_oh)\n",
    "                break\n",
    "\n",
    "    #return torch.tensor(labels_oh, requires_grad=True)\n",
    "    return torch.tensor(labels_oh, requires_grad = True, dtype = float)\n",
    "    # return labels_oh    #list of one-hot labels as tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing sentences.\n",
    "def tokenize(example, tokenizer):\n",
    "    # Preprocess the tokens and labels by adding trailing whitespace and labels\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for token, label, t_ws in zip(example[\"tokens\"], \n",
    "                                  example[\"labels\"],\n",
    "                                  example[\"trailing_whitespace\"]):\n",
    "        tokens.append(token)\n",
    "        labels.extend([label] * len(token))\n",
    "        # Added trailing whitespace and label if true and \n",
    "        if t_ws:\n",
    "            tokens.append(\" \")\n",
    "            labels.append(\"O\")  \n",
    "    \n",
    "    text = \"\".join(tokens)\n",
    "    # print(f\"len(text)={len(text)}, len(tokens)={len(tokens)}\")\n",
    "    # tokenization without truncation\n",
    "    tokenized = tokenizer(text, return_offsets_mapping=True,\n",
    "                          truncation=False)\n",
    "    # labels = np.array(labels)\n",
    "    # Labels\n",
    "    token_labels = []\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # Added 'O' \n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            #token_labels.append(label2id[\"O\"]) \n",
    "            token_labels.append(\"O\") \n",
    "        else:\n",
    "            # case when the text starts with whitespace\n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "            # Convert label to id (int)\n",
    "            #label_id = label2id[labels[start_idx]]\n",
    "            label_id = labels[start_idx]\n",
    "            token_labels.append(label_id)\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": len(tokenized.input_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict(data):\n",
    "    '''\n",
    "    Change from a list of dictionary to a dictionary\n",
    "    '''\n",
    "    dict_of_lists = {}\n",
    "    for d in data:\n",
    "        for key, value in d.items():\n",
    "            if key in dict_of_lists:\n",
    "                dict_of_lists[key].append(value)\n",
    "            else:\n",
    "                dict_of_lists[key] = [value]\n",
    "    return dict_of_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = json.load(open('../data/train.json'))\n",
    "# d = data[0]\n",
    "\n",
    "# input_ids = tokenize(d, tokenizer)['input_ids'].numpy()[0]\n",
    "# # print(input_ids)\n",
    "# print(len(input_ids))\n",
    "# tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "# # print(tokens)\n",
    "# # print(d['tokens'])\n",
    "# print(len(d['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split completed\n",
      "trainset loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c46031f6034822accdee5775e7ec48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/5785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset mapped\n",
      "valset loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61128370f284c19938189ef12443709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/1022 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valset mapped\n"
     ]
    }
   ],
   "source": [
    "#Preparing the datasets for token classification\n",
    "data = json.load(open('../data/train.json'))\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.15, random_state=42)  \n",
    "print('Data split completed')\n",
    "\n",
    "# # Limit to 100 for testing\n",
    "# train_data = train_data[:100]\n",
    "# val_data = val_data[:100]\n",
    "\n",
    "trainset = datasets.Dataset.from_dict({\n",
    "    'full_text': [x['full_text'] for x in train_data],\n",
    "    'document': [x['document'] for x in train_data],\n",
    "    'tokens': [x['tokens'] for x in train_data],\n",
    "    'trailing_whitespace': [x['trailing_whitespace'] for x in train_data],\n",
    "    'labels' :[x['labels'] for x in train_data]\n",
    "    # 'labels' :[oh_encoder(x['labels']) for x in train_data] \n",
    "})\n",
    "print('trainset loaded')\n",
    "\n",
    "trainset = trainset.map(tokenize, fn_kwargs = {\"tokenizer\": tokenizer}, num_proc=3)\n",
    "#train_labels = [oh_encoder(x['labels'] for x in train_data)]\n",
    "print('trainset mapped')\n",
    "\n",
    "# val_labels = [oh_encoder(x['labels']) for x in val_data]\n",
    "\n",
    "valset = datasets.Dataset.from_dict({\n",
    "    'full_text': [x['full_text'] for x in val_data],\n",
    "    'document': [x['document'] for x in val_data],\n",
    "    'tokens': [x['tokens'] for x in val_data],\n",
    "    'trailing_whitespace': [x['trailing_whitespace'] for x in val_data],\n",
    "    'labels' :[x['labels'] for x in val_data]\n",
    "    # 'labels' :[oh_encoder(x['labels']) for x in val_data]\n",
    "})\n",
    "print('valset loaded')\n",
    "\n",
    "valset = valset.map(tokenize, fn_kwargs = {\"tokenizer\": tokenizer}, num_proc=3)\n",
    "print('valset mapped')\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert from a list of dict to 1 big dict\n",
    "trainset = to_dict(trainset)\n",
    "#trainset['labels'] = train_labels\n",
    "\n",
    "valset = to_dict(valset)\n",
    "# #valset['labels'] = val_labels\n",
    "\n",
    "# Apply one hot encoding to labels\n",
    "trainset['one_hot_labels'] = [[oh_encoder(item) for item in sublist] for sublist in trainset['labels']]\n",
    "valset['one_hot_labels'] = [[oh_encoder(item) for item in sublist] for sublist in valset['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_data(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        self.data = data_dict['input_ids']\n",
    "        self.attention_mask = data_dict['attention_mask']\n",
    "        self.labels = data_dict['labels']\n",
    "        self.doc_no = data_dict['document']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx]), torch.tensor(self.attention_mask[idx]), oh_encoder(self.labels[idx]) , torch.tensor(self.doc_no[idx])\n",
    "\n",
    "custom_train = Custom_data(trainset)\n",
    "custom_val = Custom_data(valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    '''\n",
    "    For padding\n",
    "    '''\n",
    "    input_ids, attention_mask, one_hot_labels, document = zip(*batch)\n",
    "    # Pad the input_ids and labels\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    padded_attention_mask = pad_sequence(attention_mask, batch_first = True, padding_value = 0)\n",
    "    padded_labels = pad_sequence(one_hot_labels, batch_first=True, padding_value=0)  \n",
    "    \n",
    "    return padded_input_ids, padded_attention_mask, padded_labels, document\n",
    "\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(custom_train, batch_size=batch_size, collate_fn = custom_collate)\n",
    "val_dataloader = DataLoader(custom_val, batch_size=batch_size, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(outputs, labels):    \n",
    "    unique_labels = ['O', 'B-NAME_STUDENT','I-NAME_STUDENT','B-URL_PERSONAL', 'B-ID_NUM','I-ID_NUM','B-EMAIL','I-STREET_ADDRESS',\n",
    "                     'I-PHONE_NUM', 'B-USERNAME', 'B-PHONE_NUM','B-STREET_ADDRESS', 'I-URL_PERSONAL']\n",
    "    try:\n",
    "        #print(\"Compute metrics\")\n",
    "        predictions = torch.argmax(softmax(outputs, dim=2), dim=2)\n",
    "        # Include prediction Remove ignored index (special tokens)\n",
    "        true_preds = []\n",
    "        true_labels = []\n",
    "        for pred, label in zip(predictions, labels):\n",
    "            true_preds.append([unique_labels[p] for p, l in zip(pred, label) if l[0] != -100])\n",
    "            true_labels.append([unique_labels[torch.argmax(l)] for p, l in zip(pred, label) if l[0] != -100])\n",
    "        \n",
    "        mlb = MultiLabelBinarizer(classes=unique_labels)\n",
    "        true_preds_bin = mlb.fit_transform(true_preds)\n",
    "        true_labels_bin = mlb.transform(true_labels)\n",
    "        # Compute recall, precision and f5 score\n",
    "        recall = recall_score(true_labels_bin, true_preds_bin, average='samples')\n",
    "        precision = precision_score(true_labels_bin, true_preds_bin, average='samples')\n",
    "        # Use modified f5 score to measure the performance\n",
    "        f5_score = (1 + 5*5) * (recall * precision / (5*5*precision + recall))\n",
    "        result = {'f5': f5_score,  \n",
    "                  'recall': recall,\n",
    "                  'precision': precision}\n",
    "        # print(f\"result = {result}\")\n",
    "        return result\n",
    "    except Exception as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, device, batch_print):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    avg_train_loss = 0\n",
    "    train_f5 = 0\n",
    "    avg_train_f5 = 0\n",
    "\n",
    "    for idx, (input_ids, attention_mask, labels, doc) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss\n",
    "        f5 = compute_metrics(outputs, labels)['f5']\n",
    "        train_f5 += f5\n",
    "        \n",
    "        # if idx%batch_print == 0:\n",
    "        #     print(f\"Completed batch {idx}/{len(train_loader)}, with current train_loss: {loss:.3f}, current f5_score: {f5:.3f}\") \n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader) \n",
    "    avg_train_f5 = train_f5 / len(train_loader)\n",
    "\n",
    "    return avg_train_loss , avg_train_f5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, val_loader, criterion, device, batch_print):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    f5_score = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (input_ids, attention_mask, labels, doc) in enumerate(val_loader):\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask).logits\n",
    "            val_loss += criterion(outputs, labels)\n",
    "            f5_score += compute_metrics(outputs, labels)['f5']\n",
    "            \n",
    "            # if idx%batch_print == 0:\n",
    "            #     print(f\"Completed batch {idx}/{len(val_loader)}, with current train_loss: {criterion(outputs, labels)}\")  \n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_f5 = f5_score / len(val_loader)\n",
    "    \n",
    "    return avg_val_loss, avg_f5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Deberta small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "model_name = \"microsoft/deberta-v3-small\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "in_features = model.classifier.in_features\n",
    "model.classifier = nn.Sequential( \n",
    "    nn.Linear(in_features, 13)\n",
    ")\n",
    "\n",
    "# Unfreeze parameters in classifier layer\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's trainable parameters:  9,997\n",
      "Model's total parameters:  141,314,317\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model's trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad): ,}\")\n",
    "print(f\"Model's total parameters: {sum(p.numel() for p in model.parameters()): ,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda'\n",
    "# model = model.to(device)\n",
    "# epochs = 10\n",
    "\n",
    "# train_loader = train_dataloader\n",
    "# val_loader = val_dataloader\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=0.0005)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
    "# criterion = nn.CrossEntropyLoss().to(device)\n",
    "# train_batch_print = 10000\n",
    "# val_batch_print = 3000\n",
    "\n",
    "# best_valid_loss = float('inf')\n",
    "# best_valid_f5 = float('inf')\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     train_loss, train_f5 = train(model, train_loader, optimizer, criterion, device, train_batch_print)\n",
    "#     val_loss, val_f5 = val(model, val_loader, criterion, device, val_batch_print)\n",
    "\n",
    "#     # train_loss = train(model, train_loader, optimizer, criterion, device, train_batch_print)\n",
    "#     # val_loss = val(model, val_loader, criterion, device, val_batch_print)\n",
    "    \n",
    "#     if val_loss < best_valid_loss:\n",
    "#         best_valid_loss = val_loss\n",
    "#         torch.save(model.state_dict(), 'model_best_loss_CE.pt')\n",
    "#     if val_f5 > best_valid_f5:\n",
    "#         best_valid_f5 = val_f5\n",
    "#         torch.save(model.state_dict(), 'model_best_f5_CE.pt')\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "#     print(f'Epoch: {epoch+1:02} | training loss: {train_loss:.3f} | val loss: {val_loss:.3f} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#     print(f'train f5: {train_f5:.3f} | val f5: {val_f5:.3f} | Ending time: {end_time}')\n",
    "#     torch.cuda.empty_cache()\n",
    "#     print('========'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | training loss: 352.685 | val loss: 356.063 | Epoch Time: 39m 49s\n",
      "train f5: 0.732 | val f5: 0.742 | Ending time: 2024-04-15 09:21:13.480311\n",
      "================================================================================\n",
      "Epoch: 02 | training loss: 352.508 | val loss: 356.052 | Epoch Time: 45m 10s\n",
      "train f5: 0.737 | val f5: 0.741 | Ending time: 2024-04-15 10:06:23.749257\n",
      "================================================================================\n",
      "Epoch: 03 | training loss: 352.505 | val loss: 356.050 | Epoch Time: 52m 35s\n",
      "train f5: 0.736 | val f5: 0.754 | Ending time: 2024-04-15 10:58:59.211676\n",
      "================================================================================\n",
      "Epoch: 04 | training loss: 352.510 | val loss: 356.053 | Epoch Time: 53m 1s\n",
      "train f5: 0.732 | val f5: 0.752 | Ending time: 2024-04-15 11:52:01.119493\n",
      "================================================================================\n",
      "Epoch: 05 | training loss: 352.496 | val loss: 356.134 | Epoch Time: 52m 54s\n",
      "train f5: 0.729 | val f5: 0.747 | Ending time: 2024-04-15 12:44:55.506350\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model = model.to(device)\n",
    "epochs = 5\n",
    "\n",
    "train_loader = train_dataloader\n",
    "val_loader = val_dataloader\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "train_batch_print = 10000\n",
    "val_batch_print = 3000\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "best_valid_f5 = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_f5 = train(model, train_loader, optimizer, criterion, device, train_batch_print)\n",
    "    val_loss, val_f5 = val(model, val_loader, criterion, device, val_batch_print)\n",
    "\n",
    "    # train_loss = train(model, train_loader, optimizer, criterion, device, train_batch_print)\n",
    "    # val_loss = val(model, val_loader, criterion, device, val_batch_print)\n",
    "    \n",
    "    if val_loss < best_valid_loss:\n",
    "        best_valid_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'model_best_loss_CE_s.pt')\n",
    "    if val_f5 > best_valid_f5:\n",
    "        best_valid_f5 = val_f5\n",
    "        torch.save(model.state_dict(), 'model_best_f5_CE_s.pt')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    print(f'Epoch: {epoch+1:02} | training loss: {train_loss:.3f} | val loss: {val_loss:.3f} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'train f5: {train_f5:.3f} | val f5: {val_f5:.3f} | Ending time: {dt.datetime.fromtimestamp(time.time())}')\n",
    "    # torch.cuda.empty_cache()\n",
    "    print('========'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: DeBerta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, valset, batch_size, collator, criterion,  f5, device):\n",
    "    model.eval()\n",
    "\n",
    "    avg_val_loss = 0\n",
    "    avg_val_score = 0\n",
    "    with torch.no_grad:\n",
    "        val_loss = 0\n",
    "        val_score = 0\n",
    "        val_dataloader = DataLoader(custom_val, batch_size=batch_size, collate_fn=custom_collate)\n",
    "\n",
    "        for (input_ids, attention_mask, labels) in val_dataloader:                \n",
    "            \n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "            val_score += f5(outputs, labels)\n",
    "\n",
    "    avg_val_loss = val_loss / len(valset) \n",
    "    avg_val_score = val_score/len(valset)\n",
    "\n",
    "    print(f\"Average val_loss: {avg_val_loss}, avgerage val_score = {avg_val_score}\")\n",
    "\n",
    "    return avg_val_loss, avg_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainset, batch_size, collator, epochs, optimizer, criterion,  f5, device):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        avg_train_loss = 0\n",
    "        avg_train_score = 0\n",
    "        train_loss = 0\n",
    "        train_score = 0\n",
    "\n",
    "        train_dataloader = DataLoader(custom_train, batch_size=batch_size, collate_fn = custom_collate)\n",
    "        \n",
    "        for (input_ids, attention_mask, labels) in train_dataloader:     \n",
    "            optimizer.zero_grad()\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "\n",
    "            outputs = model(input_ids, attention_mask).logits\n",
    "            #print(outputs.logits)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss\n",
    "            train_score += f5(outputs, labels)\n",
    "\n",
    "        avg_train_loss = train_loss / len(trainset)    \n",
    "        avg_train_score = train_score / len(trainset)\n",
    "        val_loss, val_score = val(model, valset, batch_size, collator, criterion,  f5, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Average train_loss = {avg_train_loss}, average train_score = {avg_train_score}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = model.classifier.in_features\n",
    "model.classifier = nn.Sequential( \n",
    "    nn.Linear(in_features, 13)\n",
    ")\n",
    "\n",
    "# Unfreeze parameters in classifier layer\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Check which layers are frozen\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Layer: {name} | Frozen: {'NO' if param.requires_grad else 'YES'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' \n",
    "model = model.to(device)\n",
    "epochs = 5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "f5 = Fbeta(beta=5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m----> 2\u001b[0m train(model, trainset, batch_size, custom_collate, epochs, optimizer, criterion, f5, device)\n",
      "Cell \u001b[1;32mIn[29], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, trainset, batch_size, collator, epochs, optimizer, criterion, f5, device)\u001b[0m\n\u001b[0;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#print(outputs.logits)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "train(model, trainset, batch_size, collator, epochs, optimizer, criterion,  f5, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: DeBERTa\n",
    "\n",
    "Using a pretrained DeBERTa, we will build a classifier head on top of it to predict the class at token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim = 13):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.dropout_prob = 0.3\n",
    "        self.final_activation = nn.Softmax(dim = -1)\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.ReLU(nn.Linear(input_dim, hidden_dim)),\n",
    "            \n",
    "            nn.Dropout(self.dropout_prob),\n",
    "            nn.ReLU(nn.Linear(hidden_dim, hidden_dim*2)),\n",
    "\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "            nn.ReLU(nn.Linear(hidden_dim*2, hidden_dim)),\n",
    "\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        logit = self.linear(x)\n",
    "        return self.final_activation(logit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim = 13):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.dropout_prob = 0.3\n",
    "        self.final_activation = nn.Softmax(dim = -1)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        self.linear1 = nn.ReLU(nn.Linear(input_dim,hidden_dim))\n",
    "        self.linear2 = nn.ReLU(nn.Linear(hidden_dim, hidden_dim*2))                            \n",
    "        self.linear3 = nn.ReLU(nn.Linear(hidden_dim*2, hidden_dim))\n",
    "        self.output = nn.Linear(hidden_dim,output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('now start linear1')\n",
    "        print(x.size())\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        print('now start linear2')\n",
    "        print(x.size())\n",
    "        x = self.linear2(self.dropout(x))\n",
    "\n",
    "        print('now start linear3')\n",
    "        print(x.size())\n",
    "        x = self.linear3(self.dropout(x))\n",
    "\n",
    "        print('now start output')\n",
    "        print(x.size())\n",
    "        logit= self.output(x)\n",
    "        \n",
    "        print('now start activation')\n",
    "        output =  self.final_activation(logit)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deberta_Classif(Classifier):\n",
    "    def __init__(self, model_name, classif_input, classif_hidden, classif_output = 13, finetune = False):\n",
    "        super(Deberta_Classif, self).__init__(classif_input, classif_input)\n",
    "        self.ft = finetune\n",
    "\n",
    "        self.extractor = AutoModelForTokenClassification.from_pretrained(model_name).base_model\n",
    "\n",
    "        if not finetune:\n",
    "            for param in self.extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.extractor_num_param =  sum(p.numel() for p in self.extractor.parameters())\n",
    "        self.extractor_num_param_grad = sum(p.numel() for p in self.extractor.parameters() if p.requires_grad)\n",
    "        self.extractor_name = \"DeBERTa\"\n",
    "                \n",
    "\n",
    "        self.classifier = Classifier(input_dim=classif_input, hidden_dim=classif_hidden ,output_dim=classif_output)\n",
    "        self.classifier_num_param = sum(p.numel() for p in self.classifier.parameters() if p.requires_grad)\n",
    "\n",
    "        \n",
    "    def count_param(self):\n",
    "        \n",
    "        if self.ft:\n",
    "            type = 'finetuned'\n",
    "            num_param = self.extractor_num_param_grad + self.classifier_num_param\n",
    "        else:\n",
    "            type = 'non-finetuned'\n",
    "            num_param = self.extractor_num_param + self.classifier_num_param\n",
    "\n",
    "        print(f\"Number of parameters in {type} {self.extractor_name} model is {num_param:,}\")\n",
    "    \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.extractor(input_ids, attention_mask).last_hidden_state\n",
    "        x = self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([13, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's trainable parameters:  9,997\n",
      "Model's total parameters:  183,841,549\n"
     ]
    }
   ],
   "source": [
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "#model = Deberta_Classif(model_name, classif_input = 768, classif_hidden = 100, classif_output = 13, finetune=False)  #Extractor output has dim 768 \n",
    "config = AutoModelForTokenClassification.from_pretrained(model_name).config\n",
    "\n",
    "config.update({\n",
    "            'num_labels': 13,\n",
    "            'ignore_mismatched_sizes': True,\n",
    "        })\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, config = config, ignore_mismatched_sizes=True)\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Model's trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad): ,}\")\n",
    "print(f\"Model's total parameters: {sum(p.numel() for p in model.parameters()): ,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' \n",
    "model = model.to(device)\n",
    "epochs = 5\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of = 32, max_length=3500)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "f5 = Fbeta(beta=5).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7dc5da0d3746a186266a72d7d77e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/5785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56529294e2a4446fba6decb75c5cd88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/1022 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Preparing the datasets for token classification\n",
    "data = json.load(open('../data/train.json'))\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.15, random_state=42)  \n",
    "# train_labels = [oh_encoder(x['labels']) for x in train_data]\n",
    "\n",
    "trainset = Dataset.from_dict({\n",
    "    # 'full_text': [x['full_text'] for x in train_data],\n",
    "    # 'document': [x['document'] for x in train_data],\n",
    "    'tokens': [x['tokens'] for x in train_data],\n",
    "    'trailing_whitespace': [x['trailing_whitespace'] for x in train_data],\n",
    "    'labels' :[oh_encoder(x['labels']) for x in train_data]\n",
    "})\n",
    "\n",
    "print('trainset loaded')\n",
    "trainset = trainset.map(tokenize, fn_kwargs = {\"tokenizer\": tokenizer}, num_proc=3)\n",
    "#train_labels = [oh_encoder(x['labels'] for x in train_data)]\n",
    "print('trainset mapped')\n",
    "\n",
    "# val_labels = [oh_encoder(x['labels']) for x in val_data]\n",
    "\n",
    "valset = Dataset.from_dict({\n",
    "    # 'full_text': [x['full_text'] for x in val_data],\n",
    "    # 'document': [x['document'] for x in val_data],\n",
    "    'tokens': [x['tokens'] for x in val_data],\n",
    "    'trailing_whitespace': [x['trailing_whitespace'] for x in val_data],\n",
    "    'labels' :[oh_encoder(x['labels']) for x in val_data]\n",
    "})\n",
    "print('valset loaded')\n",
    "valset = valset.map(tokenize, fn_kwargs = {\"tokenizer\": tokenizer}, num_proc=3)\n",
    "print('valset mapped')\n",
    "\n",
    "#val_labels = [oh_encoder(x['labels'] for x in val_data)]\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data: 5785 || Number of validation data: 1022\n"
     ]
    }
   ],
   "source": [
    "#First item\n",
    "print(f\"Number of training data: {len(trainset)} || Number of validation data: {len(valset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict(data):\n",
    "    dict_of_lists = {}\n",
    "    for d in data:\n",
    "        for key, value in d.items():\n",
    "            if key in dict_of_lists:\n",
    "                dict_of_lists[key].append(value)\n",
    "            else:\n",
    "                dict_of_lists[key] = [value]\n",
    "    return dict_of_lists\n",
    "\n",
    "trainset = to_dict(trainset)\n",
    "#trainset['labels'] = train_labels\n",
    "\n",
    "valset = to_dict(valset)\n",
    "#valset['labels'] = val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class Custom_data(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        self.data = data_dict['input_ids']\n",
    "        self.attention_mask = data_dict['attention_mask']\n",
    "        self.labels = data_dict['labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx][0]), torch.tensor(self.attention_mask[idx][0]), torch.tensor(self.labels[idx][0])\n",
    "\n",
    "# Assuming custom_train is properly initialized as Custom_data\n",
    "custom_train = Custom_data(trainset)\n",
    "custom_val = Custom_data(valset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    input_ids, attention_mask, labels = zip(*batch)\n",
    "    # Pad the input_ids and labels\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    padded_attention_mask = pad_sequence(attention_mask, batch_first = True, padding_value = 0)\n",
    "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=-100)  # Padding for labels in token classification\n",
    "    return padded_input_ids, padded_attention_mask, padded_labels\n",
    "\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(custom_train, batch_size=batch_size, collate_fn = custom_collate)\n",
    "val_dataloader = DataLoader(custom_val, batch_size=batch_size, collate_fn=custom_collate)\n",
    "\n",
    "# for i, (input_ids, attention_mask, labels) in enumerate(train_dataloader):\n",
    "#     print(f'Batch {i + 1}:')\n",
    "#     print('Input IDs:', input_ids)\n",
    "#     print('Attention_mask:', attention_mask)\n",
    "#     print('Labels:', labels)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    data, attention, labels = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  6738, 78580,  ..., 31401,   260,     2],\n",
       "        [    1,  1391,   367,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, valset, batch_size, collator, criterion,  f5, device):\n",
    "    model.eval()\n",
    "\n",
    "    avg_val_loss = 0\n",
    "    avg_val_score = 0\n",
    "    with torch.no_grad:\n",
    "        val_loss = 0\n",
    "        val_score = 0\n",
    "        val_dataloader = DataLoader(custom_val, batch_size=batch_size, collate_fn=custom_collate)\n",
    "\n",
    "        for (input_ids, attention_mask, labels) in val_dataloader:                \n",
    "            \n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "            val_score += f5(outputs, labels)\n",
    "\n",
    "    avg_val_loss = val_loss / len(valset) \n",
    "    avg_val_score = val_score/len(valset)\n",
    "\n",
    "    print(f\"Average val_loss: {avg_val_loss}, avgerage val_score = {avg_val_score}\")\n",
    "\n",
    "    return avg_val_loss, avg_val_score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainset, batch_size, collator, epochs, optimizer, criterion,  f5, device):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        avg_train_loss = 0\n",
    "        avg_train_score = 0\n",
    "        train_loss = 0\n",
    "        train_score = 0\n",
    "\n",
    "        \n",
    "        train_dataloader = DataLoader(custom_train, batch_size=batch_size, collate_fn = custom_collate)\n",
    "        \n",
    "        for (input_ids, attention_mask, labels) in train_dataloader:     \n",
    "            optimizer.zero_grad()\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "\n",
    "            outputs = model(input_ids, attention_mask).logits\n",
    "            #print(outputs.logits)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss\n",
    "            train_score += f5(outputs, labels)\n",
    "\n",
    "        avg_train_loss = train_loss / len(trainset)    \n",
    "        avg_train_score = train_score / len(trainset)\n",
    "        val_loss, val_score = val(model, valset, batch_size, collator, criterion,  f5, device)\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Average train_loss = {avg_train_loss}, average train_score = {avg_train_score}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[123], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, trainset, batch_size, collator, epochs, optimizer, criterion, f5, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (input_ids, attention_mask, labels) \u001b[38;5;129;01min\u001b[39;00m train_dataloader:     \n\u001b[0;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 16\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     18\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mlong()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "train(model, trainset, batch_size, collator, epochs, optimizer, criterion, f5, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting from predictions to NER labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to label each token at NER stage\n",
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "    \n",
    "    idx = 0\n",
    "    spans = []\n",
    "    span = []\n",
    "\n",
    "    for i, token in enumerate(document):\n",
    "        if token != target[idx]:\n",
    "            idx = 0\n",
    "            span = []\n",
    "            continue\n",
    "        span.append(i)\n",
    "        idx += 1\n",
    "        if idx == len(target):\n",
    "            spans.append(span)\n",
    "            span = []\n",
    "            idx = 0\n",
    "            continue\n",
    "    \n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DON'T RUN ##\n",
    "#### From KAGGLE: https://www.kaggle.com/code/manavtrivedi/0-967-nlp-sakura/notebook ####\n",
    "\n",
    "triplets = []\n",
    "pairs = set()  # membership operation using set is faster O(1) than that of list O(n)\n",
    "processed = []\n",
    "emails = []\n",
    "phone_nums = []\n",
    "urls = []\n",
    "streets = []\n",
    "\n",
    "# For each prediction, token mapping, offsets, tokens, and document in the dataset\n",
    "for p, token_map, offsets, tokens, doc, full_text in zip(\n",
    "    preds_final, \n",
    "    ds[\"token_map\"], \n",
    "    ds[\"offset_mapping\"], \n",
    "    ds[\"tokens\"], \n",
    "    ds[\"document\"],\n",
    "    ds[\"full_text\"]\n",
    "):\n",
    "\n",
    "    # Iterate through each token prediction and its corresponding offsets\n",
    "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "        label_pred = id2label[str(token_pred)]  # Predicted label from token\n",
    "        if start_idx + end_idx == 0:\n",
    "            continue\n",
    "        if token_map[start_idx] == -1:\n",
    "            start_idx += 1\n",
    "        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "            start_idx += 1\n",
    "        if start_idx >= len(token_map):\n",
    "            break\n",
    "        token_id = token_map[start_idx]  # Token ID at start index\n",
    "        if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n",
    "            continue\n",
    "        pair = (doc, token_id)\n",
    "        if pair not in pairs:\n",
    "            processed.append({\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]})\n",
    "            pairs.add(pair)\n",
    "    \n",
    "    # email\n",
    "    for token_idx, token in enumerate(tokens):\n",
    "        if re.fullmatch(email_regex, token) is not None:\n",
    "            emails.append(\n",
    "                {\"document\": doc, \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "            )\n",
    "                \n",
    "    # phone number\n",
    "    matches = phone_num_regex.findall(full_text)\n",
    "    if not matches:\n",
    "        continue\n",
    "    for match in matches:\n",
    "        target = [t.text for t in nlp.tokenizer(match)]\n",
    "        matched_spans = find_span(target, tokens)\n",
    "    for matched_span in matched_spans:\n",
    "        for intermediate, token_idx in enumerate(matched_span):\n",
    "            prefix = \"I\" if intermediate else \"B\"\n",
    "            phone_nums.append(\n",
    "                {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": tokens[token_idx]}\n",
    "            )\n",
    "    \n",
    "    # url\n",
    "    matches = url_regex.findall(full_text)\n",
    "    if not matches:\n",
    "        continue\n",
    "    for match in matches:\n",
    "        target = [t.text for t in nlp.tokenizer(match)]\n",
    "        matched_spans = find_span(target, tokens)\n",
    "    for matched_span in matched_spans:\n",
    "        for intermediate, token_idx in enumerate(matched_span):\n",
    "            prefix = \"I\" if intermediate else \"B\"\n",
    "            urls.append(\n",
    "                {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-URL_PERSONAL\", \"token_str\": tokens[token_idx]}\n",
    "            )\n",
    "    \n",
    "    # street\n",
    "#     matches = street_regex.findall(full_text)\n",
    "#     if not matches:\n",
    "#         continue\n",
    "#     for match in matches:\n",
    "#         target = [t.text for t in nlp.tokenizer(match)]\n",
    "#         matched_spans = find_span(target, tokens)\n",
    "#     for matched_span in matched_spans:\n",
    "#         for intermediate, token_idx in enumerate(matched_span):\n",
    "#             prefix = \"I\" if intermediate else \"B\"\n",
    "#             streets.append(\n",
    "#                 {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-STREET_ADDRESS\", \"token_str\": tokens[token_idx]}\n",
    "#             )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
